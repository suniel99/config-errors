
%\todo{Are variants 2 and three really variants of \ourtool, or are they
%  completely different techniques that should be presented as such?  In
%  particular, can you characterize variant 2 (here and in the table) as
%  Tarantula, possibly with some small enhancements.  I have a similar
%  question about variant 3.  In any event, make clearer what part of the
%  architecture is replaced by each variant.}

Another possible way to diagnose a configuration error is to leverage
existing fault localization techniques, by treating the undesired
execution as a failing run and all correct executions (in the database)
as passing runs. We next compare \ourtool with two state-of-the-art
techniques: % in error diagnosis:

\begin{itemize}
\item \textbf{Statement-level Coverage Analysis}. This technique treats statements covered
by the undesired execution profile as potentially buggy, and statements
covered the correct execution profiles as correct.
Then, it leverages a well-known fault localization technique,
Tarantula~\cite{Jones:2002}, to rank the likelihood of each
statement being buggy, and queries the results of thin slicing
to identify its affecting configuration options as the root causes.
The results are essentially the same for a variant of
coverage analysis: using thin slicing to compute all affected statements,
and only monitoring the coverage of such affected statements.

\item \textbf{Method-level Invariant Analysis}. This technique stores invariants detected
by Daikon~\cite{Ernst:1999} from correct executions in the database.
It treats a method as having suspicious behavior if its observed invariants
from the undesired execution are different from the invariants stored
in the database~\cite{McCamant:2003}. This technique ranks a method's suspiciousness by
the number of different invariants, and queries the results of thin slicing
to identify its affecting configuration options as the root causes. 
\end{itemize}


\todo{I edited the following paragraph}
For both techniques, we measured the results of using the selected
similar execution profiles and the results of using
all execution profiles from the pre-built
database. As show in the columns ``Coverage Analysis'' and ``Invariant Analysis''
of Figure~\ref{tab:results}, both techniques produce less accurate results.
%; and for some errors, even fail to identify the actual root causes.

In Coverage Analysis, the statement-level granularity is \textit{too fine-grained}.
Many statements have exactly the same coverage by the failing/passing executions,
and thus have the same suspiciousness score as computed by Tarantula~\cite{Jones:2002}.
Furthermore, Tarantula only records whether a
statement has been executed or not but does not record how a statement is 
executed (e.g., how often a predicate evaluates to true). The combination
of these two factors causes the low accuracy.


In Invariant Analysis, the method-level granularity is \textit{too coarse-grained}.
Invariant detection techniques like Daikon~\cite{Ernst:1999}
only check program states at method entries and exits to infer
likely pre- and post-conditions, and thus are less
sensitive to many control flow details within a method (e.g., a predicate's
true ratio). In our study, Invariant Analysis failed to diagnose 3
errors because it failed to infer invariants for 1 program (Synoptic),
and reported the same invariants for the other two programs (Soot and Randoop)
on the method containing behaviorally-deviated predicates between an undesired execution and correct executions.

\todo{I added the following pragraph}
Both techniques tend to produce less accurate results when only
using the selected similar execution profiles. This is because
the suspiciousness of a statement or method
is inversely proportional to the number of correct execution profiles that cover
it. When using fewer correct execution profiles,
more statements or methods would have the same suspiciousness scores.



%method-level granularity is too coarse
%is not sensitive enough on small control flow changes. for example, the invariant is the same


\todo{I edited the following paragraph}
This experiment suggests that we cannot
treat configuration options as just another regular program input,
and then directly applying existing 
fault localization techniques~\cite{Jones:2002, McCamant:2003} to
find the error causes. 
The primary reason is that, unlike a program input,
a configuration option is often used to control a
program's control rather than produce results. 
Thus, focusing on the behaviors of relevant predicates as our tool does
 may be a good choice.
