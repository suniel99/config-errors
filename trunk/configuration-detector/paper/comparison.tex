
%\todo{Are variants 2 and three really variants of \ourtool, or are they
%  completely different techniques that should be presented as such?  In
%  particular, can you characterize variant 2 (here and in the table) as
%  Tarantula, possibly with some small enhancements.  I have a similar
%  question about variant 3.  In any event, make clearer what part of the
%  architecture is replaced by each variant.}

Another possible way to diagnose a configuration error is to leverage
existing fault localization techniques, by treating the undesired
execution as a failing run and all correct executions (in the database)
as passing runs. We next compare \ourtool with two state-of-the-art
techniques: % in error diagnosis:

\begin{itemize}
\item \textbf{Statement-level Coverage Analysis}. This technique treats statements covered
by the undesired execution profile as potentially buggy, and statements
covered the correct execution profiles as correct.
Then, it leverages a well-known fault localization technique,
Tarantula~\cite{Jones:2002}, to rank the likelihood of each
statement being buggy, and queries the results of thin slicing
to identify its affecting configuration options as the root causes.
The results are essentially the same for a variant of
coverage analysis: using thin slicing to compute all affected statements,
and only monitoring the coverage of such affected statements.

\item \textbf{Method-level Invariant Analysis}. This technique stores invariants detected
by Daikon~\cite{Ernst:1999} from correct executions in the database.
It treats a method as having suspicious behavior if its observed invariants
from the undesired execution are different from the invariants stored
in the database~\cite{McCamant:2003}. This technique ranks a method's suspiciousness by
the number of different invariants, and queries the results of thin slicing
to identify its affecting configuration options as the root causes. 
\end{itemize}


In Coverage Analysis, the statement-level granularity is \textit{too fine-grained}.
Many statements have exactly the same coverage in the failing/passing executions,
and thus have the same suspiciousness score as computed by Tarantula~\cite{Jones:2002}.
Furthermore, Tarantula only records whether a
statement has been executed or not but does not record how a statement is 
executed (e.g., how often a predicate evaluates to true). The combination
of these two factors causes the low accuracy.


In Invariant Analysis, the method-level granularity is \textit{too coarse-grained}.
Invariant detection techniques like Daikon~\cite{Ernst:1999}
only check program states at method entries and exits to infer
likely pre- and post-conditions, and thus are less
sensitive to control flow details within a method (e.g., a predicate's
true ratio). In our study, Invariant Analysis failed to diagnose 3
errors.  For Synoptic, it failed to infer invariants.  For Soot and
Randoop, it reported the same invariants
over undesired and correct executions, for
the method containing behaviorally-deviated predicates.



%method-level granularity is too coarse
%is not sensitive enough on small control flow changes. for example, the invariant is the same


This experiment suggests that we cannot
treat configuration options as just another regular program input,
and then directly apply existing 
fault localization techniques~\cite{Jones:2002, McCamant:2003} to
find the error causes. 
The primary reason is that, unlike a program input,
a configuration option is often used to control a
program's control rather than produce result data. 
Thus, focusing on the behaviors of relevant predicates as our tool does
 may be a good choice.

%%  LocalWords:  pre
