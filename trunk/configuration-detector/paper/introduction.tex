
\section{Introduction}
\label{sec:introduction}

Modern software is extraordinarily complex. Many applications have a large
number of configuration options that offer users great flexibility to
customize their behaviors. This flexibility has a cost: when something
goes wrong, diagnosing a configuration error can be both time-consuming
and frustrating. Technical support contributes 17\% of the total cost of ownership of
today's desktop computers~\cite{confevidence}, and troubleshooting misconfigurations
is a large part of technical support.

Software misconfigurations are often exhibited by an application unexpectedly terminating
(i.e., crashing errors) or producing an incorrect output (i.e., non-crashing errors). While an ideal application would always
output a helpful error message when such events occur, it is unfortunately
the case that such messages are often cryptic, misleading, or
even non-existent~\cite{Yin:2011:ESC, Attariyan:2010:ACT, Hubaux:2012, rangefix}.
Thus, users must search manuals, FAQs, and online forums to find potential
solutions to the problem. %$\blacksquare$ this process is frustrating..

\subsection{Motivating Example}

Randoop is an automated unit test generation tool, which implements
a technique called feedback-directed random test generation~\cite{randoop}.
It has been used in academic institutions and
industrial companies such as Microsoft, Google,
and ABB, and found numerous previously-unknown bugs.

During the maintenance of the Randoop tool, we received a ``bug report''
from a testing expert who has been using Randoop for quite a while.
The bug report indicated that Randoop failed to generate
tests for a subject program called Nanoxml~\cite{nanoxml}. When running Randoop,
it terminates normally but simply does not output any tests.

Compared to crashing errors, this non-crashing error is particularly difficult to diagnose.
First, Randoop silently failed without exhibiting a crashing point or
a stack trace. This makes existing techniques like dynamic slicing~\cite{Zhang:2003:PDS},
failure trace analysis~\cite{Rabkin:2011:PPC, Attariyan:2010:ACT} inapplicable. Second, there is no
available testing oracle to test whether a program behaves correctly.
This makes existing bug isolation techniques like delta debugging
inapplicable. In fact, to the best of our knowledge, we are not
aware any existing tool can be used to pinpoint the root cause.

When diagnosing using our \ourtool tool, it outputs an error report as
shown in Figure~\ref{}, pinpointing that a configuration option named
\CodeIn{maxsize} may be responsible for the observed behavior.

\begin{figure}[t]
\begin{CodeOut}
\begin{alltt} 
Configuration option: randoop.main.GenInputsAbstract.maxsize
The predicate: "newSequence.size() > GenInputsAbstract.maxsize" in
"randoop.ForwardGenerator.createNewUniqueSequence()" (line: 312)
shows different behaviors.

In good runs, it evaluates to true:  14.4\% of the time (1315 observations)
In the bad run, it evaluates to true: 32.3\% of the time (2727 observations)

\end{alltt}
\end{CodeOut}
\vspace*{-15pt}
\Caption{{\label{fig:output}
The top ranked responsbile configuraion option from the
diagnosis report generated by \ourtool for the motivating example in
Section~\ref{}. 
}} %\vspace{-5mm}
\end{figure}


The diagnosis report indicates that a predicate affected by
the configuration option \CodeIn{maxsize} behaves dramatically
different between the observed (bad) run and all other good runs
kept in \ourtool's database; and suggests users may change its
value to obtain the desirable results.

This report not only identifies the possible responsible configuration
options, but also provides relevant contextual information to
help users understand why the program functions incorrectly,
then then fix the exhibited error.

Guided by the report, a user may further find the root
cause of this behavior. Figure~\ref{fig:example} shows
the relevant code snippet. When Randoop generates a new
test (line xxx, in the form of method-call sequence for Java programs),
it first compares the sequence length with a user-settable
configuration option \CodeIn{maxsize} (default value: 100), and
discards sequences that exceed this predefined max length.
The default value 100 of \CodeIn{maxsize} works well for most
programs, and only xxx\% of the generated sequences have been pruned.
However, for the subject Nanoxml, a xxx\% of the generted sequences
are discarded because the valid sequences are much longer than usual.

\ourtool captures such deviated behavior, and suggests users to
re-configure the \CodeIn{maxsize} option. As a result, when \CodeIn{maxsize}
is reset to a larger value, say 500, Randoop generates tests..

\begin{figure}[t]
\vspace{-2mm}
\small{//a configuration option to control a sequence's max length}
\vspace{-2mm}
\begin{CodeOut}
\begin{alltt}
int maxsize = readFromCommandLine();  //\textit{seed statement}

1.  public ExecutableSequence step() \ttlcb
2.    ExecutableSequence eSeq = createNewUniqueSequence();
3.    AbstractGenerator.currSeq = eSeq.sequence;
4.    eSeq.execute(executionVisitor);
5.    processSequence(eSeq);
6.    if (eSeq.sequence.hasActiveFlags()) \ttlcb
7.      componentManager.addGeneratedSequence(eSeq.sequence);
8.    \ttrcb
9.    return eSeq;
10. \ttrcb

11. private ExecutableSequence createNewUniqueSequence() \ttlcb
12.   Sequence newSequence = ...; //sequence creation step omitted
13.   if (newSequence.size() > maxsize) \ttlcb
14.     return null;
15.   \ttrcb
16.   if (this.allSequences.contains(newSequence)) \ttlcb
17.     return null;
18.   \ttrcb
19.   return new ExecutableSequence(newSequence);
20. \ttrcb
\end{alltt}
\end{CodeOut}
\tinystep
\vspace*{-3.0ex} \Caption{{\label{fig:example} 
Code excerpt from the Randoop automated test generator~\cite{randoop}.
%A forward slice computed by the traditional slicing algorithm~\cite{Horwitz:1988:ISU}
%from the seed statement includes statements 2, 3,
%4, 5, 6, 7, 9, 13, 14, 16, 17, and 19.
%By contrast, a thin slice~\cite{Sridharan:2007}
%only contains line 13.
}} %\vspace{-1.8mm}
\end{figure}

%this is a non-crashing
%error. There is no crashing points, memory core dump, and even a
%stack trace is not available. Second, there is no exact working
%state, Third, it is even difficult to come up with a testing oracle.

%$\blacksquare$ also lead to spurious bug reports.

%A users should not be expected and should not be expected to provide
%an testing oracle.

%To the best of our knowledge, this problem cannot be diagnosed by
%existing configuration error localization approaches. First, no
%crashing, Second, no failure, Third, no state

%not everyone has the technical skill to use this solution.
%And even if one does, using the xxx, finding, and moving xx
%counter past the end of the loop can be a tedious and annoying process.




%\vspace{1mm}
%\noindent \textbf{\textit{Our technique.}} 

\subsection{Diagnosing Configuration Errors}

The process of diagnosing configuration errors can be divided into two
separate tasks: identifying which specific configuration option is
responsible for the unexpected behavior, and determining how to fix that
configuration option. In this paper, we address the former task: finding
the root cause of a configuration error.
%, and leave the later task
%of fixing

\textbf{Our technique} contains three steps to 
link the erroneous behavior to specific responsible configuratio options.

\begin{itemize}
\item \textbf{Configurtion Propagation Analysis}. For
each user-settable configuration option, \ourtool
uses a lightweight dependence analysis technique, called Thin Slicing~\cite{Sridharan:2007},
to statically identify its affected predicate in the code.

\item \textbf{Configuration Behavior Profiling}. \ourtool
performs \textit{selective instrumentation} to capture the
dynamic behaviors of the test code when erroneous behavior
is observed.

\item \textbf{Configuration Deviation Analysis}.
When erroneous behavior is revealed, \ourtool looks up a
pre-built database, selects the most similar profiles, and
performs statistical analysis to identify which configuration
option's behavior deviates most between a good run and a bad.

\end{itemize}

Essentially, \ourtool reduces the problem of explaining a
configuration error to
identifying that the xxx is in a state similar to a xxx good state
on the reference computer for which a solution is known.
The output of \ourtool is a ranked list of
configurations that could possibly explain the why the program does not produce the desirable result. Those
configurations, if changed, may even fix the unexpected behavior.

%$\blacksquare$ We assume that the error is know, i.e., the error has
%been previously encountered ... 

Compared to existing approaches~\cite{Zeller:2002:ICC, Zhang:2003:PDS,
Rabkin:2011:PPC, Whitaker:2004:CDS, Attariyan:2010:ACT, Wang:2004:AMT}, \ourtool has
several notable features:

\begin{itemize}
\item \textbf{Fully-automated}.
\ourtool does not require a user to specify
\textit{when}, \textit{why} and \textit{how} the program fail. This is
different than many well-known automated debugging techniques such
as delta debugging~\cite{Zeller:2002:ICC} and dynamic slicing~\cite{Zhang:2003:PDS}.
Our technique also generates concise descriptions of the problem
with a given option, as is ranking possible diagnoses.

\item \textbf{Diagnose both non-crashing and crashing errors}.
Most of the existing techniques~\cite{Rabkin:2011:PPC,
Whitaker:2004:CDS, Attariyan:2010:ACT} focus exclusively on configuration errors
where the value of an option is wrong and this causes a program
to fail in a deterministic way with an error message, while
ignoring configuration problems that manifest themselves as
silent failures. By contract, \ourtool is capable to diagnose
both kinds of errors.

\item \textbf{Require no system support.} Our technique requires no alterations to
the JVM or standard library. This distinguishes our work from
competing techniques such as OS-level configuration
error troubleshooting~\cite{Whitaker:2004:CDS}.% or dynamic taint tracking~\cite{clause07july}.

\end{itemize}


We envision that \ourtool can be used by end-users or
administrators to identify the likely root cause of a configuration error.
When an end-user or administrator wishes to diagnose a
problem such as a crash or incorrect output, she
reproduces the problem on a \ourtool-enabled environment,
where \ourtool tracks the causal dependencies between
configuration options and the program behavior. 
When a configuration error happens, users can
use \ourtool to diagnose the problem based on the recorded profile.

Another critical component in \ourtool is the pre-built
profile database. We envision that this being done
by the developers at release. The profile database keeps
a set of correct execution traces xxxx, by running system
tests xxxx. $\blacksquare$ easy to build, even one trace
is enough in our experiments. we use manual examples. real programmers
should even feel easier.


%the software developers
%built the database initially, and other users can also enrich
%the database.
%, a
%the software developers provide a profile database, which users can use
%to query. The users can also enrich the database, providing their
%own examples. even a single run

%use the recorded profile message to query this database, perhaps via a web
%service.
 
%The core of our approach is to xxx. 
%We envision this being done by the developers
%at release time. 

%this technique xxx could be performed by
%the software developers; users would need only to provide
%the profiles xxx to back a diagnosis.


%We have developed a tool, called \ourtool, that uses xxx





%\vspace{1mm}
%\noindent \textbf{\textit{Evaluations.}} 

\subsection{Evaluation}

We evaluated \ourtool on \errors configuration errors
from \subjectnum real-world software, comprising
\crash crashing errors and \noncrash non-crashing errors.
Our results show that \ourtool identifies the correct
root causes of most configuration errors. xxxx top 3,
it lists the correct root cause as the top 3 xxx.
This allows \ourtool user to focus on a few specific configuration
options when deciding how to fix the problem. 
In addition, \ourtool takes less than xxx minutes for diagnosing
one configuration error, making it an attractive alternative
to manual debugging.

We also compared the effectiveness of \ourtool to
three existing approaches: statement-level profiling~\cite{Jones:2002}, method-level
invariant detection~\cite{Ernst:1999}, and dynamic tainting-based configuration
error diagnosis techniques~\cite{Rabkin:2011:PPC}. The experiment results show that
\ourtool significantly outperforms both statement-level profiling
and method-level invariant detection in diagnosis accuracy, suggesting
that focusing on the behavior of program predicates can be a
good choice. Compared to the dynamic tainting-based technique: ConfAnalyzer, \ourtool
produces accurate diagnosis information for 5 non-crashing errors that
ConfAnalyzer fails to diagnose, and output better info for 5 out of 9
crashing errors. 

Finally, we investigate the effecting of varying the comparison profiles
and increasing the context length in profiling. Our experimental results
show that ... $\blacksquare$

%\ourtool outputs an ordered list of probable root causes.
%Each entry in the list is a user-settle configuration option;
%our results show that \ourtool typically outputs
%the actual responsible configuration option as the top 3 in the list.

%By finding the
%needle in the haystack, \ourtool can be an attractive ...

%While xxx analysis takes a few minutes for a complex application,
%automated error diagnosis is still considerably faster and
%less labor-intensive than manual debugging or searching
%through other resources.

%$\blacksquare$ our technique is lightweighted.

%\vspace{1mm}
%\noindent \textbf{\textit{Contributions.}}

\subsection{Contributions}
This paper makes the following contributions:

\begin{itemize}
%\item \textbf{Problem.} To the best of our knowledge, we are the first to address
%the invalid thread access error detection problem for multithreaded GUI applications.

\item \textbf{Technique.} We present a technique to diagnose
software configuration errors. Our technique uses static analysis,
dynamic profiling, and statistical inference to link the
erroneous behavior to specific configuration options (Section~\ref{sec:technique}).


\item \textbf{Implementation.} We implemented our technique 
in a tool, called \ourtool, for Java software. Our tool implementation is publicly available at
\url{http://config-errors.googlecode.com} (Section~\ref{sec:implementation}).


\item \textbf{Evaluation.} We applied \ourtool to diagnose
\errors crashing and non-crashing configuration errors in \subjectnum
real-world Java software. The results
show the usefulness of the proposed technique (Section~\ref{sec:evaluation}).

\end{itemize}



