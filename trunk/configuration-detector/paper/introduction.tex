
\section{Introduction}
\label{sec:introduction}

Modern software is extraordinarily complex. Many applications have a large
number of configuration options that offer users great flexibility to
customize their behaviors. This flexibility has a cost: when something
goes wrong, diagnosing a configuration error can be both time-consuming
and frustrating. Technical support contributes 17\% of the total cost of ownership of
today's desktop computers~\cite{confevidence}, and troubleshooting misconfigurations
is a large part of technical support.

Software misconfigurations are often exhibited by an application unexpectedly terminating
(i.e., crashing errors) or producing an incorrect output (i.e., non-crashing errors). While an ideal application would always
output a helpful error message when such events occur, it is unfortunately
the case that such messages are often cryptic, misleading, or
even non-existent~\cite{Yin:2011:ESC, Attariyan:2010:ACT, Hubaux:2012, rangefix}.
Thus, users must search manuals, FAQs, and online forums to find potential
solutions.% to the problem. %$\blacksquare$ this process is frustrating..

\subsection{Motivating Example}
\label{sec:mot}

%This section describes a real scenario in which we used \ourtool to solve %a configuration problem. 
We next describe a real scenario in which we used \ourtool to solve a configuration problem. 
During the maintenance of the Randoop
automated test generation tool~\cite{randoop}, we received a ``bug report''
from a testing expert who had been using Randoop for quite a while.
The ``bug report'' indicated that Randoop terminated normally but
failed to generate any tests for the NanoXML~\cite{nanoxml} program. 
%The sympton is that
%Randoop terminates normally but does not generate any tests.
%on NanoXML, it 

Although the reported problem is deterministic and fully-reproducible,
this silent failure (or called, non-crashing error) is
particularly difficult to diagnose. Differing from
a crashing error, Randoop did not exhibit a crashing point, dump
 a stack trace, output an error message, or indicate suspicious program variables
with incorrect values. This makes many well-known
techniques such as dynamic slicing~\cite{Zhang:2003:PDS},
dynamic information flow tracking~\cite{Attariyan:2010:ACT},
failure trace analysis~\cite{Rabkin:2011:PPC} inapplicable.
In addition, for this scenario, there is no obvious testing oracle to check whether
Randoop behaves as desired, which further makes many
search-based fault isolation techniques such as Delta debugging~\cite{Zeller:2002:ICC}
ineffective. Moreover, the configuration and
inputs attached in the bug report have already been
minimized: if any part is removed, this behavior
is no longer triggered. 

Actually, this bug report is not real. It does not reveal a bug
in the Randoop code. Its root cause is simply because
the user forgot to set one configuration option.
Despite the simplicity of the solution, to the best of our knowledge, none of the
existing configuration error diagnosis techniques~\cite{Attariyan:2008:UCD, 
Yuan:2011:COC, Su:2009:AGP, Whitaker:2004:CDS, Wang:2004:AMT,
Attariyan:2010:ACT, Rabkin:2011:PPC} can be directly applied.


%We reproduce this problem in a \ourtool-enabled environment.
%It determines which predicates exemplify the buggy behaviors.


\begin{figure}[t]
\begin{CodeOut}
\begin{alltt} 
Suspicious configuration option: maxsize

It affects the behavior of predicate:
"newSequence.size() > GenInputsAbstract.maxsize"
(line 312, class: randoop.ForwardGenerator) 

This predicate evaluates to true:
  14.4\% of the time in normal executions (1315 observations)
  32.3\% of the time in the unexpected execution (2727 observations)

\end{alltt}
\end{CodeOut}
\vspace*{-15pt}
\Caption{{\label{fig:output}
The top ranked configuraion option in \ourtool's error report
for the motivating example in Section~\ref{sec:mot}. The complete error
report contains 31 options. For brevity, we omit the remaining ones.
}} %\vspace{-5mm}
\end{figure}
%, this predicate evaluates to true:   (1315 observation%run, it evaluates to true:  (2727 observations)s)

As an alternative, we can use our technique (and its tool implementation \ourtool)
to daignose and solve this problem. We first reproduce this
error in a \ourtool-enabled environment, and then let \ourtool analyze the
recorded execution trace to diagnose this root cause.
As a result, \ourtool produces
an error report (Figure~\ref{fig:output}) in the form of an ordered list of
suspicious configuration options that should be changed.
The error report in Figure~\ref{fig:output} suggests that
a configuration option named
\CodeIn{maxsize} is the most likely one.
The error report also provides relevant 
information to explain why: % about the reason why \CodeIn{maxsize} should be account for:
a predicate affected by \CodeIn{maxsize} behaves dramatically
different between the problematic execution and other correct executions 
as kept in \ourtool's database.


%Guided by the report, a user may further find the root
%cause of this behavior. 
To better understand the root cause, we show
the relevant code snippet in Figure~\ref{fig:example}.
Guided by the report, we can further find out
the root cause of Randoop's silent failure
is because when generating a new
test (line 100, in the form of a method-call sequence),
Randoop first compares its length with
\CodeIn{maxsize}'s value (default value: 100). If a
generated sequence exceeds this pre-defined limit,
Randoop discards the generated test to avoid length explosion.
For most programs, using \CodeIn{maxsize}'s default value works remarkably
well, and only 14.4\% of the generated sequences have been discarded.
However, for NanoXML, the generated sequences are
much longer than usual; using \CodeIn{maxsize}'s default value
results in 32.2\% of the generated sequences being discarded.
%are discarded because the generated sequences are much longer.  
\ourtool captures such deviated behavior, pinpoints the
\CodeIn{maxsize} option, and suggests users to reset its value.
Finally, this problem can be easily resolved after
the user change \CodeIn{maxsize} to a larger value, for example, 500.

\begin{figure}[t]
\vspace{-2mm}
{In class: randoop.ForwardGenerator}\\
%\small{//a configuration option to control a sequence's max length}
\vspace{-4mm}
\begin{CodeOut}
\begin{alltt}
int maxsize = readFromCommandLine(); //{initialize the option value}

99.  public ExecutableSequence step() \ttlcb
100.   ExecutableSequence eSeq = createNewUniqueSequence();
101.   AbstractGenerator.currSeq = eSeq.sequence;
102.   eSeq.execute(executionVisitor);
103.   processSequence(eSeq);
104.   if (eSeq.sequence.hasActiveFlags()) \ttlcb
105.     componentManager.addGeneratedSequence(eSeq.sequence);
106.   \ttrcb
107.   return eSeq;
108. \ttrcb

310. private ExecutableSequence createNewUniqueSequence() \ttlcb
311.   Sequence newSequence = ...; //sequence creation step omitted
312.   if (newSequence.size() > maxsize) \ttlcb
313.     return null;
314.   \ttrcb
315.   if (this.allSequences.contains(newSequence)) \ttlcb
316.     return null;
317.   \ttrcb
318.   return new ExecutableSequence(newSequence);
319. \ttrcb
\end{alltt}
\end{CodeOut}
\tinystep
\vspace*{-3.0ex} \Caption{{\label{fig:example} 
Simplified code excerpt from Randoop~\cite{randoop}
to explain the configuration problem as pointed
outed in Figure~\ref{fig:output}.
%A forward slice computed by the traditional slicing algorithm~\cite{Horwitz:1988:ISU}
%from the seed statement includes statements 2, 3,
%4, 5, 6, 7, 9, 13, 14, 16, 17, and 19.
%By contrast, a thin slice~\cite{Sridharan:2007}
%only contains line 13.
}} %\vspace{-1.8mm}
\end{figure}




%\vspace{1mm}
%\noindent \textbf{\textit{Our technique.}} 

\subsection{Diagnosing Configuration Errors}

Diagnosing configuration errors can be divided into two
separate tasks: identifying which specific configuration option is
responsible for the unexpected behavior, and determining how to fix that
configuration option. In this paper, we address the former task: finding
the root cause of a configuration error.
%, and leave the later task
%of fixing


%The goal of \textbf{our technique} is to help users find solutions to the configuration
%problems they are facing.  
\textbf{Our technique} contains three steps to 
link the erroneous behavior to specific responsible configuration options:

\begin{itemize}
\item \textbf{Configuration Propagation Analysis}. For
each configuration option, \ourtool
uses a lightweight dependence analysis, called thin slicing~\cite{Sridharan:2007},
to statically identify its affected predicates in the source code.

\item \textbf{Configuration Behavior Profiling}. \ourtool
\textit{selectively} performs instrumentation on those
affected predicates, to capture the
dynamic behaviors of the diagnosed code when erroneous behavior
is observed.

\item \textbf{Configuration Deviation Analysis}.
When erroneous behavior is revealed, \ourtool looks up a
pre-built database, selects some similar profiles
from know correct executions, and performs statistical analysis
to identify which configuration option's behavior deviates the most
between the recorded erroneous execution and the selected
correct executions.

\end{itemize}

The output of \ourtool is a ranked list of
configurations that could possibly explain why the program does not produce the desirable result. 
%Those
%configurations, if changed, may even fix the unexpected behavior.

Essentially, \ourtool reduces the problem of diagnosing a
configuration error to differencing the observed erroneous 
execution with similar but correct executions recorded before,
identifying the most deviated parts, and tracking the responsible
option for those deviated parts.
Compared to existing approaches~\cite{Zeller:2002:ICC, Zhang:2003:PDS,
Rabkin:2011:PPC, Whitaker:2004:CDS, Attariyan:2010:ACT, Wang:2004:AMT}, \ourtool has
several notable features:

\begin{itemize}
\item \textbf{It is fully-automated}.
\ourtool does not require a user to specify
\textit{when}, \textit{why} and \textit{how} the program fail. This is
different than many well-known automated debugging techniques such
as delta debugging~\cite{Zeller:2002:ICC}, information flow tracking~\cite{Attariyan:2010:ACT},
 and dynamic slicing~\cite{Zhang:2003:PDS}.
Besides, our technique also produces brief explanation on
why a given option is suspicious. 

\item \textbf{It can diagnose both non-crashing and crashing errors}.
Most of the existing techniques~\cite{Rabkin:2011:PPC,
Whitaker:2004:CDS, Attariyan:2010:ACT} focus exclusively on configuration errors
where the value of an option is wrong and this causes a program
to fail with a crashing point, an error message, or a stack trace, while
ignoring configuration problems that manifest themselves as
silent failures. By contract, \ourtool is capable to diagnose
both kinds of errors.

\item \textbf{It requires no OS-level support.} Our technique requires no alterations to
the JVM or standard library. This distinguishes our work from
competing techniques such as OS-level configuration
error troubleshooting~\cite{Whitaker:2004:CDS}.% or dynamic taint tracking~\cite{clause07july}.

\end{itemize}

An important component in \ourtool is the pre-built
database, which contains profiles
from the known correct executions. We envision that this database is
built by the software developers at release, and can be
further enriched by software users. %during integration testing.
In our experiments (Section~\ref{sec:evaluation}), we
found that such a database is quite easy to built even by running
a small number of illustrative examples from the user manual.

%\ourtool is designed to help users find solutions to the configuration
%problems they are facing.  


%When a configuration error happens, users can
%use \ourtool to diagnose the problem based on the recorded profile.

%\ourtool is run offline, once erroneous
%behavior has been observed. A \ourtool user reproduces
%the problem by executing the application while \ourtool attaches to
%the executing application processes and monitors xxx.

%We envision that \ourtool can be used by end-users or
%administrators t
%When an end-user or administrator wishes to diagnose a


%The instrumented version is deployed on the user side to collect program execution
%profiles (including both good and bad runs). When the user finds the program
%does not work as expected on a given input and configurations,
%he/she can invoke the Configuration Deviation Analysis component (Section~\ref{sec:analysis}) to
%diagnose the observed behavior. Our technique's output is a ranked list of
%configurations that could possibly explain the why the program does not produce the desirable result. Those
%configurations, if changed, may even fix the unexpected behavior.



%the software developers
%built the database initially, and other users can also enrich
%the database.
%, a
%the software developers provide a profile database, which users can use
%to query. The users can also enrich the database, providing their
%own examples. even a single run

%use the recorded profile message to query this database, perhaps via a web
%service.
 
%The core of our approach is to xxx. 
%We envision this being done by the developers
%at release time. 

%this technique xxx could be performed by
%the software developers; users would need only to provide
%the profiles xxx to back a diagnosis.


%We have developed a tool, called \ourtool, that uses xxx





%\vspace{1mm}
%\noindent \textbf{\textit{Evaluations.}} 

\subsection{Evaluation}

We evaluated \ourtool on \errors configuration errors
from \subjectnum configurable software, including
\crash crashing errors and \noncrash non-crashing errors.
Our results show that \ourtool identified the source
of the configuration errors as the top 3 root cause
for \topnum configuration errors.
This permits \ourtool user to focus on a few specific configuration
options when deciding how to fix the problem. 
In addition, \ourtool takes less than 1 minutes for diagnosing
one error, making it an attractive alternative
to manual debugging.

We also compared \ourtool to
full existing approaches: full slicing~\cite{},
statement-level profiling~\cite{Jones:2002}, method-level
invariant detection~\cite{Ernst:1999}, and dynamic information flow
tracking~\cite{Rabkin:2011:PPC}. The experimental results show that
\ourtool significantly outperformed both statement-level profiling
and method-level invariant detection in the diagnosis accuracy, suggesting
that focusing on the behavior of program predicates can be a
suitable abstraction. Compared to ConfAnalyzer~\cite{}, a dynamic
information flow tracking-based technique
which precisely tracks information flow at the bit level , \ourtool
produced accurate diagnosis information for the 5 non-crashing errors that
ConfAnalyzer failed to diagnose, and produced comparable results
for 5 out of the 9 crashing errors. 

Finally, we investigated the effects of varying the comparison profiles
from the pre-built database.
Our experimental results show that varying the
profile selection strategy can result in substantially different
diagnosis results, depending on the application being analyzed;
and the similarity-based selection strategy used in \ourtool outperforms
the other two comparison strategies.


%\ourtool outputs an ordered list of probable root causes.
%Each entry in the list is a user-settle configuration option;
%our results show that \ourtool typically outputs
%the actual responsible configuration option as the top 3 in the list.

%By finding the
%needle in the haystack, \ourtool can be an attractive ...

%While xxx analysis takes a few minutes for a complex application,
%automated error diagnosis is still considerably faster and
%less labor-intensive than manual debugging or searching
%through other resources.

%$\blacksquare$ our technique is lightweighted.

%\vspace{1mm}
%\noindent \textbf{\textit{Contributions.}}

\subsection{Contributions}
This paper makes the following contributions:

\begin{itemize}
%\item \textbf{Problem.} To the best of our knowledge, we are the first to address
%the invalid thread access error detection problem for multithreaded GUI applications.

\item \textbf{Technique.} We present a technique to diagnose
software configuration errors. Our technique uses static analysis,
dynamic profiling, and statistical analysis to link the
erroneous behavior to specific configuration options (Section~\ref{sec:technique}).


\item \textbf{Implementation.} We implemented our technique 
in a tool, called \ourtool, for Java software (Section~\ref{sec:implementation}). Our tool implementation is publicly available at
\url{http://config-errors.googlecode.com}. 


\item \textbf{Evaluation.} We applied \ourtool to diagnose
\errors configuration errors in \subjectnum
configurable Java software. The results
show the usefulness of the proposed technique (Section~\ref{sec:evaluation}).

\end{itemize}



