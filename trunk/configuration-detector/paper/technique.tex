\section{Technique}
\label{sec:technique}

We model configurations as a set of key-value pairs, where
the keys are strings and the values have arbitrary type. This is
a common abstraction offered
by the POSIX system environment, the Java Properties API,
and the Windows registry.


\subsection{Overview}



%put to the profiling section?
%We also want to track caused effects of misconfigurations 

%For non-deterministic errors, \ourtool could potentially leverage one of
%several deterministic replay systems~\cite{Huang:2010:LLD}
%that can capture a buggy non-deterministic
%execution and faithfully reproduce it for late analysis.

Figure~\ref{fig:workflow} sketches the high-level workflow of our technique.
Our technique takes as input a Java program and its configuration options.
It first performs a propagation analysis to identify
the affected predicates for each configuration option (Section~\ref{sec:prop}).
After then, our technique \textit{selectively} instruments
the program only on the affected predicates, linking each configuration
to its affected predicates (Section~\ref{sec:profiling}).
After the user run the failure-revealing inputs and configuration
on the instrumented program, our technique obtains an execution profile.
Our technique further uses the obtained execution profile
to compare with the existing profiles in the pre-built database,
diagnose the root cause of the error (Section~\ref{sec:analysis}).

The error report produced by our technique is a ranked list
of suspicious configuration options that may caused the exhibited problem.


%\ourtool tracks dependencies introduced by both data and
%control flow. It propagates dependencies among xxx


\begin{figure*}[!]
  \centering
  \includegraphics[scale=0.600]{architecture}
  \vspace*{-2.0ex}\caption {{\label{fig:workflow} The workflow of our configuration error diagnosis technique.
Phases ``Instrument'' and ``Run'' correspond to the Configuration Behavior Profiling step in Section~\ref{sec:profiling}.
The other two phases: ``Propagation Analysis'' and ``Deviation Analysis'' correspond to steps in Section~\ref{sec:prop} and Section~\ref{sec:analysis}, respectively.
}}
\end{figure*}

\subsection{Configuration Propagation Analysis}
\label{sec:prop}

Given a configuration option, this step statically determines its affected program
elements in the abstraction level of \textit{predicates}. Using predicates
as the abstraction level makes our technique focus on program data flows, instead
of all executed statements.

A predicate is a boolean expression in the source code ...

%In traditional program slicing techniques~\cite{}.
In traditional taint tracking for security purposes, control
flow monitoring are often ignored to improve performance.
With \ourtool, however, we have found that monitoring control flow
dependencies is essential since they propagate the majority of
configuration-related control flow. $\blacksquare$


To identify those affected program predicates, a straightforward way is using program
slicing~\cite{Horwitz:1988:ISU} to compute a forward slice from the assignment statement of the given
configuration option. Unfortunately, traditional slicing includes all statements that
\textit{may} affect a point of interest and often grows too large.

Our technique uses thin slicing~\cite{Sridharan:2007} as a manner to include
\textit{only} statements that are directly affected by the configuration.
We illustrate the advantages of using thin slicing 
 in Figure~\ref{fig:example}.





\textbf{justification of focusing on program control-flow
alternation, rather than values inside.}

The key insight here is that, having no knowledge of
what inputs a user would provide, traditional slicing
captures every single detail of the execution, much
of which is not needed at all by the client.
However, if the provided input changes the workflow,
instead of all data flow into it. 

Take the code excerpt in Figure~\ref{fig:example} as an example.
When Randoop is used to generate tests for different inputs (here,
input mean programs under test), the created tests (method-call
sequence at line 12) would be dramatically different.
However, for similar inputs, the program execution flow should
be similar.

\textbf{justification of focusing on thin slicing, rather
than all affected parts}
Traditional slicing does not distinguish flows along
pointers from flows along values.

Thin slicing is a technique that focuses on statements
that flow values to the seed, ignoring the uses of
base pointers.

A thin data dependence graph has exactly
the same set of nodes as its corresponding data dependence
graph. However, for an access \CodeIn{v.f}, the base pointer value
in \CodeIn{v} is not considered to be used. 

This property makes thin slicing
especially attractive.

Also Take the code in Figure~\ref{fig:example} as an example.
Use traditional slicing to compute all affected statement,
the configuration option \CodeIn{maxsize} would affect
branching statement 6, 13, and 16. However, the
statements 6 and 13 are incorrectly identified, since
whether a sequence has an active flag (line 6) or
whether a sequence has been executed before (line 16)
has nothing to do with the \CodeIn{maxsize} configuration option.
In fact, there is another configuration option $\blacksquare$
that affect line 6.

% improves the relevance
%of the slice by focusing on the statements that compute
%and copy a value to the seed.

By separating pointer computations from value flow,
this approach naturally connects configurations and its
directly affected statements.


\subsection{Configuration Behavior Profiling}
\label{sec:profiling}

Our goal is to capture sufficient information to allow users to reason
about the causal effects of configurations and how they relate to
predicates that test system health, while also minimizing performance impact
on foreground applications and keeping the amount of causal state manageable.

\ourtool expresses system health as the results of executing a set of predicates.

\ourtool lets users learn from each others' experience: as \ourtool sees more correct
solutions to a problem, it has a greater database of potential solutions to
a problem.

\ourtool assigns a unique signature for each predicate in the program.

associate a predicate with its affecting configuration options during instrumentation.

For each configuration, after obtaining its affected predicates, our technique
instruments the program \textit{only} on those affected predicates. The instrumentation
code keeps the results of how an affected predicate evaluates at runtime. Besides keeping
the predicate evaluation results, the instrumentation code also keeps
the calling context information.

Take the code excerpt in Figure~\ref{fig:example} as an example, the affected
predicate of configuration option \CodeIn{maxszie} is at line 13. Thus, our technique
only instruments line 13, producing the instrumented
code as follows (the instrumentation code is highlighted by underline):


\begin{CodeOut}
\begin{alltt}
11. private ExecutableSequence createNewUniqueSequence() \ttlcb
12.   Sequence newSequence = ...; //sequence creation step omitted
      \underline{Instrumenter.beforeEvaluation(maxSize);}
      \underline{Instrumenter.saveCallingContext(maxSize);}
13.   if (newSequence.size() > maxsize) \ttlcb
        \underline{Instrumenter.evaluateToTrue(maxSize);}
14.     return null;
      ...
20. \ttrcb
\end{alltt}
\end{CodeOut}

The helper class \CodeIn{Instrumenter} saves the calling context (i.e.,
method-call chains from the main entry), and the predicate evaluation result.


\subsection{Configuration Deviation Analysis}
\label{sec:analysis}

To diagnose a configuration error, \ourtool runs each predicate, traces its
output, and generates its dependency set. It compares the dependency sets
with those generated on the reference compute for each known bug. To compare
dependency sets, the tool calculates the edit distance between the sets
for each predicate. For each known bug, it sums the edit
distances to calculate the similarity between the state of the sick
computer and the state of the reference computer. It identifies the bug
with the lowest total as the most likely diagnosis; in the case of ties,
it reports all tied bugs as being equally likely to be the root cause.

rank by the distance


\ourtool is designed to be used by system administrators and end-users when they
encountered a suspected configuration error that they do not
know how to fix. Once a \ourtool user encounter an unexpected
behavior, she can reproduce the problem on a \ourtool-enabled environment,
where \ourtool monitors the program execution and captures
the the causal dependencies between
configuration options and the program behavior. \ourtool
further uses the obtained execution trace to query the pre-built database
to find similar but correct known traces. Further, based on
the deviation analysis between a recorded trace and other similar traces,
\ourtool identifies the likely root cause of a configuration error and
outputs a ranked list of suspicious options.

\subsubsection{Selecting Similar Profiles for Comparison}

A database can contain profiles produced by dramatically
different tasks. It only makes sense to compare the buggy profile
with a similar but correct task.

\ourtool uses the pattern of observed behaviors of program predicates
to diagnose configuration errors. Using this approach, \ourtool
executes the program on the erroneous inputs with the
corresponding configuration and aggregates the result of
each instrumented predicate as a vector $S_{current}$ = $<float, float, ..>$.
\ourtool then compares $S_{current}$ with a set of
vectors in the pre-built database, where each vector is generated
by running xxxx (correct).

Intuitively, each vector is captures the system behavior..

\ourtool automatically searches for solutions to a configuration
problem.

\ourtool chooses the vector that is most similar to $S_{current}$ as
the most likely diagnosis for the error.

\ourtool uses the interproduction distance as a similarity metric, we will
refer it as the xxx method.


\subsubsection{Diagnosing Non-Crashing Errors}

Introduce the metric for ranking configurations, and its
statistical meaning,e g., balance 2 parts

\subsubsection{Diagnosing Crashing Errors}

A stack trace is available. A trace produced from a crashing error is not complete

\subsubsection{Filtering Execution Noises}

remove some off-by-one


Finally, needs to average the results
\subsubsection{Rank the Output}

\subsection{Discussions}

The fundamental differences between inputs and options. A configuration
option is fixed before input.

Why cannot use unit test to achieve the trace? since it is incomplete

Configuration customize the overall program behavior, when inputs is a specific

why cannot delta debugging? no working state, no predicate

why not store failed traces in the database? (developers
cannot anticipate the error behaviors). A broader question is which
informatin should be stored in the database for comparison, now, we use
profiling info, what about statement info? invariant? they all
summarize valuable info.

Why dynamic slicing is not usable? No seed statement, and great overhead. Using JSlicer incurs
a great overhead. It needs to track every instruction and
perform synchronization when dependence graph is updated.

Our technique can be seen as a way to reduce overhead,
including selective profiling, and static pre-processing
techniques.

