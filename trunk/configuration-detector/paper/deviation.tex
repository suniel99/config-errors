\ourtool starts error diagnosis after obtaining the execution profile from
an undesired execution. It selects similar
profiles from known correct executions (Section~\ref{sec:similar}), compares
each selected profile with
the undesired one to identify the most behavioral-deviated predicates
(Section~\ref{sec:deviation}), and then determines
the likely root cause options (Section~\ref{sec:linking}).


\subsubsection{Selecting Similar Execution Profiles for Comparison}
\label{sec:similar}

\ourtool's database contains a number of
profiles from known correct executions.  These execution profiles 
can be dramatically different from another.  To avoid reporting irrelevant
differences when 
determining how and why the observed execution profile behaves
differently from the correct ones, \ourtool first
compares the undesired profile with the existing
correct profiles, then selects a set of similar ones
as the basis of diagnosis.

Given an execution profile $e$, \ourtool first aggregates
the collected predicate profiles into a $n$-dimensional
vector $v_e$ =$\langle r_{e1}, r_{e2}, ..., r_{en}\rangle$, where $n$
is the number of all possible predicate profiles in the program
and each $r_{ei}$ is a ratio representing how often the $i$-th predicate
profile evaluated to true at runtime.
If a predicate has never been executed in an execution,
\ourtool uses 0 as its ratio. 

\ourtool computes the similarity of two executions profiles $e$ and $f$
by computing the standard cosine similarity from information retrieval~\cite{Witten96managinggigabytes}
of $v_{e}$ and $v_{f}$.

\vspace{-3mm}

{\small{
\[
\|Similarity|(e, f) = \|cos\_sim|(v_{e}, v_{f}) = \frac{\sum_{i = 1}^{n}r_{ei} \times r_{fi}}
{\sqrt{\strut \sum_{i = 1}^{n}r_{ei}^2} \times \sqrt{\sum_{i = 1}^{n}r_{fi}^2}}
\]
}}

\vspace{-3mm}
%\todo{Weird enough that why two sqrt symbols above are not the same size?}

This similarity metric basically compares two execution profiles based on
 control flow taken (approximated by how often a predicate is evaluated to
true). Its value ranges from 0 meaning completely different, to 1 meaning almost the same, 
and in-between values indicating intermediate similarity.


%$\blacksquare$ put the intuition here. dot production? why ignore
%numbers? similarity? control flow.

%I am re-inventing wheels below, just use cosine similarity above

%\ourtool computes the distance between two execution profiles $t_1$ and $t_2$ using
%the following equations:

%{\small{
%\[
%\|Distance|(t_1, t_2) = 1 - \frac{\sum_{i = 1}^{n}Delta(r_{1i}, r_{2i})^2}{n}
%\]

%\[
%\|Delta|(r_1, r_2) = 
%\left\{
%\begin{array}{l l l l}
%  0 & \ \mbox{if $r_1$ = \CodeIn{N/A} and $r_2$ $\neq$ \CodeIn{N/A}}\\
%  0 & \ \mbox{if $r_1$ $\neq$ \CodeIn{N/A} and $r_2$ = \CodeIn{N/A}}\\
%  1 & \ \mbox{if $r_1$ = \CodeIn{N/A} and $r_2$ = \CodeIn{N/A}}\\
%  \CodeIn{min}\{r_2/r_1, r_1/r_2\} & \; \mbox{otherwise}\\ \end{array} \right.
%\]
%}
%}


%$\blacksquare$ need to illustrate why use the distance metric.
%\todo{Also explain/justify.  Give intuition for the metric.}


A crashing error sometimes happens soon after the program is launched,
the resulting execution profile is much smaller than
most correct execution profiles.
To avoid comparing un-executed predicates,
when diagnosing a crashing error, \ourtool
reduces each correct execution profile by only retaining
the predicates executed by the crashing profile, and then
uses the reduced profile for comparison. 

%since many predicates in the program are not executed.
%Using the $Similarity$ metric to compare a correct execution profile from the database
%with a crashing profile, it is unlikely for \ourtool to find similar ones.
%To facilitate comparison, %avoid comparing those un-executed predicates,

Given an undesired execution profile,
\ourtool selects all execution profiles (or the reduced profiles for
a crashing error) from the database
with a $Similarity$ value above a threshold (default value: 0.9, as used in our
experiments).

%In addition,
%when diagnosing a crashing error, such chopped execution profiles will replace the original profiles
%(in the database) in the following steps.

%(also used
%the chopped profile for next steps). Doing so, \ourtool
%avoids comparing irrelevant differences in a correct execution
%profile which a crashing profile even has not reached yet.

%An execution profile from a crashing error

%\todo{Mike does not understand this paragraph.}
%For the execution profile produced in a crashing error, \ourtool 
%chops a correct execution profile from the database by only remaining the predicate
%profiles covered by the undesired profile. \ourtool performs
%this simple preprocessing because a crashing error $\blacksquare$
%often produces an incomplete execution profile, and it is unlikely
%to find a similar one from the database.


\subsubsection{Identifying Deviated Predicates}
\label{sec:deviation}
Our
automated error diagnosis approach compares an undesired execution profile with a set
of \textit{similar} and \textit{correct} execution profiles. 
%\todo{I like the following sentence.  This intuition should appear in the
%  introduction as well.  The introduction should say what we do (it does
%  this) and also give a hint as to the approach.}
The behavioral differences in the recorded predicates provide evidence for what parts of a program might be
incorrect and why. %This helps to further reason about its root cause.

\ourtool 
characterizes the dynamic behavior of a predicate by 
how often it is
evaluated (i.e., the number of 
observed executions), and how often it evaluated to true (i.e., the true ratio).
The true ratio is more important, but it is less dependable the fewer times
the predicate has been evaluated.

% Ideally, we would like to have a metric
% that captures a predicate's desired behavior
% from the known correct execution profiles, but is robust enough
% to tolerate noise so that it does
% not overfit a specific correct execution profile.
% Looking more closely, we found that although
% the general execution control flows (approximated by
% a predicate's true ratio) may be similar for many 
% execution profiles, the absolute execution
% number of a predicate may largely depend on the given input, % and
% varying greatly across executions. On ther other hand, for
% some predicates, they may have
% only been executed very few times but the true ratios
% are dramatically different across executions.
% 
% %that in two execution profiles, a predicate's is only executed
% %in very few times but the true ratios are 
% %As another example, some program has preprocessing... $\blacksquare$
% 
% 
% %Thus, if we can generate a signature that
% %captures the execution path of a predicate, we should be
% %able to more precisely identify a configuration error
% 
% Thus, we need a metric that can characterize
% a predicate's behavior with high sensitivity, meaning the predicate's
% behavior (i.e., true ratio) is correlated with
% %ratio accounts for
% the execution results (i.e., correct or undesired).
% But we also want
% high specificity, meaning a predicate's behavior
% is more representative if it is observed
% in more executions.
% %behavior should not be
% %mis-characterized only based on a small number of
% %observed executions.

We define the following
$\phi$ metric, which
combines sensitivity (informally, the need for multiple observations) and
specificity (informally, the true ratio) in a standard way by computing their
harmonic mean.

\vspace{-4mm}

{\small{
\[
\|\phi|(e, p) = \frac{2}{{1}/{\|trueRatio|(e, p)} + {1}/{\|totalExecNum|(e, p)}}
\]
}}

\vspace{-3mm}

In $\phi(e, p)$, $\|trueRatio|(e, p)$ is the ratio of executions of the predicate $p$ that
evaluated to true in $e$, and $\|totalExecNum|(e, p)$ is the the total
number of executions of predicate $p$ in $e$.
To smooth corner cases, if a predicate $p$ is not executed in $e$, i.e., 
$\|totalExecNum|(e, p) = 0$, then $\phi(e, p)$ returns 0; and if a predicate $p$'s true ratio is 0, i.e., $\|trueRatio|(e, p) = 0$, then $\phi(e, p)$ returns
$1/\|totalExecNum|(e, p)$.


The following $Deviation$ metric
characterizes a predicate $p$'s deviation across two execution
profiles $e$ and $f$. A larger $\|Deviation|$ value indicates that the
behavior is more different. % between $e$ and $f$.

\vspace{-2mm}

{\small{
\[
\|Deviation|(p, e, f) = |\phi(e, p) - \phi(f, p)|
\]
}}
\vspace{-4mm}

% %It balances a predicate's evaluation result and the total number of executions.
% \todo{the basic idea is here, but this paragraph needs re-write}
% The definitions of metrics $\phi$ and $Deviation$ have several desirable properties
% in characterizing a predicate $p$'s behavior. Clearly, $\|trueRatio|(e,p)$
% is a value between 0 and 1 and $\|totalExecNum|(e, p)$ is a value greater than 1; increasing
% either value while keeping the other one unchanged increases $\phi(e, p)$.
% \ourtool focuses on monitoring a program's control flow, which is also
% reflected in the definition of $\phi$. In $\phi(e, p)$,
% the value of $\|trueRatio|(e, p)$ is more important than the value
% of $\|totalExecNum|(e, p)$
% in deciding $\phi(e, p)$; since in theory: $1/\|trueRatio|(e, p) \geq 1/\|totalExecNum|(e, p)$, but
% In practice, 
Often $1/\|trueRatio|(e, p) \gg 1/\|totalExecNum|(e, p)$, and
then the value of $\|Deviation|(p, e, f)$ depends primarily on $p$'s
true ratio change between execution profiles $e$ and $f$. 

% \todo{A few more properties I do not known how (and whether need) to write down:
% (1) when \|totalExecNum|(e, p) increases to some extent, the change Deviation(p)
% becomes ignorable, and the change of trueRatio dominates the Deviation value. (2)
% }

%and $\|Deviation|(p)$'s value is more sensitive to the change of $\|trueRatio|(e, p)$.
%When computing $\|Deviation|(p)$ across two executions,
%if $p$'s execution number is unchanged, a small change of $p$'s true ratio
%would lead to a non-trivial $Deviation$ value. On the other hand,
%if $p$'s true ratio is unchanged, the change in $p$'s execution number
%leads to a less noticeable $Deviation$ value.

%Intuitively, for two predicates $p_1$ and $p_2$, if they have the same
%he true ratio but $p_1$ has been observed in more executions, we
%should have more confidence in its statistical significance. $\blacksquare$
%(the wording here is bad)


\ourtool computes the $Deviation$ value for each predicate $p$
appearing in two execution profiles $e$ or $f$, and
ranks them in decreasing order. 

%After comparing the undesired execution profile with one selected correct execution profile,
%\ourtool ranks all observed predicate profiles in
%a decreasing order based on the computed $Deviation$ value.




%\subsubsection{Filtering Execution Noises}
%remove some off-by-one


\subsubsection{Linking Predicates to Root Causes}
\label{sec:linking}

% and outputs a ranked list of suspicious
%configuration options.

\ourtool links the behaviorally-deviated
predicates to their root cause configuration options
by using the results of thin slicing (computed by the Configuration Propagation
Analysis step in Section~\ref{sec:prop}).
\ourtool identifies 
the affecting configuration options for each deviated predicate,
and treats the configuration option
affecting a higher-ranked deviated predicate as the more likely
root cause. If a predicate is affected by multiple
configuration options, \ourtool prefers options whose initialization
statements are \textit{closer} to the
deviated predicate (in terms of breath-first search
distance in the dependence graph of thin slicing).
This heuristic is based on the intuition that statements closer to the
predicate seem more likely to be relevant to its behavior.

%Hence, we assume the user gradually explores statements of
%increasing distance from the suspicious predicate until
%the desired statements 
%are found; a breadth-first
%search of the dependence graph simulates this strategy.


When multiple correct execution profiles are selected for comparison,
\ourtool first produces a ranked list of root cause
configuration options for each comparison pair, and then outputs
a final list by using majority voting over all ranking lists.
In the final ranking list, one configuration option ranks higher
than another if it ranks higher in more than half of the ranking lists.
%However, according to Arrow's impossibility theorem~\cite{Fishburn1970103},
%no rank order voting system can produce a non-cyclic ranking while also
%meeting a specific set of criteria (e.g., getting more than half the voters).
Our implementation breaks possible cycles by arbitrarily ranking the
involved options, but this did not occur in our experiments. 

In the final output report (e.g., Figure~\ref{fig:output}), \ourtool 
generates a brief explanation for each behaviorally-deviated predicate
by showing the difference of the predicate's true ratio between
all correct executions from the database and the undesired execution.

%The predicate's true ratio in normal runs is calculated by summing up
%all observations from the database, and its true ratio in the undesir

%\todo{Should add 1 more sentence to say how to generate the explanation?
%In particular, the true ratio in the normal runs are averaged from all
%good runs. need to mention that?}

%\todo{Such a ranking can have cycles.  Does the implementation suffer this
%  problem?}

