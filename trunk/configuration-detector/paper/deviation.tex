\ourtool starts error diagnosis after obtaining the execution profile from
an undesired execution. It selects similar
profiles from known correct executions (Section~\ref{sec:similar}), compares
each selected profile with
the undesired one to identify the most behavioral-deviated predicates
(Section~\ref{sec:deviation}), and then determines
the likely root cause configuration options (Section~\ref{sec:linking}).


\subsubsection{Selecting Similar Execution Profiles for Comparison}
\label{sec:similar}

\ourtool's database contains a number of
profiles from known correct executions.  These execution profiles 
can be dramatically different from another.  To avoid reporting irrelevant
differences when 
determining how and why the observed execution profile behaves
differently from the correct ones, \ourtool first
compares the undesired profile with the existing
correct profiles, then selects a set of similar ones
as the basis of diagnosis.

Given an execution profile $t$, \ourtool first aggregates
the observed predicate profiles into a $n$-dimensional
vector $v_{t}$ =$\langle r_{t1}, r_{t2}, ..., r_{tn}\rangle$, where $n$
is the number of all possible predicate profiles in the program
and each $r_{ti}$ is a ratio representing how often the $i$-th predicate
profile evaluated
to true at runtime in $t$. For each execution profile, if a predicate is not executed,
\ourtool uses \CodeIn{N/A} as its ratio.


\ourtool computes the distance between two execution profiles $t_1$ and $t_2$ using
the following equations:

{\small{
\[
\|Distance|(t_1, t_2) = 1 - \frac{\sum_{i = 1}^{n}Delta(r_{1i}, r_{2i})^2}{n}
\]

\[
\|Delta|(r_1, r_2) = 
\left\{
\begin{array}{l l l l}
  0 & \ \mbox{if $r_1$ = \CodeIn{N/A} and $r_2$ $\neq$ \CodeIn{N/A}}\\
  0 & \ \mbox{if $r_1$ $\neq$ \CodeIn{N/A} and $r_2$ = \CodeIn{N/A}}\\
  1 & \ \mbox{if $r_1$ = \CodeIn{N/A} and $r_2$ = \CodeIn{N/A}}\\
  \CodeIn{min}\{r_2/r_1, r_1/r_2\} & \; \mbox{otherwise}\\ \end{array} \right.
\]
}
}


$\blacksquare$ need to illustrate why use the distance metric.
\todo{Also explain/justify.  Give intuition for the metric.}

Given an undesired execution profile, \ourtool selects all execution profiles from the database
with a distance below a threshold (default value: 0.3 as used in our
experiments).

\todo{Mike does not understand this paragraph.}
For the execution profile produced in a crashing error, \ourtool 
chops a correct execution profile from the database by only remaining the predicate
profiles covered by the undesired profile. \ourtool performs
this simple preprocessing because a crashing error $\blacksquare$
often produces an incomplete execution profile, and it is unlikely
to find a similar one from the database.


\subsubsection{Identifying Deviated Predicates}
\label{sec:deviation}


Our
automated error diagnosis approach compares an undesired execution profile with a set
of \textit{similar} and \textit{correct} execution profiles. 
%\todo{I like the following sentence.  This intuition should appear in the
%  introduction as well.  The introduction should say what we do (it does
%  this) and also give a hint as to the approach.}
The behavioral differences in the recorded predicates provide evidence for what parts of a program might be
incorrect and why. %This helps to further reason about its root cause.

Given a predicate, there are two primary variables to
characterize its dynamic behavior: the number of the
observed executions, and the ratio of it being evaluated to true.


Ideally, we would like to have a metric that
is precise enough to capture the desirable predicates' behaviors
from the known correct execution profiles, but is robust enough and
able to tolerate small noises so that it does
not overfit a specific execution profile.
Looking more closely, we found that although
the general execution control flows may be similar for many 
execution profiles under similar inputs, the absolute execution number of the same predicate
can vary greatly across executions, since it may
largely depend on the given inputs. Moreover, for
some predicates, it is entirely possible they are
only executed very few times but the true ratios
are dramatically different in difference execution profiles.

%that in two execution profiles, a predicate's is only executed
%in very few times but the true ratios are 
%As another example, some program has preprocessing... $\blacksquare$


%Thus, if we can generate a signature that
%captures the execution path of a predicate, we should be
%able to more precisely identify a configuration error

Thus, we are looking for metrics that can identify
predicates with high sensitivity, meaning the predicate's true
ratio accounts for error. But we also want
high specificity, meaning predicates that do not mis-characterize
a predicate's behavior only based on a small number of
observed executions. To do so, we define the following
$\phi$ metric by using a standard way to
combine sensitivity and specificity to compute their
harmonic mean; this metric prefers high scores in both dimensions. 

\vspace{-3mm}

{\small{
\[
\|\phi|(t, p) = \frac{2}{{1}/{\|trueRatio|(t, p)} + {1}/{totalExecNum(t, p)}}
\]
}}

\vspace{-3mm}

In $\phi(t, p)$, $trueRatio(t, p)$ is a function that returns the ratio of the predicate $p$ being
evaluated to true in execution profile $t$, and $totalExecNum(t, p)$ is a function
that returns the total number of predicate $p$ being executed in execution profile $t$.

Metric $\phi$ has some good properties in characterizing the
runtime behaviors of a predicate $p$.$\blacksquare$.
It balances a predicate's evaluation result and the total number of executions.
Intuitively, for two predicates $p_1$ and $p_2$, if they have the same
the true ratio but $p_1$ has been observed in more executions, we
should have more confidence in its statistical significance. $\blacksquare$
(the wording here is bad)

To capture behavioral difference of a predicate profile $p$
across executions, we devise the  $Deviation$ metric
to characterize its deviation degree between two execution profiles $t_1$ and $t_2$:

\vspace{-2mm}

{\small{
\[
\|Deviation|(p) = |\phi(t_1, p) - \phi(t_2, p)|
\]
}}
\vspace{-4mm}

After comparing the undesired execution profile with one selected correct execution profile,
\ourtool ranks all observed predicate profiles in
a decreasing order based on the computed $Deviation$ value.




%\subsubsection{Filtering Execution Noises}
%remove some off-by-one


\subsubsection{Linking Predicates to Root Causes}
\label{sec:linking}


\ourtool finally links the behavioral-deviated
predicate profiles to their root causes, and outputs a ranked list of suspicious
configuration options.

\ourtool consults the results of thin slicing (computed in the propagation
analysis phase, Section~\ref{sec:prop}) to determine which
configuration options affect each deviated predicate.
It identifies the configuration option
affecting the highest ranked predicate profile as the most likely
root cause.  It uses a simple heuristic to break ties.
The heuristic prefers configuration options whose initialization
statements are \textit{closer} to the
crashing point. Intuitively, statements closer to the
crashing point seem more likely to be relevant to its behavior.
Hence, we assume the user gradually explores statements of
increasing distance (defined by the dependence graph of thin slicing)
from the crashing point until the desired statements (where a configuration
option is initialized) are found; a breadth-first
search of the dependence graph simulates this strategy.


When multiple correct execution profiles are selected for comparison,
\ourtool first produces a ranked list of suspicious
configuration options for each comparison pair, and then outputs
a final list by using majority voting over all ranking lists.
In the final ranking list, one configuration option ranks higher
than another if it ranks higher in more than half of the ranking lists.
However, according to Arrow's impossibility theorem~\cite{Fishburn1970103},
no rank order voting system can produce a non-cyclic ranking while also
meeting a specific set of criteria (e.g., getting more than half the voters).
Our implementation breaks possible cycles by arbitrarily ranking the
involved options, but our experiments did not use this capability.

%\todo{Such a ranking can have cycles.  Does the implementation suffer this
%  problem?}

%as being equally likely to be the root cause.

