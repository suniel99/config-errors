\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

Our evaluation answers the following research questions:

%\todo{I would organize the research questions as:
%\begin{itemize}
%\item
%How effective is \ourtool?
%\begin{itemize}
%\item
%in absolute terms
%\item
%compared to other tools
%\end{itemize}
%This can include run time
%\item
%Discussion of internal implementation choices of \ourtool
%\end{itemize}
%}

%\todo{Give cross-references to sections that address these.}

\begin{itemize}
\item How effective is \ourtool in error diagnosis? \ourtool's effectiveness can be reflected by:
\begin{itemize}
  \item the absolute ranking of the actual root cause in \ourtool's output (Section~\ref{sec:accuracy}).
  \item the time cost in error diagnosis (Section~\ref{sec:performance}).
  \item comparison with an existing configuration error diagnosis technique (Section~\ref{sec:confanalyzer}).
  \item comparison with two existing fault localization techniques (Section~\ref{sec:comparison}).
\end{itemize}
\item What are the effects of using full slicing~\cite{Horwitz:1988}
 rather than thin slicing~\cite{Sridharan:2007} to identify
the affected predicates, and the effects of varying comparison execution profiles (Section~\ref{sec:choices})?
These are two internal design choices. % in \ourtool.
\end{itemize}


\vspace{-1mm}
\vspace{-1mm}

\subsection{Subject Programs}

\vspace{-1mm}

We evaluated \ourtool on \subjectnum Java programs shown
in Figure~\ref{tab:subjects}.
Randoop~\cite{PachecoLET2007} is an automated test generator
for Java programs. Weka~\cite{weka} is a toolkit that implements
machine learning algorithms. Our evaluation
only uses its decision tree module. JChord~\cite{jchord}
is a program analysis platform that enables users to design, implement,
and evaluate static and dynamic program analyses for Java.
Synoptic~\cite{synoptic} mines a finite state machine
model representation of a system from logs.
Soot~\cite{soot} is a Java optimization framework for analyzing and transforming Java bytecode.


%\vspace{-1mm}

\smallsqueeze
\subsubsection{Configuration Errors}

\vspace{-1mm}

\begin{figure}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.50\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Config Options & \#Profiles\\
 \hline
 \hline
 Randoop (1.3.2) & 18587 & 57 & 12\\
 Weka Decision Trees (3.6.7) & 3810 & 14 & 12\\
 JChord (2.1) & 23391 &  79 & 6 \\
 Synoptic (trunk, 04/17/2012) & 19153 & 37 & 6\\
 Soot (2.5.0) & 159273 & 49 & 16 \\
\hline
\end{tabular}
}
\vspace{-2mm}
%\todo{Give a date for Synoptic}
\Caption{{\label{tab:subjects} Subject programs. 
Column ``LOC'' is the number of lines of code,
as counted by CLOC~\cite{cloc}. Column ``\#Config Options''
is the number of configuration options. Column ``\#Profiles''
is the number of execution profiles in the pre-built database.}}
\end{figure}


\begin{figure}[t]
\vspace{2mm}
\setlength{\tabcolsep}{.94\tabcolsep}
\small{
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the specified class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}
}
\vspace{-3mm}
\Caption{{\label{tab:errors} The \errors
configuration errors used in the evaluation.
%The 9 crashing errors in the bottom table are taken from~\cite{Rabkin:2011:PPC}.
}}
\vspace{-0mm}
\end{figure}

We searched forums, FAQ pages, and the literature of
configuration error diagnosis research to find actual
configuration problems that users have experienced with our
target applications. 
\todo{I modified the following sentence. say we use ALL errors
we have collected, rather than just picking up errors that this
technique works well}
We collected a total number of \errors configuration errors, in which
the misconfigured values cover various data types, such as enumerated types,
numerical ranges, regular expressions, and text entries.
\todo{I modified the following sentence}
All collected errors 
are listed in Figure~\ref{tab:errors}. 
The \noncrash non-crashing errors
are collected from actual bug reports, mailing list posts, and our own experience.
The \crash crashing errors, taken from~\cite{Rabkin:2011:PPC},
were used to evaluate the ConfAnalyzer tool. %~\cite{confanalyzer}.
All \errors configuration errors have been minimized: if
any part of the configuration or input is removed, the software
either crashes or no longer exhibits the undesired behavior.
%\todo{Were any of them used in previous research?  Was that the reason we
%  chose them?}

\input{result-fig}

\vspace{-2mm}
\subsection{Evaluation Procedure}
\vspace{-1mm}

For each subject program, we constructed a profile database
by running existing (correct) examples from its user manual, discussion
mailing list, and published papers~\cite{PachecoLET2007, Rabkin:2011:PPC}.
We spent 3 hours per program, on average, and obtained 6--16 execution profiles.
The average size of the profile database is 35MB, and the largest one
(Randoop's database) is 72MB.

We made a simple syntactic change to JChord, which affected 24 
lines of code. This change
does not modify JChord's semantics; rather, it just encapsulates
scattered configuration option initialization statements 
as static class fields, which simplifies specifying the seed statement
in performing slicing. 

%Here is a sample modification, where 
%\<chord.kobj.k> 
%is a configuration option
%passed as a system property:

%\vspace{-1mm}

%\begin{CodeOut}
%\begin{alltt}
%public void run() \ttlcb
%  ...
%  int kobjK = Integer.getInteger("chord.kobj.k");
%  ...
%\ttrcb
%\end{alltt}
%\end{CodeOut}
%\vspace{-4mm}
%\hspace{20mm}$\Downarrow$ 
%\vspace{-2mm}
%\begin{CodeOut}
%\begin{alltt}
%static int chord\_kobj\_k = Integer.getInteger("chord.kobj.k");
%public void run() \ttlcb
%  ...
%  int kobjK = chord\_kobj\_k; 
%  ...
%\ttrcb
%\end{alltt}
%\end{CodeOut}


When diagnosing a configuration error, we first reproduced the
error on a \ourtool-instrumented program to obtain the
execution profile. Then, we ran \ourtool on the obtained execution profile
to identify its root causes.

%since we know the misconfigured, root-cause entry for each case,
%we use the ranking of the entry as our evaluation metric.

Our experiments were run on a
2.67GHz Intel Core PC with 4GB physical memory (2GB was allocated
for the JVM), running Windows 7.


\vspace{-1mm}
\subsection{Results}
\label{sec:results}


\subsubsection{Accuracy in Diagnosing Configuration Errors}
\label{sec:accuracy}
\input{diagnosisresults}

\enlargethispage{5pt}

\subsubsection{Performance of \ourtool}
\label{sec:performance}
\input{performance}

\subsubsection{Comparison with an Existing Technique}
\label{sec:confanalyzer}
\input{confanalyzer}


\subsubsection{Comparison with Two Fault Localization Techniques}
\label{sec:comparison}
\input{comparison}



%\subsubsection{Effects of Varying Comparison Execution Profiles}
\subsubsection{Evaluation of Two Design Choices in \ourtool}
\label{sec:choices}
\input{designchoice}


\vspace{-1mm}
\subsection{Experimental Discussion}
\vspace{-1mm}


%\todo{Need to mention the following sentence somewhere: many
%non-crashing errors often exhibit substantially behaviorally
%differences (hot spot) in some parts of the program, and our technique captures
%such differences and link them to specific options. However, in
%rare cases (at least we have not discovered in practice), if no
%such hot spot exist for a non-crashing error, our technique may not
%be that effective.}

%\todo{Also perphas need to point out this technique does not
%support diagnosing error involving multi options, at least
%in experiments. Maybe mentioned in the end of experiments.}

\noindent \textbf{\textit{Limitations.}} 
We identified several limitations of our technique from the experiments. 
First, we only focus on named configuration options
with a common key-value semantic, and our implementation
and experiments are restricted to Java. 
Second, we only evaluated \ourtool on configuration errors
involving only one mis-configured option.
Third, \ourtool does not yet help diagnose misconfigurations
that cause poor performance (e.g., making a program run faster
by increasing JVM's heap size via the option \CodeIn{-Xmx}).
Fourth,  our implementation currently does not
support debugging non-deterministic errors. 
For non-deterministic errors, \ourtool could potentially leverage 
a deterministic replay system
that can capture an undesired non-deterministic
execution and faithfully reproduce it for later analysis.
Fifth, \ourtool's effectiveness  largely
depends on the availability of a similar but correct execution profile.
Using an arbitrary execution profile (as we demonstrated in Section~\ref{sec:choices}
by random selection) may significantly affect the results.

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} 
There are three major threats to validity in our evaluation. 
First, the \subjectnum programs and the configuration errors may not be
representative. Thus, we can not claim the results can be
generalized to an arbitrary program.
\todo{the second point is newly added}
Second, in this paper, we focus specifically on
configuration errors, assuming the application code is correct.
In our experiments, all \errors errors
have been minimized (as end-users often do
when reporting an error). \ourtool might produce
different error diagnosis results on buggy application 
code with non-minimized inputs.
Third, we only compared two dependence
analyses (thin slicing and full slicing), three
abstraction granularities (at the predicate level,
statement level~\cite{Jones:2002}, and method level~\cite{Ernst:1999}),
and three other tools (ConfAnalyzer, Coverage Analysis, and
Invariant Analysis) in our evaluation.
 Using other dependence analyses, abstraction levels, or tools
might achieve different results.

%how easy to construct such database in practice.
%User study of usefulness of the results


\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} 
We have three chief findings: \textbf{(1)} \ourtool is effective
in diagnosing both crashing and non-crashing configuration errors
with a small profile database.
\textbf{(2)} \ourtool produces more accurate diagnosis than
approaches leveraging existing fault localization
techniques~\cite{Jones:2002, McCamant:2003}. \textbf{(3)} thin slicing
%to identify the affected predicates
permits \ourtool to produce more accurate diagnosis than using
full slicing and varying the execution profile selection
strategy can result in substantially different diagnosis.

%\ourtool makes configuration error diagnosis easier by suggesting
%the specific options that may lead to an unexpected behavior. 




%Compared to
%alternative approaches, \ourtool distinguishes itself by being able to
%diagnose both crashing and non-crashing errors without requiring
%a user-provided testing oracle. $\blacksquare$

% LocalWords:  Weka JChord LOC Config CLOC pre classpath misconfigured 35MB 4GB
% LocalWords:  ConfAnalyzer 72MB JChord's kobjK getInteger kobj 67GHz 2GB mis
% LocalWords:  misconfigurations JVM's Xmx
