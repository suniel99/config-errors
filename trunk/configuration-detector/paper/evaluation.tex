\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

Our evaluation answers the following research questions:

\begin{itemize}
\item How effective is \ourtool in diagnosing the root cause of
configuration errors?
\item Can \ourtool provide more accurate diagnosis information than
the existing approaches? 
\item What are the effects of varying the comparison profiles from
the pre-built database?
\item How long does \ourtool take to diagnose a configuration error?
\end{itemize}

%how well can \ourtool identify solutions to configuration problems?
%How effective is predicate-level ..
%Can \ourtool identify slutions to configuration problems involving xx

\subsection{Subject Programs}

We evaluated \ourtool on \subjectnum Java applications shown
in Table~\ref{tab:subjects}.
Randoop~\cite{randoop} is a robust automated test generator
for Java programs. Weka~\cite{weka} is a useful toolkit implementing
a broad variety of machine learning algorithms. Our evaluation
uses its decision tree module component. JChord~\cite{jchord}
is a program analysis platform that enables users to design, implement,
and evaluate static and dynamic program analyses for Java bytecode.
Synoptic~\cite{synoptic} is a tool to mine a finite state machine
model representations of a system from logs.
Soot~\cite{soot} is a Java optimization framework, which provides four
intermediate representations for analyzing and transforming Java bytecode:

As shown in Table~\ref{tab:subjects}, each subject program has a non-trivial
codebase, and exposes a number of configuration options for users
to customize its behaviors.

\subsubsection{Configuration Errors}



\begin{table}[t]
%\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Conf Options & \#Profiles\\
 \hline
 \hline
 Randoop (1.3.2) & 18587 & 57 & 12\\
 Weka (3.6.7) & 256305 & 14 & 12\\
 JChord (2.1) & 23391 &  79 & 6 \\
 Synoptic (trunk) & 19153 & 37 & 6\\
 Soot (2.5.0) & 159273 & 49 & 16 \\
\hline
\end{tabular}


\Caption{{\label{tab:subjects} Subject programs. 
Column ``LOC'' is the number of lines of code,
as counted by CLOC~\cite{cloc}. Column ``\#Conf Options''
is the number of available configuration options. Column ``\#Profiles''
is the number of profiles in the pre-built database.}}
\end{table}


\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated for NanoXML~\cite{nanoxml}\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the specified class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}

\Caption{{\label{tab:errors} A list of \errors
configuration errors used in the evaluation.
The 9 crashing errors in the bottom table are taken from~\cite{Rabkin:2011:PPC}.
}}
\end{table}

We searched forums, FAQ pages and the literature of
configuration error diagnosis research to find actual
configuration problems that users have experienced with our
target applications. Combining with our own experience,
we chose \errors representative configuration errors.
Table~\ref{tab:errors} lists the configuration errors for each application.
The configuration error list includes \noncrash non-crashing errors and \crash
crashing errors, covering various data types, such as enumerated types,
numerical ranges, regular expressions, and text entries.

\subsection{Evaluation Procedural}

For each Java application, we constructed a profile database
by running existing examples from its user manual, discussion
mailiing list, and the published papers. The number of obtained
 profiles is shown in Table~\ref{tab:subjects}. We
found constructing such a database is quite easy. As shown
in Section~\ref{sec:results}, even a database containing
a small number of profiles is sufficient to
diagnose many configuration errors.

%Run examples from user manual to form the database. The effort
%can be amortized in development time.

We made a 30-line change to the JChord code. This change
does not modify JChord's semantic; rather, it just encapsulates
all scattered initialization statements of each configuration option
into class fields. Here is a sample modification:



\begin{CodeOut}
\begin{alltt}
   // chord.kobj.k is a configuration option
   // passed as a system property
   public void run() \ttlcb
     ...
     int kobjK = Integer.getInteger("chord.kobj.k");
     ...
   \ttrcb
\end{alltt}
\end{CodeOut}
\vspace{-4mm}
\hspace{20mm}$\Downarrow$ 
%\vspace{-2mm}
\begin{CodeOut}
\begin{alltt}
   static int chord\_kobj\_k = Integer.getInteger("chord.kobj.k");
   public void run() \ttlcb
     ...
     int kobjK = chord\_kobj\_k; 
     ...
   \ttrcb
\end{alltt}
\end{CodeOut}



%Use 1-CFA to construct call graph, achieve much better precision
%than 0-CFA. 

%Use extraction~\cite{Rabkin:2011:SEP}

%To evaluate \ourtool, 

When diagnosing a configuration error, we first reproduce the
error on a \ourtool-instrumented version to obtain the
trace file. Then, using the obtained trace file, we use \ourtool
to identify its root cause.

We use two metrics to evaluate \ourtool's effectiveness:
the absolute ranking of the actual root cause in \ourtool's output,
and the time cost used in diagnosis.


%This step
%could be automated by combining with some existing  work on
%automated configuration option extraction~\cite{}.
All of our experiments were run on a
2.67GHz Intel Core PC with 4GB physical memory (1GB is allocated
for the JVM), running Windows 7.


\subsection{Results}
\label{sec:results}

\begin{table*}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l||c||c|c||c|c|c|c|}
\hline
 Error ID.  & & \multicolumn{2}{|c||}{\ourtool} & Full Slicing & Coverage Analysis& Invariant Analysis & ConfAnalyzer~\cite{Rabkin:2011:PPC}\\
\cline{3-8}
 Program & Reponsible Option & \#Cmp Profile & Rank  & Rank & Rank & Rank & Rank \\
 \hline
\hline
\multicolumn{8}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop& \CodeIn{maxsize} & 10 & 1 & & N & N &X \\
 2. Weka&\CodeIn{m\_numFolds}&2&1& & 4 & &X\\
 3. JChord& \CodeIn{chord.kobj.k} & 3 & 2& & N &2  &X\\
 4. Synoptic& \CodeIn{partitionRegExp}& 2 & 1& & 1 & &X\\
 5. Soot& \CodeIn{keep\_line\_number} & 1 & 3 &  & N& &X\\
\hline
\hline
\multicolumn{8}{|l|}{Crashing errors}   \\
\hline
 6. JChord& \CodeIn{chord.main.class}&3& 1& & & &1\\
 7. JChord& \CodeIn{chord.main.class}&3 & 3& & & &1\\
 8. JChord& \CodeIn{chord.run.analyses}&3 & 17& & & &1\\
 9. JChord& \CodeIn{chord.ctxt.kind}&3 & 1 & & & &3\\
 10. JChord& \CodeIn{chord.print.rels}& 3& 15 & & & &1\\
 11. JChord& \CodeIn{chord.print.classes}&3 & 15 & & & &1\\
 12. JChord& \CodeIn{chord.scope.kind}&3 & 1& & & &1\\
 13. JChord& \CodeIn{chord.reflect.kind} &3 & 1& & & &3\\
 14. JChord& \CodeIn{chord.class.path}&3 & 8 & & & &N\\
\hline
\end{tabular}

\Caption{{\label{tab:results} Experimental results in diagnosing software
configuration errors. Column ``Responsible Option'' shows the actual
configuration option for the error. Column ``\ourtool'' shows the results of using
our technique. Columns ``Full Slicing'', ``Coverage Analysis'',
``Invariant Analysis'', and ``ConfAnalyzer'' show the results of
using four comparison techniques, as detailed in Section~\ref{sec:comparison}.
Column ``\#Cmp Profiles'' shows the number of similar profiles selected
from the pre-built database for comparison.
For each technique, Column ``Rank'' shows the rank of the actual responsible
option in its diagnosis outputs; in which lower is better. ``X''
means the technique is not applicable, and ``N'' means the technique
does not output the actual responsible option.}}
\end{table*}


\subsubsection{Accuracy in Diagnosing Configuration Errors}

Table~\ref{tab:results} shows the experimental results.
We can see that \ourtool is highly effective in pinpointing the root cause of
misconfigurations. For all \noncrash non-crashing errors
and 5 out of the \crash crashing errors, it lists the actual root cause as one of the
top 3 options. For the rest
4 crashing errors, the actual causes are ranked lower.
This is because $\blacksquare$


We next briefly explain the root cause of each non-crashing configuration
errors as shown Table~\ref{tab:results}, and discuss why
\ourtool could identy them. 

\begin{itemize}
\item \textbf{Randoop}. reason
\item \textbf{Weka}. reason
\item \textbf{JChord}. reason
\item \textbf{Synoptic}. reason
\item \textbf{Soot}. reason
\end{itemize}

Compared to non-crashing errors, \ourtool is less effective
in localizing non-crashing errors. They are several reasons
for this.

The nature of the root-cause configuration option is only one factor.
The ranking also depends on how the root-cause option relates to
other options in the suspect set. A highly configurable software system 
likely produces more noises, 

%The detailed illustration
%for those crashing errors can be found in~\cite{Rabkin:2011:PPC}.

The remaining errors are a direct result of $\blacksquare$ and seems
hard for \ourtool to diagnose correctly.


%\ourtool also successfully diagnoses xx\% of the xxx errors. For the
%remaining errors, \ourtool ranks the root cause 9th. The configuration
%error is that the xxx. Thus, the root cause gets ranked lower
%in the list.


\vspace{1mm}
\noindent \textbf{\textit{Summary.}} \ourtool is effectively
in diagnosing both crashing and non-crashing configuration errors. $\blacksquare$


\subsubsection{Comparison with Alternative Approaches}
\label{sec:comparison}

We next compare \ourtool with four alternative approaches
in diagnosing configuration errors.
The first three approaches are variants of the \ourtool
technique, and the fourth one is an existing technique~\cite{Rabkin:2011:PPC}

\vspace{1mm}
\noindent \textbf{\ourtool with Full Slicing.} 
In \ourtool, we use thin slicing to compute the affected predicates for
each configuration option. However, such affected predicates
can also be computed by using the traditional full slicing~\cite{Horwitz:1988}.
To evaluate the trade-offs, this alternative uses the traditional
full slicing~\cite{Horwitz:1988} to replace thin slicing
in the configuration propagation analysis phase (Section~\ref{sec:prop}).

\vspace{1mm}
\noindent \textbf{Coverage Analysis.}
In this approach, we use statement coverage rather than
predicate behaviors to diagnose configuration errors. Specifically,
we treat statements covered by the erroneous execution as
potentially buggy, and statements covered the correct executions (in
the pre-built database) as likely correct. Then, we uses
a well-known fault localization technique, Tarantula~\cite{},
to rank the likelihood of each statement being buggy.
For each ranked statement, we consult the thin slicing results
to infer which configuration option may affect it. $\blacksquare$


\vspace{1mm}
\noindent \textbf{Invariant Analysis.}
In this approach, we use method-level invariant information
to diagnose configuration errors. This approach first dynamically
detects invariants from the erroneous execution as well as invariants
from all correct executions in the pre-built database. Then, it
treats a method potentially buggy if its invariants detected
from the erroneous execution differ from invariants detected
by a correct execution. Finally, it ranks each suspicious
method by the number of different invariants. $\blacksquare$



\vspace{1mm}
\noindent \textbf{Dynamic information flow.}
Rabkin and katz proposed a family of techniques (and its tool implementation called ConfAnalyzer)
to precompute possible
configuration diagnosis for Java software~\cite{Rabkin:2011:PPC}. In their work,
the most accurate technique (also the most precise technique in the literature,
to the best of our knowledge) is based on dynamic information flow analysis, which precisely
tracks the flow of each configuration option value during program execution at the bit level.
Their technique works remarkably well for crashing errors, but can
not diagnose non-crashing errors.

\vspace{1mm}
%Compare with statistical debugging?
$\blacksquare$ put the results here

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective
Is our \textit{predicate}-level granularity a suitable one?

\subsubsection{Effects of Varying Comparison Profiles}
\label{sec:ranking}


\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|c|c||c|}
\hline
 Error ID. & \multicolumn{3}{|c|}{Rank of the Actual Responsible Option } \\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{2-4}
 Program & All Profiles & Random Selection&  Similarity-Based\\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop & 1 & 2 & 1\\
 2. Weka & 7 & 6 & 1\\
 3. JChord & 16 & 19 & 2\\
 4. Synoptic & 1 & 1 & 1\\
 5. Soot & 13 & 13 & 3\\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6. JChord & & &1\\
 7. JChord & & &3\\
 8. JChord & & &17\\
 9. JChord & & &1\\
 10. JChord & & &15\\
 11. JChord & & &15\\
 12. JChord & & &1\\
 13. JChord & & &1\\
 14. JChord & & &8\\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} Comparison with different profile selection
strategies (Section~\ref{sec:ranking}) in ranking the responsible option.
The last column ``Similarity-based'' is the selection strategy
used in \ourtool, and the data in that column is taken from Table~\ref{tab:results}.}}
\end{table}


\ourtool compares the predicate behaviors in the erroneous trace against
correct and similar traces in the pre-built database.
We next investigate the effect of using different comparison
profiles. We compare the current similarity-based strategy (Section~\ref{})
with two alternatives: randomly selecting the same number of
profiles as similarity-based strategies does, and using
all profiles in the database. Table~\ref{} shows the experimental results.

We can see that varying the comparison strategy has result in
substantially different effects on the diagnosis results,
depending on the application being analyzed.

The result difference derives from the nature of the applications.
We found....  . On the other hand,

It does not make sense to compare different profiles...

%What would the technique produce when feeding it with different inputs (e.g.,
%radically different inputs instead of similar ones)?

%In our approach, we first select a set of similar profiles from the  database,
%and then to do comparison. What about just using a single trace, i.e., the
%most similar trace? the most dissimilar traces? or what about just using a set
%of random selected trace.

%Comparison of different distance metrics to find similar statements.

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective

\subsubsection{Performance of \ourtool}

\ourtool performs very well on these errors. The average time
to diagnose xxx (with max, min xx).

The cost of slicing

The cost of instrumentation, permit the use in fielded software?


%The performance of \ourtool is reasonable. The time to diagnose
%an error varies among applications.  XXX app takes less than xxx,
%while xxx takes xxx to complete.


\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective

\vspace{1mm}

\subsection{Experimental Discussions}


\noindent \textbf{\textit{Limitations.}} similar inputs, if no input is available, test adequacy.

We focus on named configuration options with a common key-value semantic.

Our tool implementation and experiments were restricted to Java. Our analysis
does not track options that are passed between processes via the command line.

\ourtool currently does not support debugging non-deterministic errors. %Combining \ourtool with a deterministic replay system
For non-deterministic errors, \ourtool could potentially leverage one of
several deterministic replay systems~\cite{Huang:2010:LLD}
that can capture a buggy non-deterministic
execution and faithfully reproduce it for late analysis.

Our current \ourtool prototype assists in solving configuration errors that are confined
to a single computer system, such as a home computer, personal workstation, or stand-alone server. (no process communication...)

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} similar inputs, if no inputs is available.

how easy to construct such database in practice.

User study of usefulness of the results

\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} similar inputs, if no inputs is available.

\ourtool makes configuration error diagnosis easier by suggesting
the specific options that may lead to an unexpected behavior. Compared to
alternative approaches, \ourtool distinguishes itself by being able to
diagnose both crashing and non-crashing errors without requiring
a user-provided testing oracle. $\blacksquare$
