\section{Evaluation}

\subsection{Research Questions}

We aim to answer the following research questions:

\begin{itemize}
\item Is our technique useful in explaining software misconfiguration errors?
\item Is the information provided by our technique more useful than the statement-level
profiling and the method-level dynamic invariant detection?
\item Which factors (how much) can affect our technique's accuracy?
\end{itemize}

\subsection{Subject Programs}

We collected a number of subject programs and their mis-configuration problems in
Table~\ref{tab:subjects}.

\begin{table}[t]
\setlength{\tabcolsep}{.14\tabcolsep}
\begin{tabular}{|c|c|c|c|l|}
\hline
 Program & LOC & \#Config & \#Traces & Description \\
 \hline
\hline
\multicolumn{5}{|l|}{Non-crashing Configuration Problems}   \\
 \hline
 Randoop & & &&  No tests generated \\
\hline
 Weka &  & && Poor performance \\
 &  & && of the decision tree \\
\hline
 Chord & & && No datarace reported \\
\hline
 Synoptic & && & Produce an incorrect model \\
\hline
 Soot &  &  && Source code line number \\
 &  &  && is missing \\
\hline
\hline
\multicolumn{5}{|l|}{Crashing Configuration Problems}   \\
\hline
& & & &\\
\hline
& & & &\\
\hline
& & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} Subject programs and their configuration problems. Column ``\#Configs''
shows the number available configuration options. Column ``\#Traces''
shows the number of traces in the constructed database.}}
\end{table}

Need to say how to collect representative profiles

\subsubsection{Crashing Errors}

\subsubsection{Non-Crashing Errors}

\subsubsection{Injected Errors}

Use injected errors to test the technique's reliability

\subsubsection{Evaluation Procedural}

Rewrite the configuration option in the form of method fields, since
the local variable information has gone in the bytecode.

Use 1-CFA to construct call graph, achieve much better precision
than 0-CFA. 

\subsection{Experiment Design}



\subsubsection{Accuracy in Localizing Configuration Errors}

\begin{table}[t]
\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|c|c|c|c|c|}
\hline
 Program & \#Similar & \#Avg Pred & \#Rank & Explanation \\
 \hline
\hline
\multicolumn{5}{|l|}{Non-crashing Configuration Problems}   \\
 \hline
 Randoop & &  &&  \\
\hline
 Weka &  & & & \\
\hline
 Chord & & & &\\
\hline
 Synoptic & & && \\
\hline
 Soot &  &    &&\\
\hline
\hline
\multicolumn{5}{|l|}{Crashing Configuration Problems}   \\
\hline
& & & &\\
\hline
& & & &\\
\hline
& & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:accuracy} Results. Column ``\#Avg Pred'' shows
the average number of affected predicates by each configuration options
computed by our configuration propagation analysis (Section~\ref{}).
``\#Rank'' shows the ranking of the responsible configuration
option in the output list...}}
\end{table}

Are the ranked configurations useful for misconfiguration error diagnosis?
We examine the ranked configurations to see how well they can explain the behavior.

\subsubsection{Sensitivy to the Inputs}

\begin{table}[t]
\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|c|c|c|c|}
\hline
 Program & Most Similar One& Least Similar One& Random \\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing Configuration Problems}   \\
 \hline
 Randoop & & &   \\
\hline
 Weka &  & & \\
\hline
 Chord & & & \\
\hline
 Synoptic & & &  \\
\hline
 Soot &  &  &  \\
\hline
\hline
\multicolumn{4}{|l|}{Crashing Configuration Problems}   \\
\hline
& & & \\
\hline
& & & \\
\hline
& & & \\
\hline
\end{tabular}

\Caption{{\label{tab:accuracy} Results. Use three settings:
the most similar one trace, the least similar one, and randomly
select XXX number of traces. }}
\end{table}

What would the technique produce when feeding it with different inputs (e.g.,
radically different inputs instead of similar ones)?

In our approach, we first select a set of similar profiles from the  database,
and then to do comparison. What about just using a single trace, i.e., the
most similar trace? the most dissimilar traces? or what about just using a set
of random selected trace.

Comparison of different distance metrics to find similar statements.

\subsubsection{Comparison with Traditional Slicing}
\begin{table}[t]
\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|c|c|c|}
\hline
 Program &  \#Avg Pred & \#Rank \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing Configuration Problems}   \\
 \hline
 Randoop & &    \\
\hline
 Weka &  &   \\
\hline
 Chord & & \\
\hline
 Synoptic & &  \\
\hline
 Soot &  &    \\
\hline
\hline
\multicolumn{3}{|l|}{Crashing Configuration Problems}   \\
\hline
& & \\
\hline
& & \\
\hline
& & \\
\hline
\end{tabular}

\Caption{{\label{tab:slicingaccuracy} Results of using traditional slicing}}
\end{table}

How would the results change if traditional slicing~\cite{Horwitz:1988} is used
in Section~\ref{sec:prop}?

What about using traditional slicing for configuration propagation analysis?
full control and data-flow information.

\subsubsection{Comparison with Statement-level Profiling and Method-level Invariant Detection}

\begin{table}[t]
\setlength{\tabcolsep}{.54\tabcolsep}
\begin{tabular}{|c|c|c|c|}
\hline
 & \multicolumn{3}{|c|}{Rank of the responsible option} \\
\cline{2-4}
 Program & Statement Profile& Method-level & Our technique \\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing Configuration Problems}   \\
 \hline
 Randoop & & &  \\
\hline
 Weka &  & &  \\
\hline
 Chord & & &  \\
\hline
 Synoptic & & &  \\
\hline
 Soot &  &  &  \\
\hline
\hline
\multicolumn{4}{|l|}{Crashing Configuration Problems}   \\
\hline
& & & \\
\hline
& & & \\
\hline
& & & \\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} Comparison with statement-level and method-level .}}
\end{table}

Our technique is at the \textit{predicate}-level. What about using
\textit{statement}-level instrumentation and \textit{method}-level dynamic invariant detection~\cite{Ernst:1999}?

Is our \textit{predicate}-level granularity a suitable one?

The first alternative is to instrument all statement, and record the different between a bad run and
a set of good runs. Find out the statement covered most by good runs, but covered least by bad run.
After then, querying the thin slicing info, to rank the configuration options that can affect them.


The second alternative is to run the program, diff the dynamically-detected invariants. Ranked all
methods that have the most different invariants between 2 runs. Then, querying thin slicing to
figure out the configuration options.

\subsubsection{The Effects of Increasing Context Sensitivity}

When recording configuration profiles (Section~\ref{sec:profiling}), is it useful
to increase the length of calling context? Would that help improve our technique's accuracy?

The default length is 1, we increase it to 2, 3, and then observe the difference

\subsection{Discussions}

\noindent \textbf{\textit{Performance.}} low time cost when when doing
selective instrumentation, permitting the use in fielded software.

\vspace{1mm}

\noindent \textbf{\textit{Limitations.}} similar inputs, if no inputs is available, test adequacy.

We focus on named configuration options with a common key-value semantic.

Our tool implementation and experiments were restricted to Java. Our analysis
does not track options that are passed between processes via the command line.

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} similar inputs, if no inputs is available.

\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} similar inputs, if no inputs is available.

