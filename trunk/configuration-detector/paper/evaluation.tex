\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

We aim to answer the following research questions:

\begin{itemize}
\item How effective is \ourtool in diagnosing the root cause of
configuration errors?

\item $\blacksquare$ compare to existing approaches

\item How long does \ourtool take to find the root cause?

%\item Is our technique useful in diagnosing software configuration errors, including
%crashing and non-crashing errors?
%\item Is the information provided by our technique more useful than the statement-level
%profiling and the method-level dynamic invariant detection?
%\item Which factors (how much) can affect our technique's accuracy?
\end{itemize}

\subsection{Subject Programs}

We evaluated \ourtool on xxx Java applications: Randoop~\cite{randoop},
Weka~\cite{weka}, JChord~\cite{jchord}, Synoptic~\cite{synoptic},
and Soot~\cite{soot}.

\subsubsection{Configuration Errors}

We collected a number of subject programs and their mis-configuration problems in
Table~\ref{tab:subjects}.

We searched forums, FAQ pages and configuration documents to find
actual configuration problems that users have experienced with our
target applications. In total, we chose xxx misconfigurations (xxx
are crashing, xxx non-crashing) that were casued by errors in
configuration options. Table xxx lists the configuration errors for each application.

\begin{table}[t]
%\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Conf Options & \#Profiles\\
 \hline
 \hline
 Randoop & & & \\
 Weka &  & & \\
 JChord & & & \\
 Synoptic & && \\
 Soot &  &  & \\
\hline
\end{tabular}


\Caption{{\label{tab:subjects} Subject programs. Column ``\#Conf Options''
shows the number available configuration options. Column ``\#Profiles''
shows the number of profiles in the constructed database.}}
\end{table}

Need to say how to collect representative profiles

\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated for Nanoxml\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the entry class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} A list of \errors
configuration errors used in our evaluation.
The 9 crashing errors are taken from~\cite{Rabkin:2011:PPC}.
}}
\end{table}



%Use injected errors to test the technique's reliability

\subsection{Evaluation Procedural}


Rewrite the configuration option in the form of method fields, since
the local variable information has gone in the bytecode.

Use 1-CFA to construct call graph, achieve much better precision
than 0-CFA. 

Re-writing the configuration initialization code in JChord, because
it is defined in an add-hoc way

Run examples from user manual to form the database. The effort
can be amortized in development time.

We use two metrics to evaluate \ourtool's effectiveness: the absolute ranking of the
actual root cause in the list returned by \ourtool, and the
time cost to diagnose each configuration error.

Use extraction~\cite{Rabkin:2011:SEP}


\subsection{Results}

\begin{table*}[t]
\setlength{\tabcolsep}{.54\tabcolsep}
\begin{tabular}{|l||l||l|l||c||c||c||c|}
\hline
  & & \multicolumn{2}{|c||}{Our Proposed Technique} & Full Slicing & Coverage Analysis& Invariant Analysis & ConfAnalyzer~\cite{Rabkin:2011:PPC}\\
\cline{3-8}
 Error ID & Reponsible Option & \#Cmp Profile & Rank  & Rank & Rank & Rank & Rank \\
 \hline
\hline
\multicolumn{8}{|l|}{Non-crashing errors}   \\
 \hline
 1 & & & & & & &N \\
 2 & & & & & & &N\\
 3 & & & & & & &N\\
 4 & & & & & & &N\\
 5 & & & & & & &N\\
\hline
\hline
\multicolumn{8}{|l|}{Crashing errors}   \\
\hline
 6 & & & & & & &\\
 7 & & & & & & &\\
 8 & & & & & & &\\
 9 & & & & & & &\\
 10 & & & & & & &\\
 11 & & & & & & &\\
 12 & & & & & & &\\
 13 & & & & & & &\\
 14 & & & & & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:results} Experimental results. }}
\end{table*}


\subsubsection{Accuracy in Diagnosing Configuration Errors}

For each configuration error, Table~\ref{} shows the ranking of the
responsible configuration option, the number , xxx.


Are the ranked configurations useful for misconfiguration error diagnosis?
We examine the ranked configurations to see how well they can explain the behavior.

%\noindent \textbf{Comparison with Alternative Approaches.}
%The ASE paper. Only handles crashing errors.

Need to explain each configuration error


\subsubsection{Comparison with Alternative Approaches}

%\subsubsection{Comparison with Traditional Slicing}
We compare \ourtool with four alternative approaches:

\noindent \textbf{Full Slicing.}

\noindent \textbf{Coverage Analysis.}

\noindent \textbf{Invariant Analysis.}

\noindent \textbf{Taint-based Analysis (ConfAnalyzer~\cite{Rabkin:2011:PPC}).}

How would the results change if traditional slicing~\cite{Horwitz:1988} is used
in Section~\ref{sec:prop}?

What about using traditional slicing for configuration propagation analysis?
full control and data-flow information.

%\subsubsection{Comparison with Existing Approaches}


Our technique is at the \textit{predicate}-level. What about using
\textit{statement}-level instrumentation~\cite{Jones:2002}
 and \textit{method}-level dynamic invariant detection~\cite{Ernst:1999}?

Is our \textit{predicate}-level granularity a suitable one?

The first alternative is to instrument all statement, and record the different between a bad run and
a set of good runs. Find out the statement covered most by good runs, but covered least by bad run.
After then, querying the thin slicing info, to rank the configuration options that can affect them.


The second alternative is to run the program, diff the dynamically-detected invariants. Ranked all
methods that have the most different invariants between 2 runs. Then, querying thin slicing to
figure out the configuration options.

Compare with statistical debugging?

\subsubsection{Effects of Varying Comparison Profiles}


\begin{table}[t]
\setlength{\tabcolsep}{.54\tabcolsep}
\begin{tabular}{|l|l|l|l|}
\hline
  & \multicolumn{3}{|c|}{Rank of the Responsible Option } \\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{2-4}
 Error ID & Most Similar& Random Selection& Least Similar\\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1 & & &\\
 2 & & &\\
 3 & & &\\
 4 & & &\\
 5 & & &\\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6 & & &\\
 7 & & &\\
 8 & & &\\
 9 & & &\\
 10 & & &\\
 11 & & &\\
 12 & & &\\
 13 & & &\\
 14 & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} The effects of varying comparison
profiles. }}
\end{table}


We next investigate the effect of varying \ourtool's
comparison profiles. As Table~\ref{} shows, varying the
strategy..


What would the technique produce when feeding it with different inputs (e.g.,
radically different inputs instead of similar ones)?

In our approach, we first select a set of similar profiles from the  database,
and then to do comparison. What about just using a single trace, i.e., the
most similar trace? the most dissimilar traces? or what about just using a set
of random selected trace.

Comparison of different distance metrics to find similar statements.

%\subsubsection{The Effects of Increasing Context Sensitivity}

%When recording configuration profiles (Section~\ref{sec:profiling}), is it useful
%to increase the length of calling context? Would that help improve our technique's accuracy?

%The default length is 1, we increase it to 2, 3, and then observe the difference

\subsection{Experimental Discussions}

\noindent \textbf{\textit{Performance.}} low time cost when when doing
selective instrumentation, permitting the use in fielded software.

The overhead, the cost of diagnosis, acceptable?

\vspace{1mm}

\noindent \textbf{\textit{Limitations.}} similar inputs, if no inputs is available, test adequacy.

We focus on named configuration options with a common key-value semantic.

Our tool implementation and experiments were restricted to Java. Our analysis
does not track options that are passed between processes via the command line.

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} similar inputs, if no inputs is available.

\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} similar inputs, if no inputs is available.

