

Figure~\ref{tab:results} shows the experimental results.
\ourtool is highly effective in pinpointing the root cause of
misconfigurations. For all \noncrash non-crashing errors
and 5 out of the \crash crashing errors, it lists the actual root cause as one of the top 3 options. 


\ourtool is particularly effective
in diagnosing non-crashing configuration errors, which are not supported
by most existing tools. $\blacksquare$ why effective? observation?
statically significance? Give some examples here... Randoop \CodeIn{maxsize},
Weka behaves overfitting..

Weka implements a highly-tuned decision tree, which achieves 70 --- 90\%
accuracy in own dataset. However, when using a different dataset (also
from Weka), the performance of decision tree drops to 62\%. This is an
undesired behavior. \ourtool outputs the following error report (only
the first suspicious option) is shown.


\begin{CodeOut}
\begin{alltt} 
Suspicious configuration option: m\_numFolds

It affects the behavior of predicate:
"numFold < numInstances() \% numFolds"
(line 1354, class: weka.core.Instances) 

This predicate evaluates to true:
  20\% of the time in normal runs (4 observations)
  70\% of the time in the undesired run (10 observations)

\end{alltt}
\end{CodeOut}

This report directly points to the reason of low performance.
This predicate controls whether to grow the decision tree or not.
The tree grows too deep, and suffer from overfit of the given dataset.
The default value works well for the set.

Changing the default value of m\_numFolds from 3 to 2 leads to
an immediate increase of 5\% improvement

Compared to non-crashing errors, \ourtool is less effective
in diagnosing non-crashing errors. For 4 crashing errors,
the actual causes are ranked lower.
This is because $\blacksquare$ the configuration option has
a long propagation chain, and seems hard for \ourtool
to diagnose correctly. no statistical significance...

Although \ourtool ranked the actual root course of several
crashing errors lower, crashing errors are generally much easier to diagnose than non-crashing errors.
This is because a crashing error usually happens shortly after the program
is launched, and often produces a stack trace with valuable diagnosis clues.
For example, in Figure~\ref{tab:results}, \ourtool ranks the root cause of
error 14  9th.
However, when JChord crashes, it throws a \CodeIn{ClassNotFoundException}
that reminds users to check the classpath setting. For the other three crashing errors (error 8, 10, and 11),
JChord even outputs the wrong configuration option value in the
error message, which
directly guides users to the root cause. We speculated that $\blacksquare$

Many configuration predicate are only executed very few time, and the computed deviation score are the same.
//the heuristic does not


$\blacksquare$
Thus, the root cause gets
ranked lower in the list. This ordering is a direct result
of the heuristic discussed in Section XXX that considers
branches closer to the erroneous behavior to be more
likely to lead to the root cause than those farther away

seems hard for ConfAid to diagnose correctly
== imprecise of thin slicing, particularly when dealing with containers,

The combination of xxx these two factors cause ....


