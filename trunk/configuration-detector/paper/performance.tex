We measured \ourtool's performance in two ways: the time cost
in diagnosing an error and the overhead introduced
in reproducing an error in a \ourtool-instrumented program. 
%Figure~\ref{tab:performance} shows the results.

\begin{figure}[t]
\setlength{\tabcolsep}{.94\tabcolsep}
\small{
\begin{tabular}{|l|c|c|c|}
\hline
 Error ID.  & Run-time & \multicolumn{2}{|c|}{\ourtool time (seconds)}\\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{3-4}
 Program & Slowdown ($\times$) & Thin Slicing & Error Diagnosis  \\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop  & 1.1 & 50  & $<$ 1 \\
 2. Weka     & 1.2 & 43  & $<$ 1 \\
 3. JChord   & 13.2& 147 & 82    \\
 4. Synoptic & 3.6 & 24  & $<$ 1 \\
 5. Soot     & 3.1 & 95  & 21    \\
\hline
Average & 4.4 & 72 & 21 \\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6. JChord   & 2.4  & 147 & 79 \\
 7. JChord   & 1.4  & 147 & 75 \\
 8. JChord   & 1.5  & 147 & 17 \\
 9. JChord   & 28.5 & 147 & 30 \\
 10. JChord  & 13.7 & 147 & 13 \\
 11. JChord  & 65.1 & 147 & 10 \\
 12. JChord  & 1.6  & 147 & 83 \\
 13. JChord  & 1.9  & 147 & 8  \\
 14. JChord  & 1.4  & 147 & 80 \\
\hline
Average & 13 & 147 & 44 \\
\hline
\end{tabular}
}
\Caption{{\label{tab:performance} \ourtool's
performance in diagnosing configuration
errors. The time cost has been divided into
two parts: computing thin slices and diagnosing
an error.}}
\todo{Since the slowdowns are ratios, a sensible way to combine them is via
  geometric mean rather than average.  This also results in much more
  favorable numbers.}
\end{figure}

Figure~\ref{tab:performance} shows the results.
The performance of \ourtool is reasonable.
On average, it uses less than \avgtime minutes to
diagnose one configuration error (including
the time to compute thin slices and the time
to recommend suspicious configuration options). Computing
thin slices for all configuration options
is expensive. However, this step is one-time effort
per program and the computed slices can be cached
to share across diagnoses. %for future use.

The performance overhead to reproduce the buggy behavior varies
among applications. The current tool implementation
imposes an average slowdown of 13 times when reproducing
an error in a \ourtool-instrumented version.
%This performance overhead, admittedly high, is still acceptable
%for offline error diagnosis.
For errors 3, 9, 10, and 11, the slowdown is significant\todo{Can you
  explain why?  Better, I have an idea for significantly reducing the overhead.}, but the
absolute time cost for error reproduction is still low (less than 13
minutes per error).\todo{Add run times to table??}
% For the other 10 errors, the average slowdown is only 1.9.

%\todo{Maybe add one more sentence here: for error 9, 10, 11, the slowdown for
%reproducing a crashing error is very large. However, the absolute
%time cost is very low, on the original JChord version, all these three errors exhibit in less than 5 seconds. Thus, even with a 65X slowdown, the absolute time cost
%is still acceptable (around 5 mins).}

%The size of profile files in the database $\blacksquare$


%The performance of \ourtool is reasonable. The time to diagnose
%an error varies among applications.  XXX app takes less than xxx,
%while xxx takes xxx to complete.

