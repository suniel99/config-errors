\section{Related Work}
\label{sec:related}

The most closely related work falls into
three categories; (1) techniques for
supporting software evolution; (2) software
configuration error diagnosis techniques;
%(3) automated software debugging techniques;
and (3) configuration-aware software analysis techniques.

\subsection{Supporting Software Evolution}

As software evolves, its behavior must be validated.
Regression test selection and prioritization~\cite{}
indicate which tests need to be executed for a changed
program.  Program differencing techniques~\cite{Giroux:2006:DIF, Xing:2005:UAO, Thummalapenta:2010:ESM, Kim:2013, Jin:2012:BRF,Nguyen:2010:RBF,Dig:2006:ADR, Kamiya:2002:CMT}
\todo{semdiff}
try to identify changes between two program versions,
and present the change list to developers for inspection.
Related, change impact analysis techniques, which
are often built on top of program differencing
techniques, identify not only the changes, but also
which code fragments are affcted by which
changes. For example, a recent work~\cite{} uses symbolic
execution to accurately capture behavioral differences
between program versions. Different than \ourtool,
the work on change analysis identifies possible
software regressions, but is not applicable to
identifying the root-cause of a given software regression.
\todo{discuss the matching algorithm here}
\todo{check recent MSR papers on differential analysis}

Zhang et al.~\cite{} developed a technique to match entire
execution histories of two program versions running with
the same input~\cite{}. The execution history contains control
flow taken, values produced, addresses referenced and
data dependences. Significantly different than \ourtool, their
work assumes semantically equivalent versions (e.g. optimized
and unoptimized) while \ourtool compares different version of
a program that can include functional changes. 
\todo{missing above the work of semantic trace analysis,
some of tao wang's work}

When a regression error occurs, developers need
to understand its root cause~\cite{Banerjee:2010:GID, r2fix}. To address this
problem, many techniques to localize error-inducing
changes are developed for evolving
software. For example, Delta Debugging aims to find a minimal
subset of changes that still makes the test fail~\cite{}.
Test minimization techniques~\cite{} simplify the failed
test to ease comprehension for developers. Recently,
Qi et al.~\cite{} proposed a symbolic execution-based
error diagnosis approach to synthesize new inputs that
differ marginally from the failing input in their
control flow behavior, then compare the execution traces
of the failing input and the new inputs to obtain critical
clues to the root-cause of the failure. \ourtool
is significantly different from the other approaches.
First, \ourtool targets software configuration errors
casued by evolution rather than a regression bug.
As we discussed in introduction, configuration errors are
often user driven and often do not indicate problems in the source
code. Second, most of the existing techniques identify
\textit{what} causes the regression error but leave
the more challenging question of \textit{how} to
fix the error unanswered. By contrast, \ourtool
explicitly guides users to suspicious configuration options.
Third, most of the regression error diagnosis techniques
requires users to provide a testing oracle, while \ourtool
eliminates this assumption.

\todo{check related work of using history for
bug predication~\cite{Nagappan:2006:UHI}
and evolution~\cite{Bhattacharya:2012:GAP, Nguyen:2010:GAA}}
todo{discuss papers~\cite{Grechanik:2009:MEG, Fischer:2005:SET, Mostafa:2009:TPA,
publication-8111}}


\subsection{Software Configuration Error Diagnosis}

Software configuration problems are time-consuming
and frustrating to diagnose. To reduce the time and human
effort needed to troubleshoot software misconfigurations,
several prior research efforts have applied different techniques
to the problem of configuration error diagnosis~\cite{Attariyan:2008:UCD, 
Whitaker:2004:CDS, Wang:2004:AMT, rangefix,
Attariyan:2010:ACT, Rabkin:2011:PPC, keller:conferr}.
Chronus~\cite{Whitaker:2004:CDS} relies on a user-provided
testing oracle to check the behavior of the system, and uses
virtual machine checkpoint and binary search to find the
point in time where the program behavior
switched from correct to incorrect. AutoBash~\cite{Su:2007:AIC}
fixes a misconfiguration by using
OS-level speculative execution to try possible
configurations, examine their effects, and roll them back when necessary.
PeerPressure~\cite{Wang:2004:AMT} 
uses statistical methods to compare
configuration states in the Windows Registry on different machines.
When a registry entry value on a machine exhibiting erroneous behavior differs
from the value usually chosen by other machines, PeerPressure
flags the value as a potential error. More recently, ConfAid~\cite{Attariyan:2010:ACT}
uses dynamic taint analysis to diagnose configuration problems 
by monitoring causality within the program binary as it executes.
\todo{discuss X-ray~\cite{xray} here}
ConfAnalyzer~\cite{Rabkin:2011:PPC} uses dynamic information flow analysis to precompute
possible configuration error diagnoses for every possible crashing point
in a program. 

Our technique is significantly different from the other approaches.
First, most previous approaches focus exclusively on a single software
version and diagnose configuration errors that lead to a crash or
assertion failure~\cite{Attariyan:2008:UCD, Whitaker:2004:CDS, 
Attariyan:2010:ACT, Rabkin:2011:PPC}.
\todo{multiple versions}
Second, several approaches~\cite{Attariyan:2010:ACT, Whitaker:2004:CDS}
assume the existence of a testing oracle that can 
check whether the software functions correctly. However,
such oracles are often
absent in practice or may not apply to the specific configuration problem.
A typical software user should not be expected, to invest the substantial
time and effort to create an oracle.
By contrast, our technique eliminates this assumption by
\todo{comparing traces} Third, approaches like
PeerPressure~\cite{Wang:2004:AMT} and \todo{xiong's paper} benefit from
the known schema of the Windows Registry and
\todo{xx}, but cannot detect configuration errors
that lie outside these specific domains. Our technique
of analyzing the execution traces is more general.

In our previous work~\cite{}, we proposed \prevtool, an automated
software configuration error diagnosis tool. Besides working
on two different versions, \ourtool differs from \prevtool in
two additional key aspects. First, unlike \prevtool, \ourtool
does not require users to collect a set of correct execution traces
for comparison. Second, 
First, \todo{versions ...}. Second, \todo{assume hot spot}.
\todo{may refer to experiment results here}
\todo{missing above, discuss slicing-based techniques, dual slicing;
why it cannot work, since needs a slicing point}
\todo{discuss existing debugging techniques like tarantula~\cite{Jones:2002}
cannot work.}




\subsection{Configuration-Aware Software Analysis}

Empirical studies show that configuration errors are pervasive, costly,
and time-consuming to diagnose~\cite{Yin:2011:ESC, Hubaux:2012}.
\todo{say the empirical study results of this paper.}
To alleviate this problem, researchers have designed various
software analysis techniques to improve configuration
management~\cite{Garvin:2011} and understand and test
the behavior of a configurable software
system~\cite{Garvin:2011, Qu:2008:CRT}.
For example, Garvin et al.~\cite{Garvin:2011} show how to
use historical data to avoid system failures during reconfiguration.
However, their work aims to reduce the burden of
configuration management and prevent certain errors from happening,
while our technique is designed to diagnose an exhibited problem.
Qu et al.~\cite{Qu:2008:CRT} proposed a combinatorial interaction
testing technique to model and generate configuration samples for
use in regression testing. Compared to \ourtool, those techniques
can be used to find new errors in
a configurable software system earlier, but cannot
identify the root cause of a revealed configuration error.
\todo{differences} By contrast, our technique is designed
to diagnose an exhibited error.
\todo{none of them aim to find regression}

Software configuration management is a central component
of software product lines. In the literature of software
product line research, techniques for analyzing
software product lines~\cite{Bodden:2013:SLS, Kang:2005:FRL, Mende:2008:SGM, Kruger:2005:SAE}, reducing configuration
combinatoric~\cite{fse-splat, Apel:2009:FLA, Shang:2013:ADB, Staats:2011:PTO}, modelling
configurations~\cite{Denaro:Self-Test:TACOS:2003, Acher:2012:SCF}
managing configurations~\cite{Rabiser:2012:QSU, Cooray:2010:RRD, Barreiros:2009:MRC, TerBeek:2011:GCE},
and selecting good configurations~\cite{reconfig}
have been developed. However, none of the existing techniques
can be directly used to localize root causes of a configuration error.
\todo{more discussion above}
