\section{Related Work}
\label{sec:related}

The most closely related work falls into
three categories; (1) techniques for
supporting software evolution; (2) software
configuration error diagnosis techniques;
%(3) automated software debugging techniques;
and (3) configuration-aware software analysis techniques.

\subsection{Supporting Software Evolution}

As software evolves, its behavior must be validated.
Regression test selection~\cite{regression}
indicates which tests need to be executed for a changed
program.  Program differencing techniques~\cite{Giroux:2006:DIF, Xing:2005:UAO, Thummalapenta:2010:ESM, Kim:2013, Jin:2012:BRF,Nguyen:2010:RBF,Dig:2006:ADR, Kamiya:2002:CMT, Dagenais:2008}
identify changes between two program versions,
and present the change list to developers for inspection.
Related, change impact analysis techniques~\cite{STVR:STVR1475}, which
are often built on top of program differencing
techniques, identify not only the changes, but also
code fragments that are affcted by the changes. 
Different than \ourtool's predicate matching
algorithm (Section~\ref{sec:match_predicate}),
existing program differencing techniques primarily focus on matching
program elements at the method level~\cite{frameworkevolution,
Xing:2005:UAO, Kim:2013, Nguyen:2010:RBF,Dig:2006:ADR,
Kamiya:2002:CMT, Dagenais:2008},
or matching program statements on the source code based on
textual similarity~\cite{Horwitz:1990:IST}.
By contrast, \ourtool's matching algorithm, inspried by
the JDiff algorithm~\cite{Apiwattanapong:2004}, is specifically designed 
to match the evolved predicate in the new program version.
(See Section~\ref{sec:match_predicate} for a detailed
comparison with JDiff.)
The algorithm directly works on the bytecode of two program
versions without any additional information from users,
such as a software revision history~\cite{frameworkevolution}.

%these techniques can identify possible
%software regressions, but are not applicable to
%identifying the root-cause of a given software regression.
%\todo{discuss the matching algorithm here}
%\todo{check recent MSR papers on differential analysis}

Zhang et al.~\cite{Zhang:2005:MEH} developed a technique to match
control flows of two program versions running with
the same input. Differing from \ourtool, their
work assumes semantically equivalent program versions (e.g. optimized
and unoptimized), while \ourtool compares two versions
of the same program that include functional changes. 
%\todo{missing above the work of semantic trace analysis,
%some of tao wang's work}

When a regression failure occurs, developers need
to understand its root cause. To localize the
failure, many techniques are developed to identify the
failure-inducing changes for evolving
software~\cite{Banerjee:2010:GID, r2fix, Qi:2009:DAD, Hoffman:2009:STA}.
For example, Delta Debugging aims to find a minimal
subset of changes that still makes the test fail~\cite{dd}.
Test minimization techniques~\cite{Hoffman:2009:STA, Zhang:2013:PST}
simplify the failed
test to ease comprehension for developers. 
\ourtool is differs from these techniques in three aspects.
First, existing techniques focus on helping software developers
localize a software bug, while \ourtool targets software
configuration errors that are fixable by software end-users.
As we have discussed in Section~\ref{sec:tech_discuss},
configuration errors are fundmanetally different than regression bugs.
They are mostly user driven and do not indicate problems in the source
code. Second, most of the existing techniques identify
\textit{what} (e.g., a snippet of code) causes the
regression bug, but leave
the more challenging question of \textit{how} (e.g., which
configuration option should a user change?) to
fix the error unanswered. By contrast, \ourtool
explicitly guides users to suspicious configuration options.
Third, most of the regression failure localization
techniques~\cite{dd} require 
a testing oracle for automated correctness checking. However,
such oracles are often absent in practice. A typical
software end user should not be expected to invest
the time and effort to create an oracle.
By contrast, \ourtool eliminates this requirement by
approximating the software behavioral difference as the control
flow differences.
%\todo{check related work of using history for
%bug predication~\cite{Nagappan:2006:UHI}
%and evolution~\cite{Bhattacharya:2012:GAP, Nguyen:2010:GAA}}
%5todo{discuss papers~\cite{Grechanik:2009:MEG, Fischer:2005:SET, Mostafa:2009:TPA,
%publication-8111}}


\subsection{Software Configuration Error Diagnosis}

Software configuration errors are time-consuming
and frustrating to diagnose. To reduce the time and human
effort needed to troubleshoot software misconfigurations,
several prior research efforts have applied different techniques
to the problem of configuration error diagnosis~\cite{Attariyan:2008:UCD, 
Whitaker:2004:CDS, Wang:2004:AMT, rangefix,
Attariyan:2010:ACT, Rabkin:2011:PPC, keller:conferr}.
For example, Chronus~\cite{Whitaker:2004:CDS} relies
on a user-provided testing oracle to check the behavior
of the system, and uses
virtual machine checkpoint and binary search to find the
point in time where the program behavior
switched from correct to incorrect. AutoBash~\cite{Su:2007:AIC}
fixes a misconfiguration by using
OS-level speculative execution to try possible
configurations, examine their effects, and roll them back when necessary.
PeerPressure~\cite{Wang:2004:AMT} 
uses statistical methods to compare
configuration states in the Windows Registry on different machines.
When a registry entry value on a machine exhibiting erroneous behavior differs
from the value usually chosen by other machines, PeerPressure
flags the value as a potential error. More recently, 
ConfAid~\cite{Attariyan:2010:ACT} and X-Ray~\cite{xray}
use dynamic taint analysis to diagnose configuration errors
by monitoring causality within the program binary as it executes.
ConfAnalyzer~\cite{Rabkin:2011:PPC} uses dynamic information flow analysis to precompute
possible configuration error diagnoses for every possible crashing point
in a program. 

\ourtool is significantly different from the existing approaches.
First, most previous approaches is not cognizant of software evolution;
they focus exclusively on a single software
version~\cite{Attariyan:2008:UCD, Whitaker:2004:CDS, 
Attariyan:2010:ACT, Rabkin:2011:PPC}. By contrast, \ourtool focuses
on configuration errors introduced by software evolution between
two software versions. Further, most techniques
can only diagnose configuration errors that lead to a crash or
assertion failure~\cite{Attariyan:2008:UCD, Whitaker:2004:CDS, 
Attariyan:2010:ACT, Rabkin:2011:PPC}, while \ourtool supports diagnosing both
crashing and non-crashing errors.
Second, several approaches~\cite{Attariyan:2010:ACT, Whitaker:2004:CDS}
assume the existence of a testing oracle that can 
check whether the software functions correctly. However, as we discussed
before, a testing oracle is often not availabe, and it
is infeasible to expert end-users to provide it.
%By contrast, our technique eliminates this assumption by
%comparing two execution traces to reason about the control flow differences
%between them. 
Third, several techniques~\cite{Whitaker:2004:CDS, Su:2007:AIC} require alternations
to the underlying operating system or runtime environment
in order to diagnose a configuration error. Compared with
the platform-independent offline instrumentation used
in \ourtool, switching to use a new OS or runtime may be
too impractical for many ordinary end-users.
Fourth, approaches like
PeerPressure~\cite{Wang:2004:AMT} and RangeFixer~\cite{rangefix}
benefit from the known schema of the Windows Registry and
feature models, but cannot help diagnose configuration errors
that lie outside these specific domains. Our technique
of analyzing the execution traces is more general.


\subsection{Configuration-Aware Software Analysis}

Software configuration management is a central component
of software product lines.
Many configuration-aware software analysis techniques
have been developed to analyze configurable software
systems~\cite{Bodden:2013:SLS, Kang:2005:FRL, Mende:2008:SGM,
Kruger:2005:SAE}, improve software configuration
management~\cite{Garvin:2011, Rabiser:2012:QSU, Cooray:2010:RRD,
Barreiros:2009:MRC, TerBeek:2011:GCE}, and understand and test
the behavior of a configurable software
system~\cite{Qu:2008:CRT, SPLAT, Apel:2009:FLA, Shang:2013:ADB,
Staats:2011:PTO}.

Compared to \ourtool, existing techniques have rather different
goals. They primarily focus on reducing the burden of
configuration management and preventing certain errors from
happening, or creating test suites to find new errors in
a configurable software system earlier, but cannot
diagnose an exhibit configuration error. By contrast, our \ourtool technique
links the behavioral differences to a small number
of configuration options, and explicitly guides software end-users
how to fix a configuration error.



%Empirical studies show that configuration errors are pervasive, costly,
%and time-consuming to diagnose~\cite{Yin:2011:ESC, Hubaux:2012}.
%Qu et al.~\cite{Qu:2008:CRT} proposed a combinatorial interaction
%testing technique to model and generate configuration samples for
%use in regression testing. 

%identify the root cause of a revealed configuration error.
%\todo{differences} By contrast, our technique is designed
%to diagnose an exhibited error.
%\todo{none of them aim to find regression}

%However, their work aims to reduce the burden of
%configuration management and prevent certain errors from happening,
%while our technique is designed to diagnose an exhibited problem.

