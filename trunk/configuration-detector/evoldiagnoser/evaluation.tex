
\section{Evaluation}
\label{sec:evaluation}

We evaluated 4 aspects of \ourtool's effectiveness, answering
the following research questions:

\vspace{-2mm}

\begin{enumerate}
\item How accurate is \ourtool in identifying the root-cause
configuration options? That is, what is the rank of the
actual root-cause configuration option in \ourtool's output (Section~\ref{sec:accuracy})?

\item How long does it take for \ourtool to diagnose
a configuration error (Section~\ref{sec:timecost})?

\item How does \ourtool's effectiveness compare to
existing approaches (Section~\ref{sec:existing})?

\item How does \ourtool's effectiveness compare to
two variants? The first variant
uses full slicing in identifying
suspicious configuration options, and the second 
variant only uses predicate behavior
changes to recommend configuration options (Section~\ref{sec:alternative}).

\end{enumerate}

\subsection{Subject Programs}

We evaluated \ourtool on \subjnum Java programs
listed in Figure~\ref{tab:subjects}.
The first 5 subject programs are the 5 Java programs
studied in Section~\ref{sec:study},
and the remaining subject program is Javalanche~\cite{javalanche},
which is a mutation testing framework.

We included Javalanche because one of its real users
% we received a real
%configuration error from one of its end-users.
provided us a configuration error he encountered when using Javalanche.

\newcommand{\randoopoptnum}{57\xspace}
\newcommand{\wekaoptnum}{14\xspace}
\newcommand{\synopticoptnum}{37\xspace}
\newcommand{\jchordoptnum}{79\xspace}
\newcommand{\jmeteroptnum}{55\xspace}
\newcommand{\javalancheoptnum}{35\xspace}

\newcommand{\randooprank}{1\xspace}
\newcommand{\wekarank}{1\xspace}
\newcommand{\synopticrankfirst}{1\xspace}
\newcommand{\synopticranksecond}{6\xspace}
\newcommand{\jchordrankfirst}{1\xspace}
\newcommand{\jchordranksecond}{1\xspace}
\newcommand{\jmeterrank}{1\xspace}
\newcommand{\javalancherank}{3\xspace}

\newcommand{\averagerank}{1.8\xspace}

\begin{figure}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.20\tabcolsep}
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
 Program & Old Version & New Version & LOC (new version) & $\Delta$LOC & \#Options \\
 \hline
 \hline
 Randoop & 1.2.1 & 1.3.2 &18571&1893& \randoopoptnum  \\
 Weka & 3.6.1 & 3.6.2 &275035& 1458 & \wekaoptnum \\
 Synoptic & 0.05 & 0.1 &19153& 1658 & \synopticoptnum \\
 JChord & 2.0 & 2.1&26617& 3085 & \jchordoptnum \\
 JMeter & 2.8 & 2.9 &91979& 3264 &  \jmeteroptnum \\
 Javalanche & 0.36 & 0.40 & 25144 &9261& \javalancheoptnum \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:experiment-sub} All subject programs
used in the evaluation. Column ``$\Delta$LOC'' shows
the number of changed lines of code between the old and new versions.
Column ``\#Options'' shows the number of configuration options
supported in the new program version.
}
}
\end{figure}

\subsubsection{Configuration Errors}


\input{program-table}

For the 5 Java programs studied in Section~\ref{sec:study},
we manually examined all deleted and modified configuration
options listed in Figure~\ref{tab:options}. (The added
configuration options are unlikely to cause a misconfiguration.) For each
change, based on our own understanding, we wrote a test driver to cover
it, and then checked whether the test driver
could reveal different behaviors on two versions.
For those 5 programs, we collected 7 errors as listed in
Figure~\ref{tab:errors} (the first 7 errors).
For the Javalanche program, we reproduced the reported configuration
error.
We evaluated all configuration errors that we could find and reproduce;
we did not select only errors on which \ourtool works well.
In Figure~\ref{tab:errors}, errors \#3 and \#4
can be reproduced together in a single execution, and each of the other
errors is reproduced in one execution.

Our methodology of collecting
configuration errors is different from what was used in
collecting software regression bugs in the literature~\cite{dd, autoflow}.
%Differing from configuration errors,
Software regression bugs often can be found in well-maintained
bug databases. By contrast, finding recorded configuration errors
is much harder, mainly because most configuration errors have not been
documented rigorously~\cite{Yin:2011:ESC}. Usually, after a
session of code changes, when regression tests pass, developers
may treat the software behaviors as having been validated. Further,
because the software misconfigurations are user-driven,
the ``fixes'' may be recorded simply as pointers
to manuals or other documents. 


%Therefore, in
%our evaluation, we have to manually reproduce 

%\todo{cover different types of options, and different types of errors}


%We collected \errornum configuration errors caused by
%software evolution. \todo{reasons of relatively few
%errors}. 

\subsection{Evaluation Procedure}

For each subject program, we used \ourtool to instrument both versions. 
For each configuration error, we used the same 
input and configuration
to reproduce the different behaviors on two instrumented versions.

The average size of the execution traces is 40MB,
and the largest one (Randoop's trace) is 140MB.



When using \ourtool to diagnose a configuration error, we manually specify
the initialization statement of each configuration option as
the seed for thin slicing. This manual, one-time-cost step took 
20 minutes on average per subject program. After that,
\ourtool works in a fully-automatic way: it 
analyzes two program versions and two execution traces,
and outputs a ranked list of configuration options.
Future work could automate this manual step.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

\subsubsection{Accuracy}
\label{sec:accuracy}

As shown in Figure~\ref{tab:errors}, \ourtool is highly effective
in identifying the root-cause configuration options that should
be changed in the new program version. The average rank of
the root cause in \ourtool's output is 1.8. For 6 errors, the
root-cause configuration option ranks first in \ourtool's output;
for 1 error, the root-cause configuration option ranks third
in \ourtool's output; and the root-cause option ranks sixth
for the remaining error. \ourtool is successful because of its
ability to identify the behaviorally-deviated predicates with
substantial impacts through execution trace comparison.
The top-ranked deviated predicates often provide
useful clues about what parts of a program have performed
differently.

%\todo{show one more example to illustrate its effectiveness}

%\ourtool fails to recommend correct options for one error in
%JChord (error \todo{xx}). This is because the predicate matching
%algorithm outputs a wrong matching result.

%\todo{show some code here}

\vspace{1mm}

\noindent \textbf{\textit{Summary.}} \ourtool
recommends correct configuration options with
high accuracy for evolving configurable software systems
with non-trivial code changes.

\subsubsection{Performance of \ourtool}
\label{sec:timecost}

We measured \ourtool's performance in two ways:
the performance overhead introduced by instrumentation
when demonstrating the configuration error,
and the time cost of recommending configuration options.
Figure~\ref{tab:performance} shows the results.

The performance overhead to demonstrate the error
varies among programs. The current implementation
imposes an average 8$\times$ and 12.8$\times$ slowdown in a
\ourtool-instrumented old and new program version, respectively.
This is due to \ourtool's inefficient instrumentation code that
monitors the execution of every instruction.
The overhead could be reduced by instrumentating
at basic block granuarlity instead.
Even so, except for two errors (errors \#5 and \#6) in\
JChord,  all other errors can
be reproduced in less than 30 seconds. Errors \#5 and \#6
require about 20 minutes to reproduce.

%the different behaviors can be reproduced
%in less than XXX minutes on average on both versions, 
%with a worse case of XXX minutes.


\ourtool spends an average of \avgtime minutes
to recommend configuration options for one
error (including the time to compute thin slices
and the time to suggest suspicious options). 
Computing thin slices for all configuration options
is non-trivial. However, this step is one-time cost
per program and the results can be precomputed.
The time used for suggesting configuration options
is roughly proportional to the size of the execution trace
rather than the size of the subject program.

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} \ourtool
recommends configuration options for diagnosing
configuration errors with reasonable time cost.

\begin{figure}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||c|c||c|c|}
\hline
 Error ID.& \multicolumn{2}{|c||}{Run-time Slowdown ($\times$)} & \multicolumn{2}{|c|}{\ourtool time (s)}\\
 \cline{2-5}
 Program& Old Version & New Version & Slicing & Suggestion\\
 \hline
 \hline
 1. Randoop & 20.1 & 4.1 & 90 & 295 \\
 2. Weka & 1.6 & 1.6 & 80 & 49 \\
 3. Synoptic & 1.7 & 4.7 &48 & 42 \\
 4. Synoptic & 1.7 & 4.7 &48  & 42  \\
 5. JChord & 18.7 & 44.3  & 20 & 38 \\
 6. JChord & 17.6 & 41.1 & 23 & 29 \\
 7. JMeter & 1.3 & 1.4 &51 & 63 \\
 8. Javalanche& 1.4 & 1.5 & 430 & 265\\
\hline
\hline
 Average & 8.0 & 12.8 & 99 & 91 \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:performance} \ourtool's
performance. The ``Run-time Slowdown'' column
shows the cost of reproducing the error in
an \ourtool-instrumented version of the subject program.
The ``\ourtool time (s)'' column shows
the time taken by \ourtool to diagnose configuration errors in seconds.
Column ``Slicing'' is the cost of computing
thin slices on both old and new program versions.
%For both columns, the mean is the geometric mean. 
}
}
\end{figure}

\subsubsection{Comparison with Two Existing Approaches}
\label{sec:existing}

This section compares \ourtool with two existing approaches,
\prevtool~\cite{Zhang:2013:ADS} and \conftool~\cite{Rabkin:2011:PPC}.
\prevtool and \conftool are among the most
precise configuration error diagnosis techniques in the literature.


\noindent\textbf{\prevtoolnoxspace}, proposed in our previous work~\cite{Zhang:2013:ADS},
is an automated
software configuration error diagnosis technique. %Unlike \ourtool,
\prevtool is \textit{not} cognizant of
software evolution, and it diagnoses configuration errors from
a single program version.
\prevtool assumes the existence of a set of correct execution
traces, which are used to compare against the undesired
execution trace to identify the abnormal program parts.
When comparing the undesired execution trace with a
correct execution trace, \prevtool only uses a predicate's deviation
value to reason about the most suspicious options, while
ignoring the statements controlled by a predicate's evaluation result. 

To compare \ourtool with \prevtool, 
we reused the pre-built execution trace databases
for the 4 shared subject programs (Randoop, Synoptic, JChord, and Weka)
from~\cite{Zhang:2013:ADS}.
Each existing trace database contains 6--16 correct
execution traces.
For the remaining two subject programs (JMeter and
Javalanche), we manually built an execution trace database
for each of them by running correct examples from their user manuals.
The databases contain 6 and 8
execution traces for JMeter and Javalanche, respectively.

\vspace{1mm}

\noindent \textbf{\conftoolnoxspace}, proposed by Rabkin and
Katz~\cite{Rabkin:2011:PPC}, is a lightweight
static configuration error diagnosis technique.
\conftool tracks the flow of labeled objects through 
program control flow and data flow, and treats a configuration option
as a root cause if its value may flow to a crashing point.
Since \conftool cannot diagnose non-crashing errors, we
can only apply it to diagnose the crashing error in
Weka (error \#2 in Figure~\ref{tab:errors}).


\vspace{1mm}
\noindent \textbf{\textit{Results.}}
Columns ``\prevtool'' and ``\conftool'' in Figure~\ref{tab:errors} show
the experimental results.
%\todo{show the results here}

\ourtool produces significantly more accurate results than \prevtool,
primarily for two reasons. First, \prevtool focuses on diagnosing
\textit{erroneous} program behaviors and identifies their responsible
configuration options. However, for the problem addressed in this paper,
the new software version that exhibits \textit{undesired} behavior 
(after applying the same configuration used in the old version)
is working exactly as \textit{designed}. In other words, the execution
trace obtained by running the new program version is still
\textit{correct}. Therefore, just comparing execution traces obtained
from the new program version is not effective in identifying
the ``abnormal'' behavior. By contrast, \ourtool compares execution
traces from two different versions and directly reasons about the
execution differences. Second, \prevtool only focuses on
the predicate behavior changes, while ignoring the statements
potentially impacted by the affected predicate. This makes
\prevtool fail to distinguish predicates whose behavioral changes can
have different impacts.
Section~\ref{sec:alternative} further evaluates this design
choice, showing that considering the number of controlled statements
can substantially increase the diagnosis accuracy.

\conftool outputs the correct result for the crashing error in Weka,
but cannot identify root causes for other non-crashing errors.
The crashing error in Weka occurs soon after the program
is launched. \conftool correctly identifies its root cause because
a small number of configuration options
are initialized and only one of them flows to the crashing point.


%since
%it exclusively focuses on diagnosing crashing configuration errors.
%By contrast, \ourtool is capable to diagnose both crashing
%and non-crashing errors.

\ourtool is not directly comparable to other related
configuration error diagnosis approaches~\cite{Attariyan:2010:ACT,
xray, Whitaker:2004:CDS, Su:2007:AIC, Wang:2004:AMT, rangefix}.
Existing approaches target a rather
different problem than \ourtool, or require different
inputs than \ourtool. For example, 
X-Ray~\cite{xray} diagnoses configuration errors on a single
program version.
%Chronus~\cite{Whitaker:2004:CDS}
%and AutoBash~\cite{Su:2007:AIC}
%require OS-level support for capture and replay.
PeerPressure~\cite{Wang:2004:AMT} and RangerFixer~\cite{rangefix} only
support configuration options defined by certain
specific feature models. 
%It is unknown whether
%such techniques can be extended to diagnose configuration
%errors handled by \ourtool. 
General software fault localization techniques~\cite{Jones:2002, McCamant:2003}
are not well-suited for configuration error diagnosis, since such techniques
often focus on identifying the buggy code or
invalid input values. This has been empirically
validated in our previous work~\cite{Zhang:2013:ADS}.


\vspace{1mm}

\noindent \textbf{\textit{Summary.}} Configuration
error diagnosis techniques designed for a \textit{single}
program version achieve less accurate results in
%cannot be directly applied to 
diagnosing configuration errors introduced in software evolution.
\ourtool reasons about the behavioral differences between
two program versions, and produces more accurate results.


\begin{figure}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{1.10\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Error ID. & \multicolumn{3}{|c|}{Rank of the Root-Cause Configuration Option}  \\
\cline{2-4}
 Program & \ourtool & Full Slicing & Predicate Behavior  \\
 \hline
 \hline
 1. Randoop & \randooprank & 32 & 7 \\
 2. Weka & \wekarank & 7 & 1 \\
 3. Synoptic & \synopticrankfirst & 16 & 3\\
 4. Synoptic & \synopticranksecond & 17 & 8 \\
 5. JChord & \jchordrankfirst & 19 & 5 \\
 6. JChord & \jchordranksecond & 30 & 5 \\
 7. JMeter & \jmeterrank & \n & 1 \\
 8. Javalanche & \javalancherank & \n & 13 \\
\hline
\hline
 Average & \averagerank  & 20.7 & 5.4 \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:choices} 
Experimental results of evaluating two design choices
of \ourtool. Column ``\ourtool'' shows \ourtool's
results, taken from Figure~\ref{tab:errors}.
Column ``Full Slicing'' shows the results of replacing
thin slicing with full slicing in \ourtool.
``\n'' means the technique does not identify the actual
root cause. Column ``Predicate Behavior'' shows the results of 
\ourtool, if it only considers predicate behavior change.
When computing the average rank, each ``\n'' is treated
as half of the number of configuration options.
}
}
\end{figure}

\subsubsection{Evaluating Two Design Choices}
\label{sec:alternative}

This section evaluates two design choices in \ourtool.

\vspace{1mm}

\noindent \textbf{{Slicing algorithms.}} \ourtool
uses thin slicing to identify configuration options
whose values may affect a predicate. We next evaluate
a variant that replaces thin slicing with
the traditional full slicing~\cite{Horwitz:1988}.
This variant changes the getAffectingOptions
auxiliary function in Figure~\ref{fig:recommend}, by using full slicing to compute all
configuration options that may affect a predicate.
Figure~\ref{tab:choices} (Column ``Full Slicing'') shows the results.


\ourtool achieves substantially less accurate results when
using full slicing. The primary reason is that full slicing
identifies many irrelevant configuration options that \textit{indirectly}
affect a predicate of interest. Such configuration options
are not pertinent to the task of error diagnosis. Linking them
to the exhibited different behavior would degrade
\ourtool's accuracy. Further, computing full slices is much
more expensive than computing thin slices. 
WALA's full slicing algorithm failed to scale
to two subject programs (JMeter and Javalanche).

\vspace{1mm}

\noindent \textbf{Predicate behavioral change metrics.}
\ourtool considers both the predicate behavior change
and the number of affected statements in diagnosing
configuration errors. We next evaluate a variant
that only uses the predicate behavior change to diagnose errors.
This variant 
changes the getExecutedStmtNum auxiliary function in
Figure~\ref{fig:recommend}, by making it always return 1.
Figure~\ref{tab:choices} (Column ``Predicate Behavior'') shows
the results. 



\ourtool's accuracy degrades substantially when ranking
predicates based on its behavioral changes without considering
the number of affected statements.
The primary reason is that behaviorally-deviated predicates occur all over the execution traces,
but each predicate
may have different impacts to the overall program behavior change.
\ourtool uses the number of statements determined by the predicate
evaluation result to approximate such potential impacts.

%More statements a predicate's evaluation result can impact,
%he more likely that predicate is an important 
%behavior metric\todo{need rewording}, but they have different
%impacts to the overall program behavior change. 
%\todo{We also consider other metrics to measure predicate deviations}

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} Full slicing
includes too many irrelevant program statements due to its
conservatism and only using a predicate's behavior
change is not enough to identify the root-cause configuration options.
\ourtool, using thin slicing and considering both
the predicate behavior change and the impacted statements,
is a better choice in diagnosing configuration errors.

\subsection{Discussion}

\noindent \textbf{\textit{Threats to validity.}}
There are several threats to validity of our evaluation.
First, the \subjnum Java programs might not be representative, though some of them have been
used in previous research. Likewise, the
\errornum configuration errors might not be representative, even though we
evaluated every error we found. 
We only evaluated \ourtool on errors
caused by one configuration option.
It is unclear whether \ourtool would produce
useful results if fixing a particular configuration error
requires changing values of two dependent configuration options.
%
%First, \ourtool currently assumes that only one
%configuration option is responsible for a given undesired behavior,
%although it can diagnose two errors caused by two independent options
%at a time (Section~\ref{sec:evaluation}).
Second, our evaluation focused on configuration errors
rather than software regression bugs, as all regression
tests between two versions pass. We have not evaluated
whether \ourtool would help users work around
buggy program versions.
Third, \ourtool's effectiveness depends on the effectiveness of the
predicate-matching algorithm. In our experiments,
on average 12\% of lines are changed between the program
versions. \ourtool may yield less
useful results for programs with significant code changes.
However, different algorithms can be plugged into \ourtool.
Fourth, our evaluation only compared \ourtool with two other
approaches. Comparing with other analyses or tools might yield
different observations.
%Fourth, our evaluation focuses on \ourtool's algorithm for
%configuration option recommendation. A future user study should
%evaluate whether \ourtool helps users.

\vspace{1mm}

\noindent \textbf{\textit{Experimental conclusions.}}
We have three chief findings. (1) \ourtool is highly effective
in diagnosing configuration errors introduced by software 
evolution; (2) \ourtool produces more accurate results than
approaches designed to diagnose errors on a single program version;
and (3) \ourtool outperforms two variants
that use full slicing and only  a predicate's
behavior change in error diagnosis, respectively.
%using thin slicing and combining the behavioral
%difference of a predicate and its affected statements
%permit \ourtool to produce more accurate diagnosis.

