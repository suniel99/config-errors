
\section{Evaluation}
\label{sec:evaluation}

We evaluated 4 aspects of \ourtool's effectiveness, answering
the following research questions:

\begin{enumerate}
\item How accurate is \ourtool in diagnosing configuration errors
caused by software evolution? That is, what is the rank of the
actual root cause configuration option in \ourtool's output (Section~\ref{sec:accuracy})?

\item How long does it take for \ourtool to diagnose
a configuration error (Section~\ref{sec:timecost})?

\item How does \ourtool's effectiveness compare to
existing approaches (Section~\ref{sec:existing})?

\item How does \ourtool's effectiveness compare to
an alternative approach using full slicing in recommending
configuration options (Section~\ref{sec:alternative})?

\end{enumerate}

\subsection{Subject Programs}

We evaluated \ourtool on \subjnum Java programs
listed in Table~\ref{tab:subjects}.
The top 5 subject programs are the 5 Java programs
in our empirical study (Section~\ref{sec:study}),
and the remaining subject program Javalanche~\cite{javalanche}
(and its configuration error) is provided
by its real-world user.


\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||c|c|c|c|c|}
\hline
 Program & Old Version & New Version & $\Delta$LOC & \#Options \\
 \hline
 \hline
 Randoop &  &  &&   \\
 Weka &  & & &  \\
 Synoptic &  & & &  \\
 JChord &  & & &  \\
 JMeter &  & & &  \\
 Javalanche &  & & &  \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:experiment-sub} All subject programs
used in the evaluation. Column ``$\Delta$LOC'' shows
the number of changed lines of code between the old and new versions.
Column ``\#Options'' shows the number of configuration options
supported in the new program version.
}
}
\end{table}

\subsubsection{Configuration Errors}

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||l|l|}
\hline
 Error ID& Program & Description\\
 \hline
 \hline
 1 & Randoop  & An order of magnitude less tests generated \\
 2 & Weka &  A different error message when Weka crashes\\
 3 & Synoptic & The generated initial model not saved\\
 4 & Synoptic & The generated model not saved as JPEG file \\
 5 & JChord & Bytecode parsed incorrected \\
 6 & JChord &  Method names not printed in the console\\
 7 & JMeter &  Results saved to a file with different formats\\
 8 & Javalanche &  No mutants generated\\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:errorlist} All configuration errors used in the evaluation.
Only the 2-nd error is a crashing errors, and all the other errors are non-crashing
errors. }
}
\end{table}

\input{program-table}

For the 5 Java programs used in our empirical study,
we manually examinated all configuration-related changes
listed in Table~\ref{tab:options},
and wrote a test driver that reveals different
behaviors on the two corresponding versions for each configuration change.
When reproducing the different behaviors,
we excluded newly-added options, since
they options are only available in the new version
and will not be used by users of the old version.
Second, we excluded option changes that are backward
compatible. Third, for such option changes, if the
new program version already identifies the potential
configuration errors by dumping explicit error messages,
we excluded them. \todo{rephrase above}

For the Javalanche program, we reproduced the configuration
error as provided by the user.

%We collected \errornum configuration errors caused by
%software evolution. \todo{reasons of relatively few
%errors}. 
We evaluated all configuration errors that we can reproduce;
we did not select only errors that \ourtool works well.
Table~\ref{tab:errors} lists all errors.

\subsection{Evaluation Procedure}

\todo{how to specify an option}

When diagnosing a configuration error, we first use
\ourtool to instrument both versions. Then, we reproduce
the different behaviors on both instrumented versions with 
the same input and configuration. After that, \ourtool automatically
analyzed the obtained execution traces as well as the
bytecode of two program versions, and output a ranked
list of configuration options.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

\subsubsection{Diagnosis Accuracy}
\label{sec:accuracy}

\subsubsection{Performance of \ourtool}
\label{sec:timecost}

We measured \ourtool's performance in two ways:
the time cost of diagnosing an error and
the overhead introduced by instrumentation
in reproducing an error.

Table~\ref{tab:performance} shows the results.

\todo{to reduce the overhead, only instrument
every block}

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||c|c||c|c|}
\hline
 Configuration & \multicolumn{2}{|c||}{Run-time Slowdown ($\times$)} & \multicolumn{2}{|c|}{\ourtool time (s)}\\
 \cline{2-5}
 Error ID& Old Version & New Version & Slicing & Suggestion\\
 \hline
 \hline
 1 &  &  & &  \\
 2 &  &  & & \\
 3 &  &  &  &\\
 4 &  &  & & \\
 5 &  &  & & \\
 6 &  &  & & \\
 7 &  &  & & \\
\hline
\hline
 Mean & & & & \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:performance} \ourtool's
performance. The ``Run-time slow down'' column
shows the cost of reproducing the error in
an \ourtool-instrumented version of the subject program.
The ``\ourtool time (seconds)'' column shows
the time took by \ourtool to diagnose configuration errors.
For both columns, the mean is the geometric mean. 
}
}
\end{table}

\subsubsection{Comparison with Existing Approaches}
\label{sec:existing}

\todo{want to compare with ConfDiagnoser and ConfAnalyzer}

In our previous work~\cite{}, we proposed \prevtool, an automated
software configuration error diagnosis tool. Besides working
on two different versions, \ourtool differs from \prevtool in
two additional key aspects. First, unlike \prevtool, \ourtool
does not require users to collect a set of correct execution traces
for comparison. Second, 
First, \todo{versions ...}. Second, \todo{assume hot spot}.
\todo{may refer to experiment results here}
\todo{missing above, discuss slicing-based techniques, dual slicing;
why it cannot work, since needs a slicing point}
\todo{discuss existing debugging techniques like tarantula~\cite{Jones:2002}
cannot work.}


\subsubsection{Comparison with Alternative Approaches}
\label{sec:alternative}

\todo{use ConfDiagnoser's ranking, full slicing, no iterative slicing}

\todo{just identified changed cases...}

\subsection{Discussion}

\noindent \textbf{\textit{Threats to validity.}}
There are several threats to validity of our evaluation.
First, the 6 Java programs might not be representative, though some of them are used
in previous research. research. Likewise, the
8 configuration errors might not be representative, even though we
evaluated every error we found. Second, 
\ourtool's effectiveness depends on the effectiveness of the
predicate matching algorithm. It may yield less
useful results for programs with significant code changes.
However, different algorithms can be plugged into \ourtool.
Third, Our evaluation only compared \ourtool with two other
approaches. Comparing with other analyses or tools might yield
different observations.
Fourth, our evaluation focuses on \ourtool's algorithm for
configuration option recommendation. A future user study should
evaluate whether \ourtool helps users.

\vspace{1mm}

\noindent \textbf{\textit{Experimental conclusions.}}
We have three chief findings. (1) ...  (2) ... and (3)...

