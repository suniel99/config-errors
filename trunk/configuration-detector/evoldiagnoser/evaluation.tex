
\section{Evaluation}
\label{sec:evaluation}

We evaluated 4 aspects of \ourtool's effectiveness, answering
the following research questions:

\begin{enumerate}
\item How accurate is \ourtool in recommending configuration options
to fix configuration errors caused by software evolution? That is, what is the rank of the
actual root cause configuration option in \ourtool's output (Section~\ref{sec:accuracy})?

\item How long does it take for \ourtool to diagnose
a configuration error (Section~\ref{sec:timecost})?

\item How does \ourtool's effectiveness compare to
existing approaches (Section~\ref{sec:existing})?

\item How does \ourtool's effectiveness compare to
alternative approaches using full slicing in identifying
suspicious configuration options, and only using predicate behaviors
to recommend configuration options (Section~\ref{sec:alternative})?

\end{enumerate}

\subsection{Subject Programs}

We evaluated \ourtool on \subjnum Java programs
listed in Table~\ref{tab:subjects}.
The top 5 subject programs are the 5 Java programs
studied in Section~\ref{sec:study},
and the remaining subject program Javalanche~\cite{javalanche}
(and its configuration error), a popular mutation
testing framework, is provided by a Javalanche user.

\newcommand{\randoopoptnum}{57\xspace}
\newcommand{\wekaoptnum}{14\xspace}
\newcommand{\synopticoptnum}{37\xspace}
\newcommand{\jchordoptnum}{79\xspace}
\newcommand{\jmeteroptnum}{55\xspace}
\newcommand{\javalancheoptnum}{27\xspace}

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.20\tabcolsep}
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
 Program & Old Version & New Version & LOC (new version) & $\Delta$LOC & \#Options \\
 \hline
 \hline
 Randoop & 1.2.1 & 1.3.2 &18571&1893& \randoopoptnum  \\
 Weka & 3.6.1 & 3.6.2 &275035& 1458 & \wekaoptnum \\
 Synoptic & 0.05 & 0.1 &19153& 1658 & \synopticoptnum \\
 JChord & 2.0 & 2.1&26617& 3085 & \jchordoptnum \\
 JMeter & 2.8 & 2.9 &91979& 3264 &  \jmeteroptnum \\
 Javalanche & 0.36 & 0.40 & 25144 &9261& \javalancheoptnum \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:experiment-sub} All subject programs
used in the evaluation. Column ``$\Delta$LOC'' shows
the number of changed lines of code between the old and new versions.
Column ``\#Options'' shows the number of configuration options
supported in the new program version.
}
}
\end{table}

\subsubsection{Configuration Errors}

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||l|l|}
\hline
 Error ID& Program & Description\\
 \hline
 \hline
 1 & Randoop  & An order of magnitude less tests generated \\
 2 & Weka &  A different error message when Weka crashes\\
 3 & Synoptic & The generated initial model not saved\\
 4 & Synoptic & The generated model not saved as JPEG file \\
 5 & JChord & Bytecode parsed incorrected \\
 6 & JChord &  Method names not printed in the console\\
 7 & JMeter &  Results saved to a file with different formats\\
 8 & Javalanche &  No mutants generated\\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:errorlist} All configuration errors used in the evaluation.
Only the 2-nd error is a crashing errors, and all the other errors are non-crashing
errors. }
}
\end{table}

\input{program-table}

For the 5 Java programs studied in Section~\ref{sec:study},
we manually examinated all configuration-related changes
listed in Table~\ref{tab:options}. For each
change, we wrote a test driver to cover
it, and then checked whether the test driver
reveals different behaviors on two versions.
\todo{exclude}
When reproducing the different behaviors,
we excluded newly-added options, since
they options are only available in the new version
and will not be used by users of the old version.
Second, we excluded option changes that are backward
compatible. Third, for such option changes, if the
new program version already identifies the potential
configuration errors by dumping explicit error messages,
we excluded them. \todo{rephrase above}

For the Javalanche program, we reproduced the configuration
error as provided by the user.

We evaluated all configuration errors that we can reproduce;
we did not select only errors that \ourtool works well.
Table~\ref{tab:errors} lists all errors.
In Table~\ref{tab:errors}, errors \#3 and \#4
can be reproduced in a single execution, and each of the other
error is reproduced in one execution.


%We collected \errornum configuration errors caused by
%software evolution. \todo{reasons of relatively few
%errors}. 

\subsection{Evaluation Procedure}

We used \ourtool to instrument both versions. 
For each configuration error, we use the same test driver
(with the same input and configuration)
to reproduce the different behaviors on both instrumented versions.

The average size of the produced execution traces is 40MB,
and the largest one (Randoop's trace) is 140MB



When recommending configuration options, we manually specify
the initialization statement of each configuration option as
the seed for thin slicing. This manual step took around
10 minutes on average for each subject program. After that,
\ourtool works in a fully-automatic way: it 
analyzes two program versions and two execution traces,
and outputs a ranked list of configuration options.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

\subsubsection{Accuracy}
\label{sec:accuracy}

As shown in Table~\ref{tab:errors}, \ourtool is highly effective
is recommending the root cause configuration option that should
be changed in the new program version. For all \todo{xxx the
results}

\todo{show one more example to illustrate its effectiveness}

\ourtool fails to recommend correct options for one error in
JChord (error \todo{xx}). This is because the predicate matching
algorithm outputs a wrong matching result.

\todo{show some code here}

\vspace{1mm}

\noindent \textbf{\textit{Summary.}} \ourtool
recommends correct configuration options with
high accuracy for evolving configurable software systems
with non-trivial code changes.

\subsubsection{Performance of \ourtool}
\label{sec:timecost}

We measured \ourtool's performance in two ways:
the performance overhead introduced by instrumentation
in reproducing the error, and the time cost
of recommending configuration options.
Table~\ref{tab:performance} shows the results.

The performance overhead to reproduce the error
varies among programs. The current tool implementation
imposes an average 8X and 12.8X slowdown in a
\ourtool-instrumented old and new program version, respectively.
This is due to \ourtool's inefficient instrumentation code that
monitors the execution of every instruction.
Even so, except for two errors (errors \#5 and \#6) from\
the JChord subject program,  all other errors can
be reproduced in less than 30 seconds. Errors \#5 and \#6
require about 20 minutes to reproduce.

%the different behaviors can be reproduced
%in less than XXX minutes on average on both versions, 
%with a worse case of XXX minutes.


\ourtool spends an average of \todo{xx} minutes
to recommend configuration options for one
error (including time to compute thin slices
and the time to suggest suspicious options). 
Computing thin slices for all configuration options
is non-trivial. However, this step is one-time cost
per program and the computed results can
be cached to share across runs. 
The time used for suggesting configuration options
is roughly proportional to the size of the execution trace
rather than the size of the subject program.

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} \ourtool
recommends configuration options for fixing
configuration errors with acceptable time cost.

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||c|c||c|c|}
\hline
 Error ID.& \multicolumn{2}{|c||}{Run-time Slowdown ($\times$)} & \multicolumn{2}{|c|}{\ourtool time (s)}\\
 \cline{2-5}
 Program& Old Version & New Version & Slicing & Suggestion\\
 \hline
 \hline
 1. Randoop & 20.1 & 4.1 & 90 & 295 \\
 2. Weka & 1.6 & 1.6 & 80 & 49 \\
 3. Synoptic & 1.7 & 4.7 &48 & 42 \\
 4. Synoptic & 1.7 & 4.7 &48  & 42  \\
 5. JChord & 18.7 & 44.3  & & \\
 6. JChord & 17.6 & 41.1 & & \\
 7. JMeter & 1.3 & 1.4 &51 & 63 \\
 8. Javalanche& 1.4 & 1.5 & 430 & \\
\hline
\hline
 Mean & 8.0 & 12.8 & & \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:performance} \ourtool's
performance. The ``Run-time slow down'' column
shows the cost of reproducing the error in
an \ourtool-instrumented version of the subject program.
The ``\ourtool time (seconds)'' column shows
the time took by \ourtool to diagnose configuration errors.
For both columns, the mean is the geometric mean. 
}
}
\end{table}

\subsubsection{Comparison with Two Existing Approaches}
\label{sec:existing}

This section compares \ourtool with two existing approaches,
called \prevtool~\cite{Zhang:2013:ADS} and \conftool~\cite{Rabkin:2011:PPC}.

\prevtool, proposd in our previous work~\cite{}, is an automated
software configuration error diagnosis technique. Unlike \ourtool,
\prevtool diagnoses configuration errors on a single program version.
Besides, it differs from \ourtool from two key aspects:
first, \prevtool assumes the existence of a set of correct execution
traces, which are used to compare to against the undesired
execution trace to identify the behaviorally-deviated parts.
Second, when comparing the undesired execution trace with a
correct execution trace, \prevtool only focuses on the behavior
of an executed predicate, ignoring the statements determined
by the predicate's evaluation result. To compare \ourtool with
\prevtool, for 4 subject programs (Randoop, Synoptic, JChord, and
Weka) that are also used
to evaluate \prevtool, we reuse the pre-built trace database
from~\cite{}. For the other two subject programs (JMeter and
Javalanche), we manually built an execution trace database for each
of them, by running correct examples from their user manuals.

\conftool, proposed by Rabkin and Katz~\cite{}, is a lightweight
static analysis technique to precompute diagnosis for a program.
\conftool tracks the flow of labeled objects through 
program control flow and data flow, and treats a configuration option
as a root cause if its value may flow to a crashing point.
Since \conftool cannot diagnose non-crashing errors, we only use
it to diagnose the crashing error in the Weka program.


\todo{show the results here}

Table~\ref{tab:errors} shows the experimental results.

\ourtool produces significantly accurate results than \prevtool,
for two primary reasons. First,
\todo{to do}

\conftool outputs correct result for only one error, since
it exclusively focuses on diagnosing crashing configuration errors.
By contrast, \ourtool is capable to diagnose both crashing
and non-crashing errors.

We did not compare \ourtool with other related
configuration error diagnosis approaches~\cite{Attariyan:2010:ACT,
xray, Whitaker:2004:CDS, Su:2007:AIC, Wang:2004:AMT, rangefix},
because these approaches target a rather
different problem than \ourtool or require different
inputs than \ourtool. For example, ConfAid~\cite{Attariyan:2010:ACT}
and X-Ray~\cite{xray} can only diagnose crashing
or performance configuration errors on a single
program version. Chronus~\cite{Whitaker:2004:CDS}
and AutoBash~\cite{Su:2007:AIC}
require OS-level support for capture and replay.
PeerPressure~\cite{Wang:2004:AMT} and RangerFixer~\cite{rangefix} only
supports configuration options defined by certain
specific feature models. It is unknown whether
such techniques can be extended to diagnose configuration
errors caused by software evolution. On the other
hand, general software fault locaization techniques
are not well-suited for diagnosing software
configuration errors~\cite{Jones:2002}, since such techniques
often focus on identifying the buggy code or
invalid input values. This has been empirically
validated in our previous work~\cite{Zhang:2013:ADS}.


\vspace{1mm}

\noindent \textbf{\textit{Summary.}} Configuration
error diagnosis techniques designed for a \textit{single}
program version cannot be directly applied to diagnose
configuration erros introduced in software evolution.
\ourtool reasons about the behavior differences across
two program versions, and produces more accurate results.


\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{1.10\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Error ID. & \multicolumn{3}{|c|}{Rank of the Root Cause Configuration Option}  \\
\cline{2-4}
 Program & \ourtool & Full Slicing & Predicate Behavior  \\
 \hline
 \hline
 1. Randoop &  &  & \\
 2. Weka &  &  & \\
 3. Synoptic &  &  & \\
 4. Synoptic &  &  & \\
 5. JChord &  &  & \\
 6. JChord &  &  & \\
 7. JMeter &  &  & \\
 8. Javalanche &  &  & \\
\hline
\hline
 Mean &  & & \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:choices} 
Experimental results of evaluating two design choices
of \ourtool. Column ``\ourtool'' shows \ourtool's
results, taken from Table~\ref{tab:errors}.
Column ``Full Slicing'' shows the results of replacing
thin slicing with full slicing in \ourtool.
Column ``Predicate Behavior'' shows the results of 
\ourtool, if it only considers
predicate behavior changes.
For both columns, the mean is the geometric mean. 
}
}
\end{table}

\subsubsection{Evaluating Two Design Choices}
\label{sec:alternative}

This section evaluates two design choices in \ourtool:

\begin{itemize}
\item \textbf{thin slicing vs. full slicing.} \ourtool
uses thin slicing to identify the affecting configuration
options for a predicate. We next investigate the effects
of replacing thin slicing with full slicing~\cite{Horwitz:1988}.
Table~\ref{tab:choices} (Column ``Full Slicing'') shows the results.
\todo{indicate changes to the algorithm}

\item \textbf{predicate behavior + affected statements vs. predicate behavior.}
\ourtool combines the metrics of predicate behavior changes
and the number of affected statements to rank the importance
of a predicate\todo{need re-wording}. We next
investigate the effects of using the sole metric of
predicate behavior change to rank predicates, as \prevtool does.
Table~\ref{tab:choices} (Column ``Predicate Behavior'') shows
the results. \todo{indicate changes to the algorithm}
\end{itemize}

\ourtool achieves substantially less accurate results when
using full slicing. The primary reason is that full slicing
identifies many irrelevant configuration options that \textit{indirectly}
affect a predicate of interest. Such configuration options
are not pertinent to the task of error diagnosis. Linking them
to the different behaviors between two versions would degrade
\ourtool's accuracy.


\ourtool's accuracy degrades substantially when using
predicate behavior difference as the only metric (as \prevtool
does) in option recommendation.
The primary reason is that, between two execution traces,
many predicates may have the same value of the predicate
behavior metric, but they have significantly different
impacts to the overall program behavior difference. \ourtool
uses the number of statements determined by the predicate
evaluation result to approximate such potential impacts.

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} Full slicing
includes too many irrelevant program statements due to its
conservatism; and only focusing on a predicate's behavior
may ignore its potential impact to the program behavior.
Using thin slicing and a combination of predicate behavior
and its impacted statements can be a better choice in diagnosing
configuration errors.

\subsection{Discussion}

\noindent \textbf{\textit{Threats to validity.}}
There are several threats to validity of our evaluation.
First, the 6 Java programs might not be representative, though some of them are used
in previous research. research. Likewise, the
8 configuration errors might not be representative, even though we
evaluated every error we found. Second, 
\ourtool's effectiveness depends on the effectiveness of the
predicate matching algorithm. It may yield less
useful results for programs with significant code changes.
However, different algorithms can be plugged into \ourtool.
Third, Our evaluation only compared \ourtool with two other
approaches. Comparing with other analyses or tools might yield
different observations.
Fourth, our evaluation focuses on \ourtool's algorithm for
configuration option recommendation. A future user study should
evaluate whether \ourtool helps users.

\vspace{1mm}

\noindent \textbf{\textit{Experimental conclusions.}}
We have three chief findings. (1) \ourtool is highly effective
in diagnosing configuration errors introduced by configuration
evolution; (2) \ourtool produces more accurate results than
approaches designed to diagnose errors on a single program version;
and (3) using thin slicing and combining the behavior of
a predicate and its affected statements permit \ourtool
to produce more accurate diagnosis.

