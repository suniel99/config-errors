
\section{Technique}
\label{sec:technique}

In \ourtool, we model a configuration as a set of key-value
pairs, where the keys are strings and the values have
arbitrary type. 
This key-value abstraction
is used by the POSIX system environment, the Java
Properties API, and the Windows Registry.

%As an example, in the error-fixing
%configuration setting \CodeIn{output\_format = XML} for JMeter in Section~\ref{sec:evolerror},
%\CodeIn{output\_format} is the configuration option name,
%and \CodeIn{XML} is the value. 

\subsection{Overview}

There are two key insights behind \ourtool. First,
a program's control flow often propagates the majority of
the effects of a configuration option and determines
a program's execution path, while the value of a
specific expression may be largely input-dependent.
Second, the control flow differences of two execution
traces on different program versions provide evidence
for which parts of the program might be behaving
abnormally and why.
%undesirably.

Based on these two insights, \ourtool uses three
steps to link the different behaviors across program
versions to a specific configuration option.
Figure~\ref{fig:overview} sketches the high-level workflow of
\ourtool. To recommend configuration options that
fix the undesired behavior, \ourtool first asks the
user to run the same input and
configuration on two instrumented program versions
(Section~\ref{sec:profiling}).
Then, \ourtool identifies differences between
two execution traces produced
by the old and new program versions, respectively.
In particular, \ourtool identifies program predicates
that behave differently across two executions
(Section~\ref{sec:comparison}).
After that, \ourtool uses a lightweight dependence
analysis technique, called thin slicing~\cite{},
to statically reason about which configuration
options may cause such behavioral differences,
and reports a ranked list of 
suspicious configuration options to the user (Section~\ref{sec:rootcause}).

\subsection{Instrumentation and Demonstration}
\label{sec:profiling}

\input{demonstration}

\subsection{Execution Trace Comparison}
\label{sec:comparison}

\input{comparison}

\subsection{Configuration Option Recommendation}
\label{sec:rootcause}

\input{recommendation}

\subsection{Discussion}

We next discuss some design issues in \ourtool.

\vspace{1mm}

\noindent \textbf{\textit{Fixing configuration problems vs. Fixing regression bugs.}}
It appears that software configuration lies in the gray zone between
the software developers of software users.
The responsibility for creating correct configurations
lies with both parties; the developer should create
intuitive configuration logic, build logic that detects
errors, and convey configuration knowledge to users
effectively; \todo{evolve} the end-user should imbibe the
knowledge and manage cross-version configurations.
This shared responsibility is non-trivial to efficiently
achieve. For example, there is no obviously ``correct'' way to
build configuration logic; also, unlike fixing a bug once,
every software user has to be informed on the right way to
configure the system. Perhaps as a result, misconfigurations
have been one of the dominant causes of system issues and
is likely to continue so.

\todo{a few more tradeoffs can be discussed here.}

\vspace{1mm}
\noindent \textbf{\textit{Why not use a pure dynamic analysis in \ourtool?}}
\ourtool uses static thin slicing to estimate the effects of
a configuration option. Another possible way is to use a pure
dynamic analysis to assess the causality. State-of-the-art
techniques such as value replacement~\cite{} and dual slicing~\cite{}
\todo{describe them}.
However, a major challenge is that it is difficult for
\ourtool to find a replacement value for a configuration option.
For Boolean type option, it is trivial to find alternative values.
However, for many configurtion options with string types, it
is hard to determine good alternative values without a specification.
\todo{illustrate more clearly above}. Investigating how
to combine static and dynamic analyses 


\vspace{1mm}
\noindent \textbf{\textit{\ourtool's current limitations.}}
There are four major limitations in the \ourtool technique.
First, \ourtool currently assumes that only one
configuration option has an incorrect value.
If fixing a particular configuration error
requires changing values of two configuration options,
then \ourtool may not identify both of them.
Second, \ourtool assumes the different behaviors
on two program versions can be demonstrated by users,
and does not support diagnosing non-deterministic
errors. For non-deterministic errors, \ourtool
could potentially leverage a deterministic replay
system~\cite{Huang:2013:CRL, Jin:2012:BRF} to faithfully reproduce the behaviors.
Third, \ourtool matches one predicate in the old
program version to one predicate in the new program version.
It might be possible that a predicate will evolve
into multiple predicates. We did not see such cases
in our experiment, but we speculate that \ourtool
may not produce useful matching results.
Fourth, \ourtool focuses on identifying root cause
configuration options that can change behaviors of
the analyzed program rather than the underlying OS
or runtime system. Configuration options, such
as \CodeIn{-Xmx} used in launch a Java program for
specifying JVM's heap size, are not supported
in \ourtool.
