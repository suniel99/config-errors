
\section{Technique}
\label{sec:technique}

\ourtool models a configuration as a set of key-value
pairs, where the keys are strings and the values have
arbitrary type. 
This key-value abstraction
is used by the POSIX system environment, the Java
Properties API, and the Windows Registry.

%As an example, in the error-fixing
%configuration setting \CodeIn{output\_format = XML} for JMeter in Section~\ref{sec:evolerror},
%\CodeIn{output\_format} is the configuration option name,
%and \CodeIn{XML} is the value. 

\subsection{Overview}

\ourtool is based on two key insights. First,
a program's control flow, rather than data flow, often propagates the majority of
the effects of a configuration option and determines
a program's execution path.
Second, the control flow differences of two execution
traces can approximate the program behavior differences
of two versions, and provide evidence
for which parts of the program might be behaving
abnormally and why.
%undesirably.

Based on these two insights, \ourtool uses three
steps to link different behaviors across program
versions to a specific configuration option that caused the difference.
Figure~\ref{fig:overview} sketches the high-level workflow of
\ourtool. 
%To recommend configuration options that
%fix the undesired behavior, 
In the first step, \ourtool asks the
user to demonstrate the different behaviors, using the same input and
configuration, on two \ourtool-instrumented program versions
(Section~\ref{sec:profiling}).
Then, \ourtool analyzes the two execution traces produced
by user demonstration, and identifies the control flow differences between
them. In particular, \ourtool identifies program predicates
that behave differently across two executions
(Section~\ref{sec:comparison}).
After that, \ourtool uses a lightweight dependence
analysis technique, called thin slicing~\cite{Sridharan:2007},
to statically reason about which configuration
options may cause the control flow differences.
Finally, \ourtool reports a ranked list of 
suspicious configuration options to the user (Section~\ref{sec:rootcause}).

\subsection{Instrumentation and Demonstration}
\label{sec:profiling}

\input{demonstration}

\subsection{Execution Trace Comparison}
\label{sec:comparison}

\input{comparison}

\subsection{Configuration Option Recommendation}
\label{sec:rootcause}

\input{recommendation}

\subsection{Discussion}
\label{sec:tech_discuss}

We next discuss some design issues in \ourtool.

\vspace{1mm}

\noindent \textbf{\textit{Fixing configuration errors 
vs. Localizing regression bugs.}}
The problem addressed in this paper is significantly different
than the traditional regression bug localization problem~\cite{dd, autoflow}.
A regression bug occurs when developers have made a mistake,
which causes software to violate its specification after a session of code changes.
By contrast, in our problem, the software behavior on the new version
is still as \textit{designed} but \textit{undesired}. 
%\ourtool addresses
%this problem from a specific angel by recommending configuration options
%to fix the undesired behavior.
Further, compared to
regression bugs introduced by software developers,
software configuration lies in the gray zone between
the software developers of software users. The responsibility for creating
correct configurations lies with both parties; the developer should create
intuitive configuration logic, build logic that detects
errors, and convey configuration knowledge to users
effectively. This shared responsibility makes recommend configuration options
different than localizing a regression bug, and causes many existing
techniques not applicable. We will discuss related techniques below
and in Section~\ref{sec:related}, and describe an empirical comparison
in Section~\ref{sec:evaluation}.


\vspace{1mm}
\noindent \textbf{\textit{Why not use a dynamic analysis to recommend
configuration options?}}
\ourtool uses thin slicing to statically identify responsible configuration
options for a behaviorally-deviated predicate. Another possible way is to use a pure
dynamic analysis to assess the causality of how a configuration option
may affect the control flow. State-of-the-art
techniques such as Delta Debugging~\cite{dd}, value replacement~\cite{failuredoc},
and dual slicing~\cite{Sumner:2013:CCE}
use a similar idea: they repeatedly replace a variable value with other alternaives,
and then re-execute the program to check whether the outcome is desired.
There are two major challenges that prevent these dynamic analyses
from being used. First, it is
difficult to find a valid replacement value for a configuration option.
For Boolean type option, it is trivial to find alternative values.
However, for many configurtion options with string or regular expression types, it
can be hard to determine good alternative values without a specification.
Second, automatically checking program outcomes requires
a testing oracle, which is often not available in practice, and end-users
should not be  expected to provide it. To address these challenges,
\ourtool approximates the program behavioral differences by the
control flow differences of two executions, and then statically reasons
about the responsible configuration options.

%However, a major challenge is that it is difficult for
%\todo{illustrate more clearly above}. Investigating how
%to combine static and dynamic analyses 


\vspace{1mm}
\noindent \textbf{\textit{\ourtool's current limitations.}}
There are four major limitations in the our \ourtool technique.
First, \ourtool currently assumes that only one
configuration option is responsible for the undeisred behavior,
although it can diagnose two errors caused by two independent options
at a time (Section~\ref{sec:evaluation}).
If fixing a particular configuration error
requires changing values of two configuration options,
then \ourtool may not identify both of them.
Second, \ourtool assumes the different behaviors
on two program versions are not caused by non-determinism.
For non-deterministic behaviors, \ourtool
could potentially leverage a deterministic replay
system~\cite{Huang:2013:CRL, Jin:2012:BRF} to faithfully reproduce the behaviors.
Third, \ourtool only matches one predicate in the old
program version to one predicate in the new program version.
If a predicate evolves into multiple predicates in the new
version, \ourtool may output less useful results. 
%We did not see such cases
%in our experiment, but we speculate that \ourtool
%may not produce useful matching results.
Fourth, \ourtool focuses on identifying root cause
configuration options that can change the functional behaviors of
the target program.
% rather than the underlying OS
%or runtime system. 
Configuration options that essentially affect the underlying
OS or runtime system, such as the \CodeIn{-Xmx} option used to
specify JVM's heap size when launching a Java program,
are not supported in \ourtool.
