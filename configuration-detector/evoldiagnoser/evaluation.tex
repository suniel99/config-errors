
\section{Evaluation}
\label{sec:evaluation}

We evaluated 4 aspects of \ourtool's effectiveness, answering
the following research questions:

\begin{enumerate}
\item How accurate is \ourtool in recommending configuration options
to fix configuration errors caused by software evolution? That is, what is the rank of the
actual root cause configuration option in \ourtool's output (Section~\ref{sec:accuracy})?

\item How long does it take for \ourtool to diagnose
a configuration error (Section~\ref{sec:timecost})?

\item How does \ourtool's effectiveness compare to
existing approaches (Section~\ref{sec:existing})?

\item How does \ourtool's effectiveness compare to
two alternative approaches that use full slicing in identifying
suspicious configuration options, and only focus on predicate behavior
changes to recommend configuration options (Section~\ref{sec:alternative})?

\end{enumerate}

\subsection{Subject Programs}

We evaluated \ourtool on \subjnum Java programs
listed in Table~\ref{tab:subjects}.
The top 5 subject programs are the 5 Java programs
studied in Section~\ref{sec:study},
and the remaining subject program Javalanche~\cite{javalanche}
(which is a popular mutation testing framework) and
its configuration error, is provided by one of its real users.

\newcommand{\randoopoptnum}{57\xspace}
\newcommand{\wekaoptnum}{14\xspace}
\newcommand{\synopticoptnum}{37\xspace}
\newcommand{\jchordoptnum}{79\xspace}
\newcommand{\jmeteroptnum}{55\xspace}
\newcommand{\javalancheoptnum}{35\xspace}

\newcommand{\randooprank}{1\xspace}
\newcommand{\wekarank}{1\xspace}
\newcommand{\synopticrankfirst}{1\xspace}
\newcommand{\synopticranksecond}{6\xspace}
\newcommand{\jchordrankfirst}{1\xspace}
\newcommand{\jchordranksecond}{1\xspace}
\newcommand{\jmeterrank}{1\xspace}
\newcommand{\javalancherank}{3\xspace}

\newcommand{\averagerank}{1.8\xspace}

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.20\tabcolsep}
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
 Program & Old Version & New Version & LOC (new version) & $\Delta$LOC & \#Options \\
 \hline
 \hline
 Randoop & 1.2.1 & 1.3.2 &18571&1893& \randoopoptnum  \\
 Weka & 3.6.1 & 3.6.2 &275035& 1458 & \wekaoptnum \\
 Synoptic & 0.05 & 0.1 &19153& 1658 & \synopticoptnum \\
 JChord & 2.0 & 2.1&26617& 3085 & \jchordoptnum \\
 JMeter & 2.8 & 2.9 &91979& 3264 &  \jmeteroptnum \\
 Javalanche & 0.36 & 0.40 & 25144 &9261& \javalancheoptnum \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:experiment-sub} All subject programs
used in the evaluation. Column ``$\Delta$LOC'' shows
the number of changed lines of code between the old and new versions.
Column ``\#Options'' shows the number of configuration options
supported in the new program version.
}
}
\end{table}

\subsubsection{Configuration Errors}

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||l|l|}
\hline
 Error ID& Program & Description\\
 \hline
 \hline
 1 & Randoop  & Poor performance in test generation\\
 2 & Weka &  A different error message when Weka crashes\\
 3 & Synoptic & The generated initial model not saved\\
 4 & Synoptic & The generated model not saved as JPEG file \\
 5 & JChord & Bytecode parsed incorrected \\
 6 & JChord &  Method names not printed in the console\\
 7 & JMeter &  Results saved to a file with a different format\\
 8 & Javalanche &  No mutants generated\\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:errorlist} All configuration errors used in the evaluation.
Only the 2-nd error is a crashing errors, and all the other errors are non-crashing
errors. }
}
\end{table}

\input{program-table}

For the 5 Java programs studied in Section~\ref{sec:study},
we manually examinated all configuration-related changes
listed in Table~\ref{tab:options}. For each
change, we wrote a test driver to cover
it, and then checked whether the test driver
reveals different behaviors on two versions.
\todo{exclude}
When reproducing the different behaviors,
we excluded newly-added options, since
they options are only available in the new version
and will not be used by users of the old version.
Second, we excluded option changes that are backward
compatible. Third, for such option changes, if the
new program version already identifies the potential
configuration errors by dumping explicit error messages,
we excluded them. \todo{rephrase above}

For the Javalanche program, we reproduced the configuration
error as provided by the user.

We evaluated all configuration errors that we can reproduce;
we did not select only errors that \ourtool works well.
Table~\ref{tab:errors} lists all errors.
In Table~\ref{tab:errors}, errors \#3 and \#4
can be reproduced in a single execution, and each of the other
error is reproduced in one execution.

\todo{cover different types of options, and different types of errors}


%We collected \errornum configuration errors caused by
%software evolution. \todo{reasons of relatively few
%errors}. 

\subsection{Evaluation Procedure}

We used \ourtool to instrument both versions. 
For each configuration error, we use the same test driver
(with the same input and configuration)
to reproduce the different behaviors on both instrumented versions.

The average size of the produced execution traces is 40MB,
and the largest one (Randoop's trace) is 140MB



When recommending configuration options, we manually specify
the initialization statement of each configuration option as
the seed for thin slicing. This manual step took around
10 minutes on average for each subject program. After that,
\ourtool works in a fully-automatic way: it 
analyzes two program versions and two execution traces,
and outputs a ranked list of configuration options.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

\subsubsection{Accuracy}
\label{sec:accuracy}

As shown in Table~\ref{tab:errors}, \ourtool is highly effective
is recommending the root cause configuration option that should
be changed in the new program version. For all \todo{xxx the
results}

\todo{show one more example to illustrate its effectiveness}

\ourtool fails to recommend correct options for one error in
JChord (error \todo{xx}). This is because the predicate matching
algorithm outputs a wrong matching result.

\todo{show some code here}

\vspace{1mm}

\noindent \textbf{\textit{Summary.}} \ourtool
recommends correct configuration options with
high accuracy for evolving configurable software systems
with non-trivial code changes.

\subsubsection{Performance of \ourtool}
\label{sec:timecost}

We measured \ourtool's performance in two ways:
the performance overhead introduced by instrumentation
when demonstrating the configuration error,
and the time cost of recommending configuration options.
Table~\ref{tab:performance} shows the results.

The performance overhead to demonstrate the error
varies among programs. The current tool implementation
imposes an average 8X and 12.8X slowdown in a
\ourtool-instrumented old and new program version, respectively.
This is due to \ourtool's inefficient instrumentation code that
monitors the execution of every instruction.
Even so, except for two errors (errors \#5 and \#6) from\
the JChord subject program,  all other errors can
be reproduced in less than 30 seconds. Errors \#5 and \#6
require about 20 minutes to reproduce.

%the different behaviors can be reproduced
%in less than XXX minutes on average on both versions, 
%with a worse case of XXX minutes.


\ourtool spends an average of \avgtime minutes
to recommend configuration options for one
error (including time to compute thin slices
and the time to suggest suspicious options). 
Computing thin slices for all configuration options
is non-trivial. However, this step is one-time cost
per program and the computed results can
be cached to share across runs. 
The time used for suggesting configuration options
is roughly proportional to the size of the execution trace
rather than the size of the subject program.

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} \ourtool
recommends configuration options for fixing
configuration errors with acceptable time cost.

\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.80\tabcolsep}
\begin{tabular}{|l||c|c||c|c|}
\hline
 Error ID.& \multicolumn{2}{|c||}{Run-time Slowdown ($\times$)} & \multicolumn{2}{|c|}{\ourtool time (s)}\\
 \cline{2-5}
 Program& Old Version & New Version & Slicing & Suggestion\\
 \hline
 \hline
 1. Randoop & 20.1 & 4.1 & 90 & 295 \\
 2. Weka & 1.6 & 1.6 & 80 & 49 \\
 3. Synoptic & 1.7 & 4.7 &48 & 42 \\
 4. Synoptic & 1.7 & 4.7 &48  & 42  \\
 5. JChord & 18.7 & 44.3  & 20 & 38 \\
 6. JChord & 17.6 & 41.1 & 23 & 29 \\
 7. JMeter & 1.3 & 1.4 &51 & 63 \\
 8. Javalanche& 1.4 & 1.5 & 430 & 265\\
\hline
\hline
 Average & 8.0 & 12.8 & 99 & 91 \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:performance} \ourtool's
performance. The ``Run-time slow down'' column
shows the cost of reproducing the error in
an \ourtool-instrumented version of the subject program.
The ``\ourtool time (seconds)'' column shows
the time took by \ourtool to diagnose configuration errors.
Column ``Slicing'' includes the cost of computing
thin slices on both old and new program versions.
%For both columns, the mean is the geometric mean. 
}
}
\end{table}

\subsubsection{Comparison with Two Existing Approaches}
\label{sec:existing}

This section compares \ourtool with two existing approaches,
called \prevtool~\cite{Zhang:2013:ADS} and \conftool~\cite{Rabkin:2011:PPC}.

\prevtool, proposd in our previous work~\cite{Zhang:2013:ADS},
is an automated
software configuration error diagnosis technique. Unlike \ourtool,
\prevtool is \textit{not} cognizant of
software evolution: it diagnoses configuration errors on
a single program version.
Besides, it differs from \ourtool from two key aspects:
first, \prevtool assumes the existence of a set of correct execution
traces, which are used to compare against the undesired
execution trace to identify the abnormal program parts.
Second, when comparing the undesired execution trace with a
correct execution trace, \prevtool only focuses on the behavior
change of an executed predicate (using the same $\phi$
function in Section~\ref{sec:comparison}), ignoring the statements determined
by the predicate's evaluation result. \todo{explain
above} To compare \ourtool with \prevtool, 
we reuse the pre-built execution trace database
for 4 subject programs (Randoop, Synoptic, JChord, and Weka)
from our existing work~\cite{Zhang:2013:ADS}.
For the remaining two subject programs (JMeter and
Javalanche), we manually built an execution trace database
for each of them by running correct examples from their user manuals.
The built databases contain \todo{xx} and \todo{xx}
execution traces for JMeter and Javalanche, respectively.

\conftool, proposed by Rabkin and Katz~\cite{}, is a lightweight
static analysis technique to precompute diagnosis for a program.
\conftool tracks the flow of labeled objects through 
program control flow and data flow, and treats a configuration option
as a root cause if its value may flow to a crashing point.
Since \conftool cannot diagnose non-crashing errors, we
can only apply it to diagnose the crashing error in
Weka (error\#2 in Table~\ref{tab:errors}).


\vspace{1mm}
\noindent \textbf{\textit{Results.}}
Columns ``\prevtool'' and ``\conftool'' in Table~\ref{tab:errors} show
the experimental results.
%\todo{show the results here}

\ourtool produces significantly accurate results than \prevtool,
for two primary reasons. First,
\todo{to do}

\conftool outputs correct result for the crashing error in Weka,
but is not applicable to diagnosing other non-crashing errors.
The crashing error in Weka occurs soon after the program
is launched; a small number of configuration options
are initialized and only one of them flows to the crashing point.


%since
%it exclusively focuses on diagnosing crashing configuration errors.
%By contrast, \ourtool is capable to diagnose both crashing
%and non-crashing errors.

We did not compare \ourtool with other related
configuration error diagnosis approaches~\cite{Attariyan:2010:ACT,
xray, Whitaker:2004:CDS, Su:2007:AIC, Wang:2004:AMT, rangefix},
because these approaches target a rather
different problem than \ourtool, or require different
inputs than \ourtool. For example, 
X-Ray~\cite{xray} can only diagnose
performance-related configuration errors on a single
program version. Chronus~\cite{Whitaker:2004:CDS}
and AutoBash~\cite{Su:2007:AIC}
require OS-level support for capture and replay.
PeerPressure~\cite{Wang:2004:AMT} and RangerFixer~\cite{rangefix} only
supports configuration options defined by certain
specific feature models. It is unknown whether
such techniques can be extended to diagnose configuration
errors handled by \ourtool. On the other
hand, general software fault locaization techniques
are not well-suited for diagnosing software
configuration errors~\cite{Jones:2002, McCamant:2003}, since such techniques
often focus on identifying the buggy code or
invalid input values. This has been empirically
validated in our previous work~\cite{Zhang:2013:ADS}.


\vspace{1mm}

\noindent \textbf{\textit{Summary.}} Configuration
error diagnosis techniques designed for a \textit{single}
program version cannot be directly applied to diagnose
configuration erros introduced in software evolution.
\ourtool reasons about the behavior differences between
two program versions, and produces more accurate results.


\begin{table}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{1.10\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Error ID. & \multicolumn{3}{|c|}{Rank of the Root Cause Configuration Option}  \\
\cline{2-4}
 Program & \ourtool & Full Slicing & Predicate Behavior  \\
 \hline
 \hline
 1. Randoop & \randooprank & 32 & 7 \\
 2. Weka & \wekarank & 7 & 1 \\
 3. Synoptic & \synopticrankfirst & 16 & 3\\
 4. Synoptic & \synopticranksecond & 17 & 8 \\
 5. JChord & \jchordrankfirst & 19 & 5 \\
 6. JChord & \jchordranksecond & 30 & 5 \\
 7. JMeter & \jmeterrank & \n & 1 \\
 8. Javalanche & \javalancherank & \n & 13 \\
\hline
\hline
 Average & \averagerank  & 20.7 & 5.4 \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:choices} 
Experimental results of evaluating two design choices
of \ourtool. Column ``\ourtool'' shows \ourtool's
results, taken from Table~\ref{tab:errors}.
Column ``Full Slicing'' shows the results of replacing
thin slicing with full slicing in \ourtool.
``\n'' means the technique does not identify the actual
root cause. Column ``Predicate Behavior'' shows the results of 
\ourtool, if it only considers predicate behavior changes.
When computing the average rank, each ``\n'' is treated
as half of the number of configuration options.
}
}
\end{table}

\subsubsection{Evaluating Two Design Choices}
\label{sec:alternative}

This section evaluates two design choices in \ourtool:

\begin{itemize}
\item \textbf{thin slicing vs. full slicing.} \ourtool
uses thin slicing to identify configuration options
whose values may affect a predicate. We next investigate
the effects of replacing thin slicing with
the traditional full slicing~\cite{Horwitz:1988}.
Table~\ref{tab:choices} (Column ``Full Slicing'') shows the results.
\todo{indicate changes to the algorithm}

\item \textbf{predicate behavior + affected statements vs. predicate behavior.}
\ourtool combines the metrics of predicate behavior changes
and the number of affected statements to rank the importance
of a predicate\todo{need re-wording}. We next
investigate the effects of using the 
predicate behavior change as the sole metric to rank predicates, as \prevtool does.
Table~\ref{tab:choices} (Column ``Predicate Behavior'') shows
the results. \todo{indicate changes to the algorithm}
\end{itemize}

\ourtool achieves substantially less accurate results when
using full slicing. The primary reason is that full slicing
identifies many irrelevant configuration options that \textit{indirectly}
affect a predicate of interest. Such configuration options
are not pertinent to the task of error diagnosis. Linking them
to the exhibited different behavior would degrade
\ourtool's accuracy. Further, computing full slices is much
more expensive than computing thin slices. The mature
full slicing algorithm implemented in WALA failed to scale
to two of our subject programs (JMeter and Javalanche).


\ourtool's accuracy degrades substantially when using
predicate behavior change as the metric in option
recommendation. The primary reason is that, between two execution traces,
many predicates may have the same value of the predicate
behavior metric\todo{need rewording}, but they have different
impacts to the overall program behavior change. \ourtool
uses the number of statements determined by the predicate
evaluation result to approximate such potential impacts.

%\todo{We also consider other metrics to measure predicate deviations}

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} Full slicing
includes too many irrelevant program statements due to its
conservatism; and only focusing on a predicate's behavior
difference
may ignore the impact to the program behavior. \todo{need
rephrase}
Using thin slicing and a combination of predicate behavior
and its impacted statements can be a better choice in diagnosing
configuration errors.

\subsection{Discussion}

\noindent \textbf{\textit{Threats to validity.}}
There are several threats to validity of our evaluation.
First, the \subjnum Java programs might not be representative, though some of them had been
used in previous research. Likewise, the
\errornum configuration errors might not be representative, even though we
evaluated every error we found. 
Second, in our evaluation, we assume the configuration errors
are not caused by software regression bugs, as all regression
tests between two versions pass. Applying \ourtool to a buggy
program version may yield a different result.
Third, \ourtool's effectiveness depends on the effectiveness of the
predicate matching algorithm. It may yield less
useful results for programs with significant code changes.
However, different algorithms can be plugged into \ourtool.
Fourth, our evaluation only compared \ourtool with two other
approaches. Comparing with other analyses or tools might yield
different observations.
Fourth, our evaluation focuses on \ourtool's algorithm for
configuration option recommendation. A future user study should
evaluate whether \ourtool helps users.

\vspace{1mm}

\noindent \textbf{\textit{Experimental conclusions.}}
We have three chief findings. (1) \ourtool is highly effective
in diagnosing configuration errors introduced by configuration
evolution; (2) \ourtool produces more accurate results than
approaches designed to diagnose errors on a single program version;
and (3) \ourtool outperforms two alternative approaches
that use full slicing and solely focuses on a predicate's
behavior change, respectively.
%using thin slicing and combining the behavioral
%difference of a predicate and its affected statements
%permit \ourtool to produce more accurate diagnosis.

