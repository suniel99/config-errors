\section{Related Work}

Work related to this paper falls into three major categories;
(1) software configuration error diagnosis; (2) automated
debugging techniques; and (3) configuration-aware software
analysis techniques.
 
\subsection{Software Configuration Error Diagnosis}

Several prior research efforts have applied different techniques to the
problem of software configuration error diagnosis~\cite{Attariyan:2008:UCD, 
Yuan:2011:COC, Su:2009:AGP, Whitaker:2004:CDS, Wang:2004:AMT,
Attariyan:2010:ACT, Rabkin:2011:PPC}. For example,
Chronus~\cite{Whitaker:2004:CDS} relies on a user-provided
testing oracle to check the behavior of the system, and uses
virtual machine checkpoint and binary search to find the
point in time where the program behavior
switched from correct to incorrect. AutoBash~\cite{Su:2007:AIC}
fixes a misconfiguration by using
OS-level speculation execution to try possible
configurations, examine their effects, and roll them back when necessary.
PeerPressure~\cite{Wang:2004:AMT} 
uses statistical methods to compare
configuration state in the Windows registry on different machines.
When a registry entry value on a machine exhibiting erroneous behavior differs
from the value usually chosen by other machines, PeerPressure
flags the value as a potential error. More recently, ConfAid~\cite{Attariyan:2010:ACT}
takes a dynamic-taint-based approach to infer configuration problems 
by monitoring causality within the program binary as it executes.
ConfAnalyzer~\cite{Rabkin:2011:PPC} combines static dataflow analysis
and dynamic taint analysis to precompute
possible configuration error diagnoses for every possible crashing point
in a program. 

Our technique is significantly different
from the existing approaches.
First, almost all existing approaches focus exclusively on configuration errors
that lead to software crash or assertion failures, by leveraging
valuable crashing information such as stack traces.
By contrast, our technique can diagnose both
\textit{crashing} and \textit{non-crashing} errors~\cite{Yin:2011:ESC}.
Second, existing approaches~\cite{Attariyan:2010:ACT, Whitaker:2004:CDS}
assume the existence of a testing oracle that can safely
reveal the error during the diagnosis. However, such error-revealing oracles are often
absent in practice, and writing them may
require a substantial amount of time and effort that a
typical software user would not prefer, and should not be expected, to invest.
By constrast, our technique eliminates this assumption.
Third, approaches like PeerPressure~\cite{Wang:2004:AMT} benefit from
the known schema of the Windows registry, but cannot detect configuration errors
that lie outside the registry. Our technique of analyzing the
affected predicate behavior is more general for Java software.

%Most prior work on configuration debugging has relied
%on large user communities or on modifying the program’s execution
%environment $\blacksquare$ focusing on crashing errors.

%In contrast, current industrial practice uses stack traces to cluster
%failure reports into equivalence classes. Two crash reports showing
%the same stack trace, or perphas only the same top-of-stack function,
%are presumed to be two reports of the same failure. This heuristic
%works to the extent that a single cause corresponds to a single point
%of failure, but our experience with xxx suggests that this
%assumption may not often hold.




%Most prior work has taken a \textit{black box} approach that uses
%only state external to the application being debugged to infer the problem.

\subsection{Automated Debugging Techniques}

Many automated debugging techniques~\cite{Jones:2002, Zeller:2002:ICC,
 Horwitz:1988, clause07july} have been developed to localize errors in software.

Program slicing~\cite{Horwitz:1988} and taint analysis~\cite{clause07july}
are two techniques to determine which statements and inputs
could affect a particular variable value. Despite their effectiveness
in diagnosing a crashing configuration error by performing backward
reachability analysis from the crashing point~\cite{Rabkin:2011:PPC,
Attariyan:2010:ACT}, these two techniques
can not be directly applied to diagnose a crashing error.

%By performing
%backward reachability analysis from the crashing point, these two
%techniques can be effective in diagnosing crashing configuration errors.
%Nevertheless How


Delta debugging~\cite{Zeller:2002:ICC} is a general
algorithm for isolating software error causes.
It works by differences between a working state and broken
state to isolate a set of failure-inducing inputs.
However, it requires users to provide a testing oracle to check
whether an intermediate program state behaves correctly.
In addition, Delta debugging cannot diagnose errors
where no working configuration is available. \ourtool overcomes the above limitations $\blacksquare$

%a particular failrue. It also pushes more of the work of troubleshooting
%onto the user site.

Recently, statistical algorithms have been applied to the automated
debugging domain. Clarify~\cite{Ha:2007:IER} uses program features
such as function call counts, call site, and stack dumps to improve
error reporting. The improved error reporting, although helpful,
does not disagnose the root cause. The CBI project~\cite{Liblit:2005:SSBI}
analyzes execution traces collected from deployed software
to isolate software failure causes. By constrast, CBI
identifies likely buggy statements as final output, while
\ourtool links the erroneous program behavior to specific
configuration options and outputs. $\blacksquare$

%a different abstraction 

%Specifically, the program
%is instrumented to collect information from certain run-time
%values and this information is passed on to a statistical engine
%to compute likely buggy predicates.$\blacksquare$

%\textbf{Configuration error diagnosis approaches.} Existing approaches include
%pure slicing-based technique~\cite{}, binary search-based
%technique~\cite{}, dynamic information flow-based
%technique~\cite{}, template-based technique~\cite{},
%invariant-based technique~\cite{}, decision-tree-based approach~\cite{Mickens:2007:SID}, and
%causality-based technique~\cite{Attariyan:2008:UCD}.
%problem shooting~\cite{ }. 

\subsection{Configuration-Aware Software Analysis}

Empirical studies show that configuration errors are pervasive, costly,
and time-consuming to diagnose~\cite{Yin:2011:ESC, Hubaux:2012}.
To over this problem, researchers have designed various
software analysis techniques to improve configuration
management~\cite{Sarma:2007:TSA, Giese:2004:MDV, Jezequel:1999:RVC,
Kloukinas:2000:ACM, hubaux:towards, Holl:2012:MEC}, understand
and test the behavior of configurable software systems~\cite{Grundy:1998:ECU,
Ma:2011:VDR, Georg:2001:UAU, Qu:2008:CRT, hoek02testbed, Reisner:2010:USE, itrees}.
For example, Rao et al.~\cite{Rao:2009:VRL} developed a technique
to automatically learn good configurations from historical data.
However, their approach has a quite different
focus than ours. Their approach aims to reduce the burden of
configuration management and prevent certain errors from happening,
rather than diagnose an exhibited error.
Qu et al.~\cite{Qu:2008:CRT} proposed to use combinatorial interaction
testing techniques to model and generate configuration samples for
use in regression testing. Recently, Song et al.~\cite{itrees}
presented iTree to improve combinatorial interaction testing
by discovering sets of configurations to test that are smaller
but can achieve higher teesting coverage. Compared to \ourtool,
those testing techniques can be used to find possible errors in
a configurable software sysem earlier, but our technique is
designed to localize the root cause of a configuration error.




%Typical
%approaches include easy configuration management interfaces
%for non-professional end-users~\cite{Kushman:2010:ECA},
%OS-supported configuration management~\cite{Su:2007:AIC},
%automated configuration recommendations~\cite{Zheng:2007:ACI},
%learning good configurations from historical data~\cite{Rao:2009:VRL}, and
%leveraging community users' knowledge for configuration tuning~\cite{Zheng:2011:MAC}.
%The above approaches focus on general configuration management issues, instead
%of error localizations.



%\textbf{Configuration-aware software analysis and testing.} Ways to understand
%configurable software and find configuration errors by testing. Representative work along this
%research line includes improving regression testing of configurable software~\cite{Qu:2008:CRT},
%using symbolic execution to understand configurable software~\cite{Reisner:2010:USE},
%white-box approaches~\cite{whiteboxconf},
%finding high-coverage configurations for testing~\cite{itrees}. Those approaches
%can serve as alternative ways to find configuration errors.



%\textbf{The use of execution spectrum to localize bugs.} There is a rich body of
%work in software engineering community in using execution spectrum for error diagnosis.
%Typical work includes~\cite{Liblit:2005:SSBI, Santelices:2009:LFU, Reps:1997:UPP,
%Yilmaz:2008:TTF}. However, our work has a different abstraction than theirs.

%\textbf{Software error isolation techniques.} Many techniques have been developed
%in the software engineering community. Representative work include delta debugging~\cite{Zeller:2002:ICC},
%dynamic slicing~\cite{Zhang:2006:LFT},
%capture and replay-based technique~\cite{Qi:2011:LFE}, approaches permitting
%users to ask program behaviors~\cite{Ko:2008:DRA},
%clustering-based~\cite{Dickinson:2001:FFC},
%error explanation~\cite{Groce:2006:EED},
%and approach combining static and dynamic information~\cite{Holmes:2011:IPT, Zhang:2008:EIF}.



%\textbf{Other related work.} This category includes inferring configuration options
%from source code with static analysis~\cite{Rabkin:2011:SEP}, 
%reverse engineering of software configurations~\cite{Wang:2008:TAR}, automatically
%fixing certain configuration errors by constraint solving~\cite{rangefix},
%empirical studies of configuration errors in practice~\cite{Yin:2011:ESC, Hubaux:2012},
%change impact analysis for configuration options~\cite{configimpact},
%and using program profiling and statistical analysis to solve
%other problems (e.g., critical program region identification)~\cite{Carbin:2010:AIC}.
%Program steering~\cite{Lin:2004:IAM}.


