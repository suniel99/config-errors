\section{Related Work}

We next discuss closely-related work on
software configuration error diagnosis, automated
debugging techniques; and configuration-aware software
analysis techniques.
 
%\subsection{Software Configuration Error Diagnosis}
\vspace{1mm}
\noindent \textbf{\textit{Software Configuration Error Diagnosis.}}
Several prior research efforts have applied different techniques to the
problem of software configuration error diagnosis~\cite{Attariyan:2008:UCD, 
Whitaker:2004:CDS, Wang:2004:AMT,
Attariyan:2010:ACT, Rabkin:2011:PPC, keller:conferr}. For example,
Chronus~\cite{Whitaker:2004:CDS} relies on a user-provided
testing oracle to check the behavior of the system, and uses
virtual machine checkpoint and binary search to find the
point in time where the program behavior
switched from correct to incorrect. AutoBash~\cite{Su:2007:AIC}
fixes a misconfiguration by using
OS-level speculation execution to try possible
configurations, examine their effects, and roll them back when necessary.
PeerPressure~\cite{Wang:2004:AMT} 
uses statistical methods to compare
configuration states in the Windows registry on different machines.
When a registry entry value on a machine exhibiting erroneous behavior differs
from the value usually chosen by other machines, PeerPressure
flags the value as a potential error. More recently, ConfAid~\cite{Attariyan:2010:ACT}
uses dynamic taint analysis to diagnose configuration problems 
by monitoring causality within the program binary as it executes.
ConfAnalyzer~\cite{Rabkin:2011:PPC} uses dynamic information flow analysis to precompute
possible configuration error diagnoses for every possible crashing point
in a program. 

Our technique is significantly different
from the existing approaches.
First, most existing approaches focus exclusively on configuration errors
that lead to software crash or assertion failures~\cite{Attariyan:2008:UCD, 
Whitaker:2004:CDS, 
Attariyan:2010:ACT, Rabkin:2011:PPC}.
%, by leveraging valuable crashing information such as stack traces.
By contrast, our technique can diagnose both
\textit{crashing} and \textit{non-crashing} errors.
Second, several existing approaches~\cite{Attariyan:2010:ACT, Whitaker:2004:CDS}
assume the existence of a testing oracle that can safely
check whether the software functions correctly. However, such oracles are often
absent in practice. Writing them may
require a substantial amount of time and effort that a
typical software user would not prefer, and should not be expected, to invest.
By contrast, our technique eliminates this assumption.
Third, approaches like PeerPressure~\cite{Wang:2004:AMT} benefit from
the known schema of the Windows registry, but cannot detect configuration errors
that lie outside the registry. Our technique of analyzing the
affected predicate behavior is more general for configurable software.

%Most prior work on configuration debugging has relied
%on large user communities or on modifying the program’s execution
%environment $\blacksquare$ focusing on crashing errors.

%In contrast, current industrial practice uses stack traces to cluster
%failure reports into equivalence classes. Two crash reports showing
%the same stack trace, or perhaps only the same top-of-stack function,
%are presumed to be two reports of the same failure. This heuristic
%works to the extent that a single cause corresponds to a single point
%of failure, but our experience with xxx suggests that this
%assumption may not often hold.




%Most prior work has taken a \textit{black box} approach that uses
%only state external to the application being debugged to infer the problem.

%\subsection{Automated Debugging Techniques}
\vspace{1mm}
\noindent \textbf{\textit{Automated Debugging Techniques.}}
%Many automated debugging techniques~\cite{Jones:2002, Zeller:2002:ICC,
% Horwitz:1988, clause07july} have been developed to localize software errors.
Program slicing~\cite{Horwitz:1988} and taint analysis~\cite{clause07july}
are two well-known techniques to determine which statements and inputs
could affect a particular variable. Despite their effectiveness
in diagnosing a crashing configuration error by performing backward
reachability analysis from the crashing point~\cite{Rabkin:2011:PPC,
Attariyan:2010:ACT}, these two techniques
can not be directly applied to diagnose a non-crashing error.

%By performing
%backward reachability analysis from the crashing point, these two
%techniques can be effective in diagnosing crashing configuration errors.
%Nevertheless How


Delta debugging~\cite{Zeller:2002:ICC} is a general
algorithm to isolate software error causes.
It works by differences between a working state and a broken
state to isolate a set of failure-inducing changes.
When using Delta debugging, users must provide a testing oracle to check
whether an intermediate program state behaves correctly,
and a working state which Delta debugging can refer to.
However, besides the difficulty in writing a good testing oracle,
a working state is often absent in practice since the error-revealing
input and configuration may have already been minimized (such as the
\errors configuration errors in our experiments).
By contrast, \ourtool eliminates these two assumptions by comparing
the undesired execution profile with correct execution profiles, and
then identifying the root cause configuration options to account for
the behavioral difference.

%diagnoses a configuration error from a different perspective.
%It compares the profile from an undesired execution with 
%similar correct profiles to identify behaviorally-d



%.
%In addition, when using Delta debugging to diagnose an error, the user must
%also provide a working state. However, besides 
% cannot diagnose errors
%where no working configuration is available. \ourtool overcomes the above limitations $\blacksquare$

%a particular failure. It also pushes more of the work of troubleshooting
%onto the user site.

Recently, statistical algorithms have been applied to the automated
debugging domain. Clarify~\cite{Ha:2007:IER} uses program features
such as function call counts, call site, and stack dumps to improve
error reporting. The improved error reporting, although helpful,
does not diagnose the root cause. The CBI project~\cite{Liblit:2005:SSBI}
analyzes a large number of executions collected from deployed software
to isolate software failure causes. Compared to \ourtool, besides
the significant difference in the error diagnosis algorithm, CBI
identifies likely buggy statements as final output, while
\ourtool identifies the behaviorally-deviated program predicates, and
links the undesired behavior to specific configuration options.

\todo{Not sure should meantion another significant difference
between CBI and \ourtool: CBI computes the probability of program failure
\textit{once} a predicate is observed to be true. \ourtool correlates
a predicate's true ratio (more precisely, the deviation degree of
a predicate) with the failure.}

%Furthermore, CBI 

%a different abstraction 

%Specifically, the program
%is instrumented to collect information from certain run-time
%values and this information is passed on to a statistical engine
%to compute likely buggy predicates.$\blacksquare$

%\textbf{Configuration error diagnosis approaches.} Existing approaches include
%pure slicing-based technique~\cite{}, binary search-based
%technique~\cite{}, dynamic information flow-based
%technique~\cite{}, template-based technique~\cite{},
%invariant-based technique~\cite{}, decision-tree-based approach~\cite{Mickens:2007:SID}, and
%causality-based technique~\cite{Attariyan:2008:UCD}.
%problem shooting~\cite{ }. 

%\subsection{Configuration-Aware Software Analysis and Testing}

\vspace{1mm}
\noindent \textbf{\textit{Configuration-Aware Software Analysis and Testing.}}
Empirical studies show that configuration errors are pervasive, costly,
and time-consuming to diagnose~\cite{Yin:2011:ESC, Hubaux:2012}.
To alleviate this problem, researchers have designed various
software analysis techniques to improve configuration
management~\cite{Giese:2004:MDV, Jezequel:1999:RVC,
Kloukinas:2000:ACM, Garvin:2011}, understand
and test the behavior of a configurable software system~\cite{Grundy:1998:ECU,
Qu:2008:CRT, Nanda:2011}.
For example, Garvin et al.~\cite{Garvin:2011} show how to
use historical data to avoid system failures during reconfiguration.
%a technique to automatically learn a good configuration from historical data.
However, their work aims to reduce the burden of
configuration management and prevent certain errors from happening,
while our technique is designed to diagnose an exhibited error.
Qu et al.~\cite{Qu:2008:CRT} proposed a combinatorial interaction
testing technique to model and generate configuration samples for
use in regression testing.
%Recently, Song et al.~\cite{itrees}
%presented iTree to improve combinatorial interaction testing
%by discovering sets of configurations to test that are smaller
%but can achieve higher testing coverage. 
Compared to \ourtool,
those testing techniques can be used to find new errors in
a configurable software system earlier, but can not
identify the root cause of a revealed configuration error.




%Typical
%approaches include easy configuration management interfaces
%for non-professional end-users~\cite{Kushman:2010:ECA},
%OS-supported configuration management~\cite{Su:2007:AIC},
%automated configuration recommendations~\cite{Zheng:2007:ACI},
%learning good configurations from historical data~\cite{Rao:2009:VRL}, and
%leveraging community users' knowledge for configuration tuning~\cite{Zheng:2011:MAC}.
%The above approaches focus on general configuration management issues, instead
%of error localizations.



%\textbf{Configuration-aware software analysis and testing.} Ways to understand
%configurable software and find configuration errors by testing. Representative work along this
%research line includes improving regression testing of configurable software~\cite{Qu:2008:CRT},
%using symbolic execution to understand configurable software~\cite{Reisner:2010:USE},
%white-box approaches~\cite{whiteboxconf},
%finding high-coverage configurations for testing~\cite{itrees}. Those approaches
%can serve as alternative ways to find configuration errors.



%\textbf{The use of execution spectrum to localize bugs.} There is a rich body of
%work in software engineering community in using execution spectrum for error diagnosis.
%Typical work includes~\cite{Liblit:2005:SSBI, Santelices:2009:LFU, Reps:1997:UPP,
%Yilmaz:2008:TTF}. However, our work has a different abstraction than theirs.

%\textbf{Software error isolation techniques.} Many techniques have been developed
%in the software engineering community. Representative work include delta debugging~\cite{Zeller:2002:ICC},
%dynamic slicing~\cite{Zhang:2006:LFT},
%capture and replay-based technique~\cite{Qi:2011:LFE}, approaches permitting
%users to ask program behaviors~\cite{Ko:2008:DRA},
%clustering-based~\cite{Dickinson:2001:FFC},
%error explanation~\cite{Groce:2006:EED},
%and approach combining static and dynamic information~\cite{Holmes:2011:IPT, Zhang:2008:EIF}.



%\textbf{Other related work.} This category includes inferring configuration options
%from source code with static analysis~\cite{Rabkin:2011:SEP}, 
%reverse engineering of software configurations~\cite{Wang:2008:TAR}, automatically
%fixing certain configuration errors by constraint solving~\cite{rangefix},
%empirical studies of configuration errors in practice~\cite{Yin:2011:ESC, Hubaux:2012},
%change impact analysis for configuration options~\cite{configimpact},
%and using program profiling and statistical analysis to solve
%other problems (e.g., critical program region identification)~\cite{Carbin:2010:AIC}.
%Program steering~\cite{Lin:2004:IAM}.


