
\section{Introduction}
\label{sec:introduction}

Modern software is extraordinarily complex. Many applications have a large
number of configuration options that offer users great flexibility to
customize their behaviors. This flexibility has a cost: when something
goes wrong, diagnosing a configuration error can be both time-consuming
and frustrating. Technical support contributes 17\% of the total cost of ownership of
today's desktop computers~\cite{confevidence}, and troubleshooting misconfigurations
is a large part of technical support.

Software misconfigurations are often exhibited by an application unexpectedly terminating
(i.e., crashing errors) or producing an incorrect output (i.e., non-crashing errors). While an ideal application would always
output a helpful error message when such events occur, it is unfortunately
the case that such messages are often cryptic, misleading, or
even non-existent~\cite{Yin:2011:ESC, Attariyan:2010:ACT, Hubaux:2012, rangefix}.
Thus, users must search manuals, FAQs, and online forums to find potential
solutions to the problem. %$\blacksquare$ this process is frustrating..

\subsection{Motivating Example}
\label{sec:mot}

This section describes a real scenario in which we used \ourtool to solve
a configuration problem. During the maintenance of the Randoop
automated test generation tool~\cite{randoop}, we received a ``bug report''
from a testing expert who had been using Randoop for quite a while.
The ``bug report'' indicated that Randoop failed to generate
any tests for the NanoXML~\cite{nanoxml} program. The sympton is that
Randoop terminates normally but does not generate any tests.
%on NanoXML, it 

Although the reported behavior is deterministic and fully-reproducible,
this silent failure (or called, non-crashing error) is
particularly difficult to diagnose. Differing from
a crashing error, Randoop just exits normally without exhibiting
a crashing point or dumping a stack trace. This makes many well-known
techniques such as dynamic slicing~\cite{Zhang:2003:PDS},
dynamic information flow tracking~\cite{Attariyan:2010:ACT},
failure trace analysis~\cite{Rabkin:2011:PPC} inapplicable.
In addition, for this scenario, there is no obvious testing oracle to check whether
Randoop behaves as desired. This unfortunately makes many
search-based fault isolation techniques such Delta Debugging~\cite{Zeller:2002:ICC}
ineffective. Furthermore, the configuration and
inputs reported for reproducing this error have already been
minimized: if any part of them is removed, this erroneous behavior
is no longer triggered. 

Actually, this bug report is not real. It does not reveal a bug
in the Randoop code. Its root cause is because
the user forgot to set one configuration option.
However, to the best of our knowledge, none of the
existing configuration error diagnosis technique~\cite{Attariyan:2008:UCD, 
Yuan:2011:COC, Su:2009:AGP, Whitaker:2004:CDS, Wang:2004:AMT,
Attariyan:2010:ACT, Rabkin:2011:PPC} can be directly applied.


%We reproduce this problem in a \ourtool-enabled environment.
%It determines which predicates exemplify the buggy behaviors.


\begin{figure}[t]
\begin{CodeOut}
\begin{alltt} 
Suspicious configuration option: maxsize

It affects the behavior of predicate:
"newSequence.size() > GenInputsAbstract.maxsize"
(line 312, class: randoop.ForwardGenerator) 

This predicate evaluates to true:
  14.4\% of the time in normal executions (1315 observations)
  32.3\% of the time in the unexpected execution (2727 observations)

\end{alltt}
\end{CodeOut}
\vspace*{-15pt}
\Caption{{\label{fig:output}
The top ranked configuraion option in \ourtool's error report
for the motivating example in Section~\ref{sec:mot}. The complete error
report contains 31 options. For brevity, we omit the remaining ones.
}} %\vspace{-5mm}
\end{figure}
%, this predicate evaluates to true:   (1315 observation%run, it evaluates to true:  (2727 observations)s)

As an alternative, we can use our technique (and its tool implementation \ourtool)
to daignose and solve this problem. We first reproduce this
error in a \ourtool-enabled environment, and let \ourtool to diagnose this root cause.
As a result, \ourtool produces
an error report (Figure~\ref{fig:output}) in the form of an ordered list of possible responsible
configuration options.
The error report in Figure~\ref{fig:output} suggests
a configuration option named
\CodeIn{maxsize} is the most likely responsible one.
The error report also provides relevant explantory
information: % about the reason why \CodeIn{maxsize} should be account for:
a predicate affected by \CodeIn{maxsize} behaves dramatically
different between the observed erroneous execution and all correct executions 
as kept in \ourtool's database.


%Guided by the report, a user may further find the root
%cause of this behavior. 
Figure~\ref{fig:example} shows
the relevant code snippet for this problem. Guided by the report,
we can further understand the root cause of Randoop's failure
is because when Randoop generates a new
test (line 100, in the form of method-call sequence for Java programs),
it first compares the length of the created sequence with
\CodeIn{maxsize}'s value (default value: 100). If a
generated sequence exceeds this pre-defined max length,
Randoop discards it to avoid length explosion.
For most programs, using \CodeIn{maxsize}'s default value works remarkably
well, and only 14.4\% of the generated sequences have been discarded.
However, for NanoXML, the generated sequences are
much longer than usual; using the \CodeIn{maxsize}'s default value
results in 32.2\% of the generted sequences being discarded.
%are discarded because the generated sequences are much longer.  
\ourtool captures such deviated behavior, pinpoints the
\CodeIn{maxsize} option, and suggests users to change its value.
Finally, this problem can be easily resolved after
the user set \CodeIn{maxsize} to a larger value, for example, 500.

\begin{figure}[t]
\vspace{-2mm}
{In class: randoop.ForwardGenerator}\\
%\small{//a configuration option to control a sequence's max length}
\vspace{-4mm}
\begin{CodeOut}
\begin{alltt}
int maxsize = readFromCommandLine(); //{initialize the option value}

99.  public ExecutableSequence step() \ttlcb
100.   ExecutableSequence eSeq = createNewUniqueSequence();
101.   AbstractGenerator.currSeq = eSeq.sequence;
102.   eSeq.execute(executionVisitor);
103.   processSequence(eSeq);
104.   if (eSeq.sequence.hasActiveFlags()) \ttlcb
105.     componentManager.addGeneratedSequence(eSeq.sequence);
106.   \ttrcb
107.   return eSeq;
108. \ttrcb

310. private ExecutableSequence createNewUniqueSequence() \ttlcb
311.   Sequence newSequence = ...; //sequence creation step omitted
312.   if (newSequence.size() > maxsize) \ttlcb
313.     return null;
314.   \ttrcb
315.   if (this.allSequences.contains(newSequence)) \ttlcb
316.     return null;
317.   \ttrcb
318.   return new ExecutableSequence(newSequence);
319. \ttrcb
\end{alltt}
\end{CodeOut}
\tinystep
\vspace*{-3.0ex} \Caption{{\label{fig:example} 
Code excerpt from Randoop~\cite{randoop}
to explain the configuration problem as pointed
outed by the Figure~\ref{fig:output}.
%A forward slice computed by the traditional slicing algorithm~\cite{Horwitz:1988:ISU}
%from the seed statement includes statements 2, 3,
%4, 5, 6, 7, 9, 13, 14, 16, 17, and 19.
%By contrast, a thin slice~\cite{Sridharan:2007}
%only contains line 13.
}} %\vspace{-1.8mm}
\end{figure}




%\vspace{1mm}
%\noindent \textbf{\textit{Our technique.}} 

\subsection{Diagnosing Configuration Errors}

The process of diagnosing configuration errors can be divided into two
separate tasks: identifying which specific configuration option is
responsible for the unexpected behavior, and determining how to fix that
configuration option. In this paper, we address the former task: finding
the root cause of a configuration error.
%, and leave the later task
%of fixing

\textbf{Our technique} contains three steps to 
link the erroneous behavior to specific responsible configuratio options.

\begin{itemize}
\item \textbf{Configurtion Propagation Analysis}. For
each user-settable configuration option, \ourtool
uses a lightweight dependence analysis technique, called Thin Slicing~\cite{Sridharan:2007},
to statically identify its affected predicate in the code.

\item \textbf{Configuration Behavior Profiling}. \ourtool
performs \textit{selective instrumentation} to capture the
dynamic behaviors of the test code when erroneous behavior
is observed.

\item \textbf{Configuration Deviation Analysis}.
When erroneous behavior is revealed, \ourtool looks up a
pre-built database, selects the most similar profiles, and
performs statistical analysis to identify which configuration
option's behavior deviates most between a good run and a bad.

\end{itemize}

Essentially, \ourtool reduces the problem of explaining a
configuration error to
identifying that the xxx is in a state similar to a xxx good state
on the reference computer for which a solution is known.
The output of \ourtool is a ranked list of
configurations that could possibly explain the why the program does not produce the desirable result. Those
configurations, if changed, may even fix the unexpected behavior.

%$\blacksquare$ We assume that the error is know, i.e., the error has
%been previously encountered ... 

Compared to existing approaches~\cite{Zeller:2002:ICC, Zhang:2003:PDS,
Rabkin:2011:PPC, Whitaker:2004:CDS, Attariyan:2010:ACT, Wang:2004:AMT}, \ourtool has
several notable features:

\begin{itemize}
\item \textbf{It is fully-automated}.
\ourtool does not require a user to specify
\textit{when}, \textit{why} and \textit{how} the program fail. This is
different than many well-known automated debugging techniques such
as delta debugging~\cite{Zeller:2002:ICC} and dynamic slicing~\cite{Zhang:2003:PDS}.
Our technique also generates concise descriptions of the problem
with a given option, as is ranking possible diagnoses.

\item \textbf{It can diagnose both non-crashing and crashing errors}.
Most of the existing techniques~\cite{Rabkin:2011:PPC,
Whitaker:2004:CDS, Attariyan:2010:ACT} focus exclusively on configuration errors
where the value of an option is wrong and this causes a program
to fail in a deterministic way with an error message, while
ignoring configuration problems that manifest themselves as
silent failures. By contract, \ourtool is capable to diagnose
both kinds of errors.

\item \textbf{It require no OS-level support.} Our technique requires no alterations to
the JVM or standard library. This distinguishes our work from
competing techniques such as OS-level configuration
error troubleshooting~\cite{Whitaker:2004:CDS}.% or dynamic taint tracking~\cite{clause07july}.

\end{itemize}


We envision that \ourtool can be used by end-users or
administrators to identify the likely root cause of a configuration error.
When an end-user or administrator wishes to diagnose a
problem such as a crash or incorrect output, she
reproduces the problem on a \ourtool-enabled environment,
where \ourtool tracks the causal dependencies between
configuration options and the program behavior. 
When a configuration error happens, users can
use \ourtool to diagnose the problem based on the recorded profile.

Another critical component in \ourtool is the pre-built
profile database. We envision that this being done
by the developers at release. The profile database keeps
a set of correct execution traces xxxx, by running system
tests xxxx. $\blacksquare$ easy to build, even one trace
is enough in our experiments. we use manual examples. real programmers
should even feel easier.


%the software developers
%built the database initially, and other users can also enrich
%the database.
%, a
%the software developers provide a profile database, which users can use
%to query. The users can also enrich the database, providing their
%own examples. even a single run

%use the recorded profile message to query this database, perhaps via a web
%service.
 
%The core of our approach is to xxx. 
%We envision this being done by the developers
%at release time. 

%this technique xxx could be performed by
%the software developers; users would need only to provide
%the profiles xxx to back a diagnosis.


%We have developed a tool, called \ourtool, that uses xxx





%\vspace{1mm}
%\noindent \textbf{\textit{Evaluations.}} 

\subsection{Evaluation}

We evaluated \ourtool on \errors configuration errors
from \subjectnum configurable software, comprising
\crash crashing errors and \noncrash non-crashing errors.
Our results show that \ourtool identifies the correct
root causes of most configuration errors. xxxx top 3,
it lists the correct root cause as the top 3 xxx.
This allows \ourtool user to focus on a few specific configuration
options when deciding how to fix the problem. 
In addition, \ourtool takes less than xxx minutes for diagnosing
one configuration error, making it an attractive alternative
to manual debugging.

We also compared the effectiveness of \ourtool to
4 existing approaches: statement-level profiling~\cite{Jones:2002}, method-level
invariant detection~\cite{Ernst:1999}, and dynamic tainting-based configuration
error diagnosis techniques~\cite{Rabkin:2011:PPC}. The experiment results show that
\ourtool significantly outperforms both statement-level profiling
and method-level invariant detection in diagnosis accuracy, suggesting
that focusing on the behavior of program predicates can be a
suitable abstraction. Compared to ConfAnalyzer~\cite{}, a dynamic taint-based technique
which precisely tracks information flow at the bit level , \ourtool
produces accurate diagnosis information for 5 non-crashing errors that
ConfAnalyzer fails to diagnose, and output better info for 5 out of 9
crashing errors. 

Finally, we investigate the effecting of varying the comparison profiles.
Our experimental results show that varying the
profile selection strategy can result in substantially different effects on the
diagnosis results, depending on the application being analyzed;
and the similarity-based selection strategy used in \ourtool outperforms
the other two alternatives.


%\ourtool outputs an ordered list of probable root causes.
%Each entry in the list is a user-settle configuration option;
%our results show that \ourtool typically outputs
%the actual responsible configuration option as the top 3 in the list.

%By finding the
%needle in the haystack, \ourtool can be an attractive ...

%While xxx analysis takes a few minutes for a complex application,
%automated error diagnosis is still considerably faster and
%less labor-intensive than manual debugging or searching
%through other resources.

%$\blacksquare$ our technique is lightweighted.

%\vspace{1mm}
%\noindent \textbf{\textit{Contributions.}}

\subsection{Contributions}
This paper makes the following contributions:

\begin{itemize}
%\item \textbf{Problem.} To the best of our knowledge, we are the first to address
%the invalid thread access error detection problem for multithreaded GUI applications.

\item \textbf{Technique.} We present a technique to diagnose
software configuration errors. Our technique uses static analysis,
dynamic profiling, and statistical analysis to link the
erroneous behavior to specific configuration options (Section~\ref{sec:technique}).


\item \textbf{Implementation.} We implemented our technique 
in a tool, called \ourtool, for Java software (Section~\ref{sec:implementation}). Our tool implementation is publicly available at
\url{http://config-errors.googlecode.com}. 


\item \textbf{Evaluation.} We applied \ourtool to diagnose
\errors configuration errors in \subjectnum
configurable Java software. The results
show the usefulness of the proposed technique (Section~\ref{sec:evaluation}).

\end{itemize}



