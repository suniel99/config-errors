\ourtool starts error diagnosis after obtaining the execution profile from
an undesired execution. It selects similar
profiles from known correct executions (Section~\ref{sec:similar}), compares
each selected profile with
the undesired one to identify the most behavioral-deviated predicates
(Section~\ref{sec:deviation}), and then determines
the likely root cause options (Section~\ref{sec:linking}).


\subsubsection{Selecting Similar Execution Profiles for Comparison}
\label{sec:similar}

\ourtool's database contains a number of
profiles from known correct executions.  These execution profiles 
can be dramatically different from another.  To avoid reporting irrelevant
differences when 
determining how and why the observed execution profile behaves
differently from the correct ones, \ourtool first
compares the undesired profile with the existing
correct profiles, then selects a set of similar ones
as the basis of diagnosis.

Given an execution profile $e$, \ourtool first aggregates
the collected predicate profiles into a $n$-dimensional
vector $v_e$ =$\langle r_{e1}, r_{e2}, ..., r_{en}\rangle$, where $n$
is the number of all possible predicate profiles in the program
and each $r_{ei}$ is a ratio representing how often the $i$-th predicate
profile evaluated to true at runtime.
If a predicate has never been executed in an execution,
\ourtool uses 0 as its true ratio. %in its corresponding predicate profiles.

\ourtool computes the similarity of two executions profiles $e$ and $f$
by computing the standard cosine similarity from information retrieval~\cite{Witten96managinggigabytes}
of $v_{e}$ and $v_{f}$.

\vspace{-2mm}

{\small{
\[
\|Similarity|(e, f) = \|cos\_sim|(v_{e}, v_{f}) = \frac{\sum_{i = 1}^{n}r_{ei} \times r_{fi}}
{\sqrt{\sum_{i = 1}^{n}r_{ei}^2} \times \sqrt{\sum_{i = 1}^{n}r_{fi}^2}}
\]
}}

\vspace{-2mm}
\todo{Weird enough that why two sqrt symbols above are not the same size?}

This similarity metric basically compares two execution profiles based on
 control flow taken (approximated by how often a predicate is evaluated to
true). Its resulting value ranges from 0 meaning completely different, to 1 meaning almost the same, 
and in-between values indicating intermediate similarity.


%$\blacksquare$ put the intuition here. dot production? why ignore
%numbers? similarity? control flow.

%I am re-inventing wheels below, just use cosine similarity above

%\ourtool computes the distance between two execution profiles $t_1$ and $t_2$ using
%the following equations:

%{\small{
%\[
%\|Distance|(t_1, t_2) = 1 - \frac{\sum_{i = 1}^{n}Delta(r_{1i}, r_{2i})^2}{n}
%\]

%\[
%\|Delta|(r_1, r_2) = 
%\left\{
%\begin{array}{l l l l}
%  0 & \ \mbox{if $r_1$ = \CodeIn{N/A} and $r_2$ $\neq$ \CodeIn{N/A}}\\
%  0 & \ \mbox{if $r_1$ $\neq$ \CodeIn{N/A} and $r_2$ = \CodeIn{N/A}}\\
%  1 & \ \mbox{if $r_1$ = \CodeIn{N/A} and $r_2$ = \CodeIn{N/A}}\\
%  \CodeIn{min}\{r_2/r_1, r_1/r_2\} & \; \mbox{otherwise}\\ \end{array} \right.
%\]
%}
%}


%$\blacksquare$ need to illustrate why use the distance metric.
%\todo{Also explain/justify.  Give intuition for the metric.}


A crashing error may happen shorty after the program is launched.
Therefore, the resulting execution profile can be much smaller than
%a correct execution profile, 
most correct execution profiles,
since many predicates in the program are not executed.
Using the $Similarity$ metric to compare a correct execution profile from the database
with a crashing profile, it is unlikely for \ourtool to find similar ones.
To facilitate comparison, %avoid comparing those un-executed predicates,
when diagnosing a crashing error, \ourtool first
chops each correct execution profile by only remaining
the common predicates executed by the crashing profile, and then
uses the chopped profile for comparison. 

Given an undesired execution profile,
\ourtool selects all execution profiles (or the chopped profiles for
a crashing error) from the database
with a $Similarity$ value above a threshold (default value: 0.9, as used in our
experiments).

%In addition,
%when diagnosing a crashing error, such chopped execution profiles will replace the original profiles
%(in the database) in the following steps.

%(also used
%the chopped profile for next steps). Doing so, \ourtool
%avoids comparing irrelevant differences in a correct execution
%profile which a crashing profile even has not reached yet.

%An execution profile from a crashing error

%\todo{Mike does not understand this paragraph.}
%For the execution profile produced in a crashing error, \ourtool 
%chops a correct execution profile from the database by only remaining the predicate
%profiles covered by the undesired profile. \ourtool performs
%this simple preprocessing because a crashing error $\blacksquare$
%often produces an incomplete execution profile, and it is unlikely
%to find a similar one from the database.


\subsubsection{Identifying Deviated Predicates}
\label{sec:deviation}


Our
automated error diagnosis approach compares an undesired execution profile with a set
of \textit{similar} and \textit{correct} execution profiles. 
%\todo{I like the following sentence.  This intuition should appear in the
%  introduction as well.  The introduction should say what we do (it does
%  this) and also give a hint as to the approach.}
The behavioral differences in the recorded predicates provide evidence for what parts of a program might be
incorrect and why. %This helps to further reason about its root cause.

For a predicate in an execution profile,
there are two primary factors to
characterize its dynamic behavior: how often it is
evaluated (i.e., the number of the
observed executions), and how often it is evaluated to true (i.e., the true ratio).
Ideally, we would like to have a metric
to capture a predicate's desired behavior
from the known correct execution profiles, but is robust enough
to tolerate small noises so that it does
not overfit a specific correct execution profile.
Looking more closely, we found that although
the general execution control flows (approximated by
a predicate's true ratio) may be similar for many 
execution profiles, the absolute execution
number of a predicate may largely depend on the given input, % and
varying greatly across executions. On ther other hand, for
some predicates, they may have
only been executed very few times but the true ratios
are dramatically different across executions.

%that in two execution profiles, a predicate's is only executed
%in very few times but the true ratios are 
%As another example, some program has preprocessing... $\blacksquare$


%Thus, if we can generate a signature that
%captures the execution path of a predicate, we should be
%able to more precisely identify a configuration error

Thus, we need to a metric that can characterize
a predicate's behavior with high sensitivity, meaning the predicate's
behavior (i.e., true ratio) is correlated with
%ratio accounts for
the execution results (i.e., correct or undesired).
But we also want
high specificity, meaning a predicate's behavior
is more representative if it is observed
in more executions.
%behavior should not be
%mis-characterized only based on a small number of
%observed executions.
To do so, we define the following
$\phi$ metric by using a standard way to
combine sensitivity and specificity to compute their
harmonic mean; this metric prefers high scores in both dimensions. 

\vspace{-3mm}

{\small{
\[
\|\phi|(e, p) = \frac{2}{{1}/{\|trueRatio|(e, p)} + {1}/{totalExecNum(e, p)}}
\]
}}

\vspace{-3mm}

In $\phi(e, p)$, $trueRatio(e, p)$ is a function that returns the ratio of the predicate $p$ being
evaluated to true in $e$, and $totalExecNum(e, p)$ is a function
that returns the total number of predicate $p$ being executed in $e$.
To smooth corner cases, if a predicate $p$ is not executed in $e$, i.e., 
$totalExecNum(e, p) = 0$, then $\phi(e, p)$ returns 0; and if a predicate $p$'s true ratio is 0, i.e., $trueRatio(e, p) = 0$, then $\phi(e, p)$ returns
$1/totalExecNum(e, p)$.


Using metric $\phi$, we define the following $Deviation$ metric
to characterize a predicate $p$'s deviation degree across two execution
profiles $e$ and $f$. A larger $Deviation$ value indicates that
predicate $p$ has a more deviated behavior. % between $e$ and $f$.

\vspace{-2mm}

{\small{
\[
\|Deviation|(p, e, f) = |\phi(e, p) - \phi(f, p)|
\]
}}
\vspace{-4mm}

%It balances a predicate's evaluation result and the total number of executions.
\todo{the basic idea is here, but this paragraph needs re-write}
The definitions of metrics $\phi$ and $Deviation$ have several desirable properties
in characterizing a predicate $p$'s behavior. Clearly, $trueRatio(e,p)$
is a value between 0 and 1 and $totalExecNum(e, p)$ is a value greater than 1; increasing
either value while keeping the other one unchanged increases $\phi(e, p)$.
\ourtool focuses on monitoring a program's control flow, which is also
reflected in the definition of $\phi$. In $\phi(e, p)$,
the value of $trueRatio(e, p)$ is more important than the value
of $totalExecNum(e, p)$
in deciding $\phi(e, p)$; since in theory: $1/trueRatio(e, p) \geq 1/totalExecNum(e, p)$, but
in practice: often $1/trueRatio(e, p) \gg 1/totalExecNum(e, p)$.
Thus, the value of $Deviation(p, e, f)$ is much more sensitive to $p$'s
true ratio change between execution profiles $e$ and $f$. 
\todo{A few more properties I do not known how (and whether need) to write down:
(1) when totalExecNum(e, p) increases to some extent, the change Deviation(p)
becomes ignorable, and the change of trueRatio dominates the Deviation value. (2)
}

%and $Deviation(p)$'s value is more sensitive to the change of $trueRatio(e, p)$.
%When computing $Deviation(p)$ across two executions,
%if $p$'s execution number is unchanged, a small change of $p$'s true ratio
%would lead to a non-trivial $Deviation$ value. On the other hand,
%if $p$'s true ratio is unchanged, the change in $p$'s execution number
%leads to a less noticeable $Deviation$ value.

%Intuitively, for two predicates $p_1$ and $p_2$, if they have the same
%he true ratio but $p_1$ has been observed in more executions, we
%should have more confidence in its statistical significance. $\blacksquare$
%(the wording here is bad)


\ourtool computes the $Deviation$ value for each predicate $p$
appearing in two execution profiles $e$ or $f$, and
ranks them in a decreasing order. 

%After comparing the undesired execution profile with one selected correct execution profile,
%\ourtool ranks all observed predicate profiles in
%a decreasing order based on the computed $Deviation$ value.




%\subsubsection{Filtering Execution Noises}
%remove some off-by-one


\subsubsection{Linking Predicates to Root Causes}
\label{sec:linking}

% and outputs a ranked list of suspicious
%configuration options.

\ourtool links the behavioral-deviated
predicate to their root cause configuration options
by using the results of thin slicing (computed by the Configuration Propagation
Analysis step in Section~\ref{sec:prop}).
\ourtool identifies 
the affecting configuration options for each deviated predicate,
and treats the configuration option
affecting a higher ranked deviated predicate as the more likely
root cause. If a predicate is affected by multiple
configuration options, \ourtool prefers options whose initialization
statements are \textit{closer} to the
deviated predicate (in terms of the breath-first search
distance in the dependence graph of thin slicing).
This heuristic is based on the intuition that statements closer to the
predicate seem more likely to be relevant to its behavior.
Hence, we assume the user gradually explores statements of
increasing distance from the suspicious predicate until
the desired statements %(where a configuration option is initialized)
are found; a breadth-first
search of the dependence graph simulates this strategy.


When multiple correct execution profiles are selected for comparison,
\ourtool first produces a ranked list of root cause
configuration options for each comparison pair, and then outputs
a final list by using majority voting over all ranking lists.
In the final ranking list, one configuration option ranks higher
than another if it ranks higher in more than half of the ranking lists.
However, according to Arrow's impossibility theorem~\cite{Fishburn1970103},
no rank order voting system can produce a non-cyclic ranking while also
meeting a specific set of criteria (e.g., getting more than half the voters).
Our implementation breaks possible cycles by arbitrarily ranking the
involved options, but our experiments did not use this capability.

\todo{Should add 1 more sentence to say how to generate the explanation?
In particular, the true ratio in the normal runs are averaged from all
good runs. need to mention that?}

%\todo{Such a ranking can have cycles.  Does the implementation suffer this
%  problem?}

