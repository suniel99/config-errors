\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

Our evaluation answers the following research questions:

%\todo{I would organize the research questions as:
%\begin{itemize}
%\item
%How effective is \ourtool?
%\begin{itemize}
%\item
%in absolute terms
%\item
%compared to other tools
%\end{itemize}
%This can include run time
%\item
%Discussion of internal implementation choices of \ourtool
%\end{itemize}
%}

%\todo{Give cross-references to sections that address these.}

\begin{itemize}
\item How effective is \ourtool in error diagnosis? \ourtool's effectiveness can be reflected by:
\begin{itemize}
  \item the absolute ranking of the actual root cause in \ourtool's output (Section~\ref{sec:accuracy}).
  \item the time cost in error diagnosis (Section~\ref{sec:performance}).
  \item comparison with existing configuration error diagnosis techniques (Section~\ref{sec:confanalyzer}).
  \item comparison with fault-localization-based configuration error diagnosis techniques (Section~\ref{sec:comparison}).
\end{itemize}
\item What are the effects of using full slicing rather than thin slicing to identify
the affected predicates, and the effects of varying comparison execution profiles (Section~\ref{sec:choices})?
These are two internal design choices. % in \ourtool.
\end{itemize}

%\begin{itemize}
%\item How effective is \ourtool in diagnosing the root cause of
%a configuration error (Section~\ref{sec:accuracy})?
%\item Can \ourtool provide more accurate diagnosis information than
%other approaches (Section~\ref{sec:comparison})? 
%\item How long does \ourtool take to diagnose a configuration error (Section~\ref{sec:performance})?
%\item What are the effects of varying the internal implementation of \ourtool,
%such as using a different configuration propagation analysis and
%different comparison execution profiles (Section~\ref{sec:choices})?
%\item What are the effects of varying the comparison execution profiles (Section~\ref{sec:choices})?
%\end{itemize}

%We use two metrics to evaluate \ourtool's effectiveness:
%the absolute ranking of the actual root cause in \ourtool's output,
%and the time cost used in diagnosis.

\subsection{Subject Programs}


We evaluated \ourtool on \subjectnum Java programs shown
in Figure~\ref{tab:subjects}.
Randoop~\cite{PachecoLET2007} is an automated test generator
for Java programs. Weka~\cite{weka} is a toolkit that implements
machine learning algorithms. Our evaluation
only uses its decision tree module. JChord~\cite{jchord}
is a program analysis platform that enables users to design, implement,
and evaluate static and dynamic program analyses for Java.
Synoptic~\cite{Beschastnikh:2011} mines a finite state machine
model representations of a system from logs.
Soot~\cite{soot} is a Java optimization framework for analyzing and transforming Java bytecode.


\subsubsection{Configuration Errors}

\begin{figure}[t]
\centering
\small{
\setlength{\tabcolsep}{.64\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Config Options & \#Profiles\\
 \hline
 \hline
 Randoop (1.3.2) & 18587 & 57 & 12\\
 Weka (3.6.7) & 256305 & 14 & 12\\
 JChord (2.1) & 23391 &  79 & 6 \\
 Synoptic (trunk, 04/17/2012) & 19153 & 37 & 6\\
 Soot (2.5.0) & 159273 & 49 & 16 \\
\hline
\end{tabular}
}

%\todo{Give a date for Synoptic}
\Caption{{\label{tab:subjects} Subject programs. 
Column ``LOC'' is the number of lines of code,
as counted by CLOC~\cite{cloc}. Column ``\#Config Options''
is the number of available configuration options. Column ``\#Profiles''
is the number of execution profiles in the pre-built database.}}
\end{figure}


\begin{figure}[t]
\setlength{\tabcolsep}{.94\tabcolsep}
\small{
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the specified class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}
}
\Caption{{\label{tab:errors} A list of \errors
configuration errors used in the evaluation.
%The 9 crashing errors in the bottom table are taken from~\cite{Rabkin:2011:PPC}.
}}
\end{figure}

We searched forums, FAQ pages and the literature of
configuration error diagnosis research to find actual
configuration problems that users have experienced with our
target applications. 
We chose \errors configuration errors, in which
the misconfigured values cover various data types, such as enumerated types,
numerical ranges, regular expressions, and text entries;
as listed in Figure~\ref{tab:errors}. The \noncrash non-crashing errors
are collected from actual bug reports, mailing list posts, and our own experience.
The \crash crashing errors, taken from~\cite{Rabkin:2011:PPC},
were used to evaluate the ConfAnalyzer tool.
All \errors configuration errors have been minimized: if
any part of the configuration or input is removed, the software
either crashes or no longer exhibits the undesired behavior.
%\todo{Were any of them used in previous research?  Was that the reason we
%  chose them?}

\input{result-fig}

\subsection{Evaluation Procedure}

For each subject program, we constructed a profile database
by running existing (correct) examples from its user manual, discussion
mailing list, and published papers~\cite{PachecoLET2007, Beschastnikh:2011, Rabkin:2011:PPC}.
We spent 3 hours per program, on average, and obtained 6--16 execution profiles.
The average size of the profile database is 35MB, and the largest database (Randoop's)
is 72MB.

We made a simple syntactic change to JChord, which affected 24 
lines of code. This change
does not modify JChord's semantics; rather, it just encapsulates
scattered configuration option initialization statements 
as static class fields. \todo{This sentence needs a rewrite:}This is purely an implementation
choice because having a centralized initialization statement
makes our tool implementation easier to specify the seed statement
in performing slicing. Here is a sample modification, where 
\<chord.kobj.k> 
is a configuration option
passed as a system property:


\begin{CodeOut}
\begin{alltt}
public void run() \ttlcb
  ...
  int kobjK = Integer.getInteger("chord.kobj.k");
  ...
\ttrcb
\end{alltt}
\end{CodeOut}
\vspace{-4mm}
\hspace{20mm}$\Downarrow$ 
%\vspace{-2mm}
\begin{CodeOut}
\begin{alltt}
static int chord\_kobj\_k = Integer.getInteger("chord.kobj.k");
public void run() \ttlcb
  ...
  int kobjK = chord\_kobj\_k; 
  ...
\ttrcb
\end{alltt}
\end{CodeOut}


When diagnosing a configuration error, we first reproduce the
error on a \ourtool-instrumented program to obtain the
execution profile. Then, using the obtained execution profile, we use \ourtool
to identify its root causes.

%since we know the misconfigured, root-cause entry for each case,
%we use the ranking of the entry as our evaluation metric.

Our experiments were run on a
2.67GHz Intel Core PC with 4GB physical memory (2GB was allocated
for the JVM), running Windows 7.


\subsection{Results}
\label{sec:results}


\subsubsection{Accuracy in Diagnosing Configuration Errors}
\label{sec:accuracy}
\input{diagnosisresults}

\subsubsection{Performance of \ourtool}
\label{sec:performance}
\input{performance}

\subsubsection{Comparison with an Existing Technique}
\label{sec:confanalyzer}
\input{confanalyzer}


\subsubsection{Comparison with Two Fault-Localization-Based Approaches}
\label{sec:comparison}
\input{comparison}



%\subsubsection{Effects of Varying Comparison Execution Profiles}
\subsubsection{Evaluation of Two Design Choices in \ourtool}
\label{sec:choices}
\input{designchoice}


\subsection{Experimental Discussions}


\noindent \textbf{\textit{Limitations.}} 
We conclude three limitations of our technique from the experiments. %is limited in three aspects.
First, we only focus on named configuration options
with a common key-value semantic, and our implementation
and experiments are restricted to Java. 
Second,  our implementation currently does not
support debugging non-deterministic errors. 
For non-deterministic errors, \ourtool could potentially leverage 
a deterministic replay system
that can capture an undesired non-deterministic
execution and faithfully reproduce it for late analysis.
Third, \ourtool's effectiveness  largely
depends on the availability of a similar but correct execution profile.
Using an arbitrary execution profile (as we demonstrated in Section~\ref{sec:choices}
by random selection) may significantly affect the results.

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} 
There are two major threats to validity in our evaluation. 
First, the \subjectnum programs and the configuration errors may not be
representative. Thus, we can not claim the results can be
extended to an arbitrary program.
Another threat is that we only employed two dependence
analyses (thin slicing and full slicing) and three
abstraction granularities (at the predicate level,
statement level, and method level) in our evaluation.
 Using other dependence analyses or abstraction levels
might achieve different results.

%how easy to construct such database in practice.
%User study of usefulness of the results


\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} 
We have three chief findings: (1) \ourtool is effective
in diagnosing both crashing and non-crashing configuration errors
with a small profile database.
(2) \ourtool produces more accurate diagnosis than
approaches leveraging existing fault localization
techniques~\cite{Jones:2002, McCamant:2003}, 
suggesting the necessity of designing new configuration error
diagnosis techniques. And (3) Using thin slicing
%to identify the affected predicates
permits \ourtool to produce more accurate diagnosis than using
full slicing; and varying the execution profile selection
strategy can result in substantially different diagnosis.

%\ourtool makes configuration error diagnosis easier by suggesting
%the specific options that may lead to an unexpected behavior. 




%Compared to
%alternative approaches, \ourtool distinguishes itself by being able to
%diagnose both crashing and non-crashing errors without requiring
%a user-provided testing oracle. $\blacksquare$
