\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

Our evaluation answers the following research questions:

%\todo{I would organize the research questions as:
%\begin{itemize}
%\item
%How effective is \ourtool?
%\begin{itemize}
%\item
%in absolute terms
%\item
%compared to other tools
%\end{itemize}
%This can include run time
%\item
%Discussion of internal implementation choices of \ourtool
%\end{itemize}
%}

%\todo{Give cross-references to sections that address these.}

\begin{itemize}
\item How effective is \ourtool in error diagnosis? \ourtool's effectiveness can be reflected by:
\begin{itemize}
  \item the absolute ranking of the actual root cause in \ourtool's output (Section~\ref{sec:accuracy}).
  \item the time cost of error diagnosis (Section~\ref{sec:performance}).
  \item comparison with a previous configuration error diagnosis technique (Section~\ref{sec:confanalyzer}).
  \item comparison with two fault localization techniques (Section~\ref{sec:comparison}).
\end{itemize}
\item What are the effects of using full slicing~\cite{Horwitz:1988}
 rather than thin slicing~\cite{Sridharan:2007} to identify
the affected predicates?  What are the effects of varying comparison execution profiles?
These are two internal design choices (Section~\ref{sec:choices}). % in \ourtool.
\end{itemize}


\vspace{-1mm}
\vspace{-1mm}

\subsection{Subject Programs}

\vspace{-1mm}

We evaluated \ourtool on \subjectnum Java programs shown
in Figure~\ref{tab:subjects}.
Randoop~\cite{PachecoLET2007} is an automated test generator
for Java programs. Weka~\cite{weka} is a toolkit that implements
machine learning algorithms. Our evaluation
uses only its decision tree module. JChord~\cite{jchord}
is a program analysis platform that enables users to design, implement,
and evaluate static and dynamic program analyses for Java.
Synoptic~\cite{synoptic} mines a finite state machine
model representation of a system from logs.
Soot~\cite{soot} is a Java optimization framework for analyzing and transforming Java bytecode.


%\vspace{-1mm}

\smallsqueeze
\subsubsection{Configuration Errors}

\vspace{-1mm}

\begin{figure}[t]
\vspace{1mm}
\centering
\small{
\setlength{\tabcolsep}{.50\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Config Options & \#Profiles\\
 \hline
 \hline
 Randoop (1.3.2) & 18587 & 57 & 12\\
 Weka Decision Trees (3.6.7) & 3810 & 14 & 12\\
 JChord (2.1) & 23391 &  79 & 6 \\
 Synoptic (trunk, 04/17/2012) & 19153 & 37 & 6\\
 Soot (2.5.0) & 159273 & 49 & 16 \\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:subjects} Subject programs. 
Column ``LOC'' is the number of lines of code,
as counted by CLOC~\cite{cloc}. Column ``\#Config Options''
is the number of configuration options. Column ``\#Profiles''
is the number of execution profiles in the pre-built database.}}
\end{figure}


\begin{figure}[t]
\vspace{2mm}
\setlength{\tabcolsep}{.94\tabcolsep}
\small{
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the specified class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}
}
\vspace{-2mm}
\Caption{{\label{tab:errors} The \errors
configuration errors used in the evaluation.
%The 9 crashing errors in the bottom table are taken from~\cite{Rabkin:2011:PPC}.
}}
\vspace{-0mm}
\end{figure}

We collected \errors configuration errors, listed in Figure~\ref{tab:errors}. 
This paper evaluates all the configuration errors we found; we did not
select only errors on which \ourtool works well.
The misconfigured values include enumerated types,
numerical ranges, regular expressions, and strings.
The \noncrash non-crashing errors
are collected from actual bug reports, mailing list posts, and our own experience.
The \crash crashing errors, taken from~\cite{Rabkin:2011:PPC},
were previously used to evaluate the ConfAnalyzer tool. %~\cite{confanalyzer}.
All \errors configuration errors are minimal: if
any part of the configuration or input is removed, the software
either crashes or no longer exhibits the undesired behavior.
%\todo{Were any of them used in previous research?  Was that the reason we
%  chose them?}

\input{result-fig}

\vspace{-2mm}
\subsection{Evaluation Procedure}
\vspace{-1mm}

For each subject program, we constructed a profile database
by running existing (correct) examples from its user manual,
FAQs, discussion mailing list, forum posts,
and published papers~\cite{PachecoLET2007, Rabkin:2011:PPC}.
%\todo{Earlier, the paper says the 6--16 come from ``from software
%user manuals, FAQs, and forum posts''.  Those two pieces of text must be
%made consistent (and correct).}
We spent 3 hours per program, on average, and obtained 6--16 execution profiles.
The average size of the profile database is 35MB, and the largest one
(Randoop's database) is 72MB.

We made a simple syntactic change to JChord, which affected 24 
lines of code. This change
does not modify JChord's semantics; rather, it just encapsulates
scattered configuration option initialization statements 
as static class fields, which simplifies specifying the seed statement
in performing slicing. 

%Here is a sample modification, where 
%\<chord.kobj.k> 
%is a configuration option
%passed as a system property:

%\vspace{-1mm}

%\begin{CodeOut}
%\begin{alltt}
%public void run() \ttlcb
%  ...
%  int kobjK = Integer.getInteger("chord.kobj.k");
%  ...
%\ttrcb
%\end{alltt}
%\end{CodeOut}
%\vspace{-4mm}
%\hspace{20mm}$\Downarrow$ 
%\vspace{-2mm}
%\begin{CodeOut}
%\begin{alltt}
%static int chord\_kobj\_k = Integer.getInteger("chord.kobj.k");
%public void run() \ttlcb
%  ...
%  int kobjK = chord\_kobj\_k; 
%  ...
%\ttrcb
%\end{alltt}
%\end{CodeOut}


When diagnosing a configuration error, we first reproduced the
error on a \ourtool-instrumented program to obtain the
execution profile. Then, we ran \ourtool on the obtained execution profile
to identify its root causes.

%since we know the misconfigured, root-cause entry for each case,
%we use the ranking of the entry as our evaluation metric.

Our experiments were run on a
2.67GHz Intel Core PC with 4GB physical memory (2GB was allocated
for the JVM), running Windows 7.


\vspace{-1mm}
\subsection{Results}
\label{sec:results}


\subsubsection{Accuracy in Diagnosing Configuration Errors}
\label{sec:accuracy}
\input{diagnosisresults}

\subsubsection{Performance of \ourtool}
\label{sec:performance}
\input{performance}

\subsubsection{Comparison with a Previous Technique}
\label{sec:confanalyzer}
\input{confanalyzer}


\subsubsection{Comparison with Two Fault Localization Techniques}
\label{sec:comparison}
\input{comparison}



%\subsubsection{Effects of Varying Comparison Execution Profiles}
\subsubsection{Evaluation of Two Design Choices in \ourtool}
\label{sec:choices}
\input{designchoice}


\vspace{-1mm}
\subsection{Experimental Discussion}
\vspace{-1mm}


%\todo{Need to mention the following sentence somewhere: many
%non-crashing errors often exhibit substantially behaviorally
%differences (hot spot) in some parts of the program, and our technique captures
%such differences and link them to specific options. However, in
%rare cases (at least we have not discovered in practice), if no
%such hot spot exist for a non-crashing error, our technique may not
%be that effective.}

%\todo{Also perphas need to point out this technique does not
%support diagnosing error involving multi options, at least
%in experiments. Maybe mentioned in the end of experiments.}

\noindent \textbf{\textit{Limitations.}} 
The experiments indicate several limitations of our technique. 
%
First, we only focus on named configuration options
with a common key-value semantic, and our implementation
and experiments are restricted to Java. 
%
Second, we evaluated \ourtool on configuration errors
involving just one mis-configured option.
%
Third,  our implementation currently does not
support debugging non-deterministic errors. 
For non-deterministic errors, \ourtool could potentially leverage 
a deterministic replay system
that can capture an undesired non-deterministic
execution and faithfully reproduce it for later analysis.
%
Fourth, \ourtool's effectiveness  largely
depends on the availability of a similar but correct execution profile.
Using an arbitrary execution profile (as we demonstrated in Section~\ref{sec:choices}
by random selection) may significantly affect the results.

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} 
There are three major threats to validity in our evaluation. 
%
First, the \subjectnum programs and the configuration errors may not be
representative. Thus, we can not claim the results can be
generalized to an arbitrary program.
For example, we did not evaluate \ourtool to diagnose misconfigurations
that cause poor performance.
% (e.g., making a program run faster
% by increasing JVM's heap size via the option \CodeIn{-Xmx}).
%
Second, in this paper, we focus specifically on
configuration errors, assuming the application code is correct.
Furthermore, in our experiments, all \errors errors
have been minimized (as end-users often do
when reporting an error). \ourtool might produce
different error diagnosis results on buggy application 
code with non-minimized inputs.  A user who did not know whether a program
was misbehaving due to a bug in the code or an incorrect configuration
option would need to apply multiple debugging techniques.  We have not
yet formulated guidance regarding when the user should give up on \ourtool
and assume the error is not related to a configuration option.
%
Third, we only compared two dependence
analyses (thin slicing and full slicing), three
abstraction granularities (at the predicate level,
statement level~\cite{Jones:2002}, and method level~\cite{Ernst:1999}),
and three other tools (ConfAnalyzer, Coverage Analysis, and
Invariant Analysis) in our evaluation.
 Using other dependence analyses, abstraction levels, or tools
might achieve different results.

%how easy to construct such database in practice.
%User study of usefulness of the results


\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} 
We have three chief findings: \textbf{(1)} \ourtool is effective
in diagnosing both crashing and non-crashing configuration errors
with a small profile database.
\textbf{(2)} \ourtool produces more accurate diagnosis than
approaches leveraging existing fault localization
techniques~\cite{Jones:2002, McCamant:2003}. \textbf{(3)} Thin slicing
and selection of similar comparison traces
permit \ourtool to produce more accurate diagnosis than other approaches.

%\ourtool makes configuration error diagnosis easier by suggesting
%the specific options that may lead to an unexpected behavior. 




%Compared to
%alternative approaches, \ourtool distinguishes itself by being able to
%diagnose both crashing and non-crashing errors without requiring
%a user-provided testing oracle. $\blacksquare$

% LocalWords:  Weka JChord LOC Config CLOC pre classpath misconfigured 35MB 4GB
% LocalWords:  ConfAnalyzer 72MB JChord's kobjK getInteger kobj 67GHz 2GB mis
% LocalWords:  misconfigurations JVM's Xmx datarace 35MB 72MB 67GHz 4GB
%%  LocalWords:  2GB
