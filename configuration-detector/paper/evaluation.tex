\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

We aim to answer the following research questions:

\begin{itemize}
\item How effective is \ourtool in diagnosing the root cause of
configuration errors?
\item Can \ourtool provide more accurate diagnosis information than
the existing approaches? 
\item What are the effects of varying the comparison profiles?
\item How long does \ourtool take to diagnose a configuration error?
\end{itemize}

%how well can \ourtool identify solutions to configuration problems?
%How effective is predicate-level ..
%Can \ourtool identify slutions to configuration problems involving xx

\subsection{Subject Programs}

We evaluated \ourtool on \subjectnum Java applications: Randoop~\cite{randoop},
Weka~\cite{weka}, JChord~\cite{jchord}, Synoptic~\cite{synoptic},
and Soot~\cite{soot}. Randoop is an automated unit test generation tool
for Java. Weka $\blacksquare$ need for brief introduction

\subsubsection{Configuration Problems}


We searched forums, FAQ pages and the literature on configuration error diagnosis
research to find actual configuration problems that users have experienced with our
target applications. In total, we chose xxx misconfigurations (xxx
are crashing, xxx non-crashing) that were caused by errors in
configuration options. Table xxx lists the configuration errors for each application.
The xxx misconfigured values cover various data types, such as enumerated types,
numerical ranges, regular expressions, and text entries. Table~\ref{} lists
the configuration errors for each application~\cite{tab:subjects}.

\begin{table}[t]
%\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Conf Options & \#Profiles\\
 \hline
 \hline
 Randoop (1.3.2) & 18587 & 57 & 12\\
 Weka (3.6.7) & 256305 & 14 & 12\\
 JChord (2.1) & 23391 &  79 & \\
 Synoptic (trunk) & 19153 & 37 & \\
 Soot (2.5.0) & 159273 & 49 & \\
\hline
\end{tabular}


\Caption{{\label{tab:subjects} Subject programs. 
Column ``LOC'' is the number of lines of code,
as counted by CLOC~\cite{cloc}. Column ``\#Conf Options''
is the number of available configuration options. Column ``\#Profiles''
is the number of profiles in the pre-built database.}}
\end{table}

Need to say how to collect representative profiles

\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated for NanoXML~\cite{nanoxml}\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the specified class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} A list of \errors
configuration errors used in the evaluation.
The 9 crashing errors in the bottom table are taken from~\cite{Rabkin:2011:PPC}.
}}
\end{table}


\subsection{Evaluation Procedural}


To evaluate \ourtool, we used \ourtool to instrument
the application, ran it with its error-revealing
inputs and configuration, and obtained xxxx profile xxx. 

Rewrite the configuration option in the form of method fields, since
the local variable information has gone in the bytecode.

Use 1-CFA to construct call graph, achieve much better precision
than 0-CFA. 

Re-writing the configuration initialization code in JChord, because
it is defined in an add-hoc way

Run examples from user manual to form the database. The effort
can be amortized in development time.

Cannot use unit test, since they are incomplete

We use two metrics to evaluate \ourtool's effectiveness: the absolute ranking of the
actual root cause in the list returned by \ourtool, and the
time cost to diagnose each configuration error.

Use extraction~\cite{Rabkin:2011:SEP}

Are the ranked configurations useful for misconfiguration
error diagnosis?  We examine the ranked configurations to see how well they can explain the behavior.

%This step
%could be automated by combining with some existing  work on
%automated configuration option extraction~\cite{}.
All of our experiments were run on a
2.67GHz Intel Core PC with 4GB physical memory (1GB is allocated
for the JVM), running Windows 7.


\subsection{Results}

\begin{table*}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l||c||c|c||c||c||c||c|}
\hline
 Error ID.  & & \multicolumn{2}{|c||}{\ourtool} & Full Slicing & Coverage Analysis& Invariant Analysis & ConfAnalyzer~\cite{Rabkin:2011:PPC}\\
\cline{3-8}
 Program & Reponsible Option & \#Cmp Profile & Rank  & Rank & Rank & Rank & Rank \\
 \hline
\hline
\multicolumn{8}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop& \CodeIn{maxsize} & & 1 & & N & N &X \\
 2. Weka&\CodeIn{m\_numFolds}&2&1& & 4 & &X\\
 3. JChord& \CodeIn{chord.kobj.k} &  & 2& & & &X\\
 4. Synoptic& \CodeIn{partitionRegExp}&  & 1& & & &X\\
 5. Soot& \CodeIn{keep\_line\_number} & & 3 & & & &X\\
\hline
\hline
\multicolumn{8}{|l|}{Crashing errors}   \\
\hline
 6. JChord& \CodeIn{chord.main.class}& & 1& & & &\\
 7. JChord& \CodeIn{chord.main.class}& & 3& & & &\\
 8. JChord& \CodeIn{chord.run.analyses}& & 17& & & &\\
 9. JChord& \CodeIn{chord.ctxt.kind}& & 1 & & & &\\
 10. JChord& \CodeIn{chord.print.rels}& & 15 & & & &\\
 11. JChord& \CodeIn{chord.print.classes}& & 15 & & & &\\
 12. JChord& \CodeIn{chord.scope.kind}& & 1& & & &\\
 13. JChord& \CodeIn{chord.reflect.kind} & & 1& & & &\\
 14. JChord& \CodeIn{chord.class.path}& & 8 & & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:results} Experimental results in diagnosing software
configuration errors. Column ``Responsible Option'' shows the actual
configuration option for the error. Column ``\ourtool'' shows the results of using
our technique. Columns ``Full Slicing'', ``Coverage Analysis'',
``Invariant Analysis'', and ``ConfAnalyzer'' show the results of
using four comparison techniques, as detailed in Section~\ref{sec:comparison}.
Column ``\#Cmp Profiles'' shows the number of similar profiles selected
from the pre-built database for comparison.
For each technique, Column ``Rank'' shows the rank of the actual responsible
option in its diagnosis outputs; in which lower is better. ``X''
means the technique is not applicable, and ``N'' means the technique
does not output the actual responsible option.}}
\end{table*}


\subsubsection{Accuracy in Diagnosing Configuration Errors}

For each configuration error, Table~\ref{} shows the ranking of the
responsible configuration option, the number , xxx.

The nature of the root-cause configuration option is only one factor.
The ranking also depends on how the root-cause option relates to
other options in the suspect set. A highly customized machine
likely produces more noises, since the unique customizations
can be even less conforming than a sick option value.

As shown in Table~\ref{tab:results}, \ourtool is highly effective
in pinpointing the root cause of misconfigurations. \ourtool
ranks the actual root cause top 3 in xx cases. Sometimes,
when the actual causes is ranked lower, the option ranked
higher provides a valuable clue to help debug the problem.
For instance, in xxx the actual error usually occurs nested xxx.

For xxx, \ourtool successfully pinpointed the root causes (where
we define success as listing the actual root cause as one of the
top 3 options) for xxx of the errors. For the last error, \ourtool
cannot run to completion due to xxxx.

\ourtool also successfully diagnoses xx\% of the xxx errors. For the
remaining errors, \ourtool ranks the root cause 9th. The configuration
error is that the xxx. Thus, the root cause gets ranked lower
in the list.

Overall, \ourtool successfully diagnosed xxx out of xxx errors
by ranking the actual responsible option at top 3.
The remaining errors are a direct result of xxxx and seems
hard for \ourtool to diagnose correctly.


We next briefly explain the root cause of each non-crashing configuration
errors as shown Table~\ref{tab:results}. The detailed illustration
for those crashing errors can be found in~\cite{Rabkin:2011:PPC}.

\begin{itemize}
\item \textbf{Randoop}. reason
\item \textbf{Weka}. reason
\item \textbf{JChord}. reason
\item \textbf{Synoptic}. reason
\item \textbf{Soot}. reason
\end{itemize}


\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective


\subsubsection{Comparison with Alternative Approaches}
\label{sec:comparison}

We next compare \ourtool with four alternative approaches
in diagnosing configuration errors.
%the first three has not been discussed in the literature

\noindent \textbf{Full Slicing.}
How would the results change if traditional slicing~\cite{Horwitz:1988} is used
in Section~\ref{sec:prop}?

What about using traditional slicing for configuration propagation analysis?
full control and data-flow information.

\vspace{1mm}
\noindent \textbf{Coverage Analysis.}
The first alternative is to instrument all statement, and record the different between a bad run and
a set of good runs. Find out the statement covered most by good runs, but covered least by bad run.
After then, querying the thin slicing info, to rank the configuration options that can affect them.

\vspace{1mm}
\noindent \textbf{Invariant Analysis.}
The second alternative is to run the program, diff the dynamically-detected invariants. Ranked all
methods that have the most different invariants between 2 runs. Then, querying thin slicing to
figure out the configuration options.


\vspace{1mm}
\noindent \textbf{Taint-based Analysis.}
Rabkin and katz proposed a family of techniques (and its tool implementation called ConfAnalyzer)
to precompute possible
configuration diagnosis for Java software~\cite{Rabkin:2011:PPC}. In their work,
the most accurate technique is based on dynamic taint analysis, which precisely
tracks the information flow during program execution. % bit level
This taint-based analysis works remarkably well for crashing errors, but can
not diagnose non-crashing errors.

%Compare with statistical debugging?

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective
Is our \textit{predicate}-level granularity a suitable one?

\subsubsection{Effects of Varying Comparison Profiles}
\label{sec:ranking}


\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|c|c||c|}
\hline
  & \multicolumn{3}{|c|}{Rank of the Actual Responsible Option } \\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{2-4}
 Error ID & All Profiles & Random Selection&  Similarity-Based\\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop & & &\\
 2. Weka & 7 & 6 & 1\\
 3. JChord & & &\\
 4. Synoptic & & &\\
 5. Soot & & &\\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6 & & &\\
 7 & & &\\
 8 & & &\\
 9 & & &\\
 10 & & &\\
 11 & & &\\
 12 & & &\\
 13 & & &\\
 14 & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:subjects} Comparison with different profile selection
strategies (Section~\ref{sec:ranking}) in ranking the responsible option.
The last column ``Similarity-based'' is the selection strategy
used in \ourtool, and the data in that column is taken from Table~\ref{tab:results}.}}
\end{table}


We next investigate the effect of varying \ourtool's
comparison profiles. As Table~\ref{} shows, varying the
strategy xxx has substantially different effects on the
diagnosis results, depending on the application being analyzed.

The result difference derives from the nature of the applications.
We found....  . On the other hand,


What would the technique produce when feeding it with different inputs (e.g.,
radically different inputs instead of similar ones)?

In our approach, we first select a set of similar profiles from the  database,
and then to do comparison. What about just using a single trace, i.e., the
most similar trace? the most dissimilar traces? or what about just using a set
of random selected trace.

Comparison of different distance metrics to find similar statements.

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective

\subsubsection{Performance of \ourtool}
low time cost when doing
selective instrumentation, permitting the use in fielded software.

The overhead, the cost of diagnosis, acceptable?

The performance of \ourtool is reasonable. The time to diagnose
an error varies among applications.  XXX app takes less than xxx,
while xxx takes xxx to complete.

\ourtool performs very well on these errors. The average time
to diagnose xxx (with max, min xx).

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} how effective

\vspace{1mm}

\subsection{Experimental Discussions}


\noindent \textbf{\textit{Limitations.}} similar inputs, if no input is available, test adequacy.

We focus on named configuration options with a common key-value semantic.

Our tool implementation and experiments were restricted to Java. Our analysis
does not track options that are passed between processes via the command line.

\ourtool currently does not support debugging non-deterministic errors. %Combining \ourtool with a deterministic replay system
For non-deterministic errors, \ourtool could potentially leverage one of
several deterministic replay systems~\cite{Huang:2010:LLD}
that can capture a buggy non-deterministic
execution and faithfully reproduce it for late analysis.

Our current \ourtool prototype assists in solving configuration errors that are confined
to a single computer system, such as a home computer, personal workstation, or stand-alone server. (no process communication...)

\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} similar inputs, if no inputs is available.

\vspace{1mm}

\noindent \textbf{\textit{Experimental Conclusions.}} similar inputs, if no inputs is available.

\ourtool makes configuration error diagnosis easier by suggesting
the specific options that may lead to an unexpected behavior. Compared to
alternative approaches, \ourtool distinguishes itself by being able to
diagnose both crashing and non-crashing errors without requiring
a user-provided testing oracle. $\blacksquare$
