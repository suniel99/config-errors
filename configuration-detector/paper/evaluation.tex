\section{Evaluation}
\label{sec:evaluation}

%\subsection{Research Questions}

Our evaluation answers the following research questions:

\begin{itemize}
\item How effective is \ourtool in diagnosing the root cause of
configuration errors?
\item Can \ourtool provide more accurate diagnosis information than
the existing approaches? 
\item What are the effects of varying the comparison profiles from
the pre-built database?
\item How long does \ourtool take to diagnose a configuration error?
\end{itemize}

%how well can \ourtool identify solutions to configuration problems?
%How effective is predicate-level ..
%Can \ourtool identify slutions to configuration problems involving xx

\subsection{Subject Programs}

We evaluated \ourtool on \subjectnum Java applications shown
in Table~\ref{tab:subjects}.
Randoop~\cite{randoop} is a robust automated test generator
for Java programs. Weka~\cite{weka} is a useful toolkit implementing
a broad variety of machine learning algorithms. Our evaluation
uses its decision tree module component. JChord~\cite{jchord}
is a program analysis platform that enables users to design, implement,
and evaluate static and dynamic program analyses for Java bytecode.
Synoptic~\cite{synoptic} is a tool to mine a finite state machine
model representations of a system from logs.
Soot~\cite{soot} is a Java optimization framework, which provides four
intermediate representations for analyzing and transforming Java bytecode:

As shown in Table~\ref{tab:subjects}, each subject program has a non-trivial
codebase, and exposes a number of configuration options for users
to customize its behaviors.

\subsubsection{Configuration Errors}



\begin{table}[t]
%\setlength{\tabcolsep}{.84\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Program (version) & LOC & \#Conf Options & \#Profiles\\
 \hline
 \hline
 Randoop (1.3.2) & 18587 & 57 & 12\\
 Weka (3.6.7) & 256305 & 14 & 12\\
 JChord (2.1) & 23391 &  79 & 6 \\
 Synoptic (trunk) & 19153 & 37 & 6\\
 Soot (2.5.0) & 159273 & 49 & 16 \\
\hline
\end{tabular}


\Caption{{\label{tab:subjects} Subject programs. 
Column ``LOC'' is the number of lines of code,
as counted by CLOC~\cite{cloc}. Column ``\#Conf Options''
is the number of available configuration options. Column ``\#Profiles''
is the number of profiles in the pre-built database.}}
\end{table}


\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|l|l|}
\hline
 Error ID & Program & Description \\
 \hline
\hline
\multicolumn{3}{|l|}{Non-crashing errors}   \\
 \hline
 1 & \randoop & No tests generated for NanoXML~\cite{nanoxml}\\
 2 & \weka & Low accuracy of the decision tree\\
 3 & \jchord & No datarace reported for a racy program\\
 4 & \synoptic & Generate an incorrect model\\
 5 & \soot & Source code line number is missing\\
\hline
\hline
\multicolumn{3}{|l|}{Crashing errors}   \\
\hline
 6 & \jchord & No main class is specified\\
 7 & \jchord& No main method in the specified class\\
 8 & \jchord & Running a nonexistent analysis\\
 9 & \jchord & Invalid context-sensitive analysis name\\
 10 & \jchord & Printing nonexistent relations\\
 11 & \jchord & Disassembling nonexistent classes\\
 12 & \jchord & Invalid scope kind\\
 13 & \jchord & Invalid reflection kind\\
 14 & \jchord & Wrong classpath\\
\hline
\end{tabular}

\Caption{{\label{tab:errors} A list of \errors
configuration errors used in the evaluation.
The 9 crashing errors in the bottom table are taken from~\cite{Rabkin:2011:PPC}.
}}
\end{table}

We searched forums, FAQ pages and the literature of
configuration error diagnosis research to find actual
configuration problems that users have experienced with our
target applications. Combining with our own experience,
we chose \errors representative configuration errors.
Table~\ref{tab:errors} lists the configuration errors for each application.
The configuration errors cover various data types, such as enumerated types,
numerical ranges, regular expressions, and text entries.

\subsection{Evaluation Procedural}

For each Java application, we constructed a trace database
by running existing (correct) examples from its user manual, discussion
mailiing list, and the published papers. The number of obtained
 traces is shown in Table~\ref{tab:subjects}. 
We found constructing such a database is quite easy. On average,
we spent 3 hours on one application.

%Run examples from user manual to form the database. The effort
%can be amortized in development time.

We made a 88-line change to the JChord code. This change
does not modify JChord's semantic; rather, it just encapsulates
all scattered configuration option initialization statements 
as static class fields. This is purely an implementation
choice because having a centralized initialization statement
makes us relatively easier to specify the seed statement
in performing slicing during experiments. Here is a sample modification:



\begin{CodeOut}
\begin{alltt}
   // chord.kobj.k is a configuration option
   // passed as a system property
   public void run() \ttlcb
     ...
     int kobjK = Integer.getInteger("chord.kobj.k");
     ...
   \ttrcb
\end{alltt}
\end{CodeOut}
\vspace{-4mm}
\hspace{20mm}$\Downarrow$ 
%\vspace{-2mm}
\begin{CodeOut}
\begin{alltt}
   static int chord\_kobj\_k = Integer.getInteger("chord.kobj.k");
   public void run() \ttlcb
     ...
     int kobjK = chord\_kobj\_k; 
     ...
   \ttrcb
\end{alltt}
\end{CodeOut}



%Use 1-CFA to construct call graph, achieve much better precision
%than 0-CFA. 

%Use extraction~\cite{Rabkin:2011:SEP}

%To evaluate \ourtool, 

When diagnosing a configuration error, we first reproduce the
error on a \ourtool-enabled environment to obtain the
trace file. Then, using the obtained trace file, we use \ourtool
to identify its root cause.

We use two metrics to evaluate \ourtool's effectiveness:
the absolute ranking of the actual root cause in \ourtool's output,
and the time cost used in diagnosis.
All of our experiments were run on a
2.67GHz Intel Core PC with 4GB physical memory (2GB is allocated
for the JVM), running Windows 7.


\subsection{Results}
\label{sec:results}

\begin{table*}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l||c||c|c||c|c|c||c|}
\hline
 Error ID.  & & \multicolumn{2}{|c||}{\ourtool} & Full Slicing & Coverage Analysis& Invariant Analysis & ConfAnalyzer~\cite{Rabkin:2011:PPC}\\
\cline{3-8}
 Program & Reponsible Option & \#Cmp Profile & Rank  & Rank & Rank & Rank & Rank \\
 \hline
\hline
\multicolumn{8}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop& \CodeIn{maxsize} & 10 & 1 & 46 & N & N &X \\
 2. Weka&\CodeIn{m\_numFolds}&2&1& 9 & 4 & 5 &X\\
 3. JChord& \CodeIn{chord.kobj.k} & 3 & 2& 73 & N &2  &X\\
 4. Synoptic& \CodeIn{partitionRegExp}& 2 & 1& 6 & 1 & &X\\
 5. Soot& \CodeIn{keep\_line\_number} & 1 & 3 & N & N& N &X\\
\hline
\hline
\multicolumn{8}{|l|}{Crashing errors}   \\
\hline
 6. JChord& \CodeIn{chord.main.class}&5& 1& 5 & 1 & 4 &1\\
 7. JChord& \CodeIn{chord.main.class}&5 & 1 & 5 & 1 & 4 &1\\
 8. JChord& \CodeIn{chord.run.analyses}&5 & 17& 21 &14 & 17 &1\\
 9. JChord& \CodeIn{chord.ctxt.kind}&4 & 1 & 75 & 27 & 30 &3\\
 10. JChord& \CodeIn{chord.print.rels}& 4& 15 & 24 & 16 & 19 &1\\
 11. JChord& \CodeIn{chord.print.classes}&4 & 16 & 22 & 15 & 18 &1\\
 12. JChord& \CodeIn{chord.scope.kind}&5 & 1& 10 & 1 & N &1\\
 13. JChord& \CodeIn{chord.reflect.kind} &6 & 1& 11 & 6 & 9 &3\\
 14. JChord& \CodeIn{chord.class.path}&5 & 8 & 6 & 2 & 5 &N\\
\hline
\end{tabular}

\Caption{{\label{tab:results} Experimental results in diagnosing software
configuration errors. Column ``Responsible Option'' shows the actual
configuration option for the error. Column ``\ourtool'' shows the results of using
our technique. Columns ``Full Slicing'', ``Coverage Analysis'',
and ``Invariant Analysis'' show the results of three variants of \ourtool
as described in Section~\ref{sec:comparison}.
Column ``ConfAnalyzer'' shows the results of an existing
technique~\cite{Rabkin:2011:PPC}.
Column ``\#Cmp Profiles'' shows the number of similar profiles selected
from the pre-built database for comparison.
For each technique, Column ``Rank'' shows the rank of the actual responsible
option in its output (lower is better). ``X''
means the technique is not applicable (i.e., requiring a crashing point), and ``N'' means the technique
does not identify the actual responsible option.}}
\end{table*}


\subsubsection{Accuracy in Diagnosing Configuration Errors}

Table~\ref{tab:results} shows the experimental results.
We can see that \ourtool is highly effective in pinpointing the root cause of
misconfigurations. For all \noncrash non-crashing errors
and 5 out of the \crash crashing errors, it lists the actual root cause as one of the
top 3 options. 


As shown in Table~\ref{tab:results}, \ourtool is particularly effective
in diagnosing non-crashing configuration errors, which are not supported
by most existing tools. $\blacksquare$ why effective? observation?
statically significance? Give some examples here... Randoop \CodeIn{maxsize},
Weka behaves overfitting..


Compared to non-crashing errors, \ourtool is less effective
in localizing non-crashing errors. For 4 crashing errors,
the actual causes are ranked lower.
This is because $\blacksquare$ the configuration option has
a long propagation chain, and seems hard for \ourtool
to diagnose correctly.

Although \ourtool ranks the actual root course of several
crashing errors lower, crashing errors are often much easier to diagnose than non-crashing errors.
This is because a crashing error usually happens shortly after the program
is launched, and often produces a stack trace with valuable diagnosis clues.
For example, \ourtool ranks the root cause of
Error 14 (Table~\ref{tab:results}) 8th.
However, when JChord crashes, it dumps an \CodeIn{ClassNotFoundException}
that explicitly leads users to check its classpath setting (via the
\CodeIn{chord.class.path} option). For the other three crashing errors (Error ID: 8, 10, and 11),
JChord even dumps out the actual wrong configuration option value, which
directly guides users to the root cause.

%The nature of the root-cause configuration option is only one factor.
%The ranking also depends on how the root-cause option relates to
%other options in the suspect set. A highly configurable software system 
%likely produces more noises, 

%The detailed illustration
%for those crashing errors can be found in~\cite{Rabkin:2011:PPC}.

%The remaining errors are a direct result of $\blacksquare$ and seems
%hard for \ourtool to diagnose correctly.


%\ourtool also successfully diagnoses xx\% of the xxx errors. For the
%remaining errors, \ourtool ranks the root cause 9th. The configuration
%error is that the xxx. Thus, the root cause gets ranked lower
%in the list.



\vspace{1mm}
\noindent \textbf{\textit{Summary.}} \ourtool is effectively
in diagnosing both crashing and non-crashing configuration errors. $\blacksquare$
As shown
in Section~\ref{sec:results}, even a database containing
a small number of profiles is sufficient to
diagnose many configuration errors.

\subsubsection{Comparison with Alternative Approaches}
\label{sec:comparison}

We next compare \ourtool with three variants and
one existing technique~\cite{Rabkin:2011:PPC}.

\vspace{1mm}
\noindent \textbf{Variant 1. \ourtool with Full Slicing.} 
\ourtool uses thin slicing~\cite{Sridharan:2007} to compute the affected predicates
of each configuration option. Another way to do so is
using the full slicing algorithm~\cite{Horwitz:1988}.
This variant replaces thin slicing with 
full slicing~\cite{Horwitz:1988} in the configuration
propagation analysis (Section~\ref{sec:prop}) step.

\vspace{1mm}
\noindent \textbf{Variant 2. \ourtool with Coverage Analysis.}
This variant uses statement-level coverage information
to diagnose a configuration error. It treats statements covered
by the erroneous trace as potentially buggy, and statements
covered the correct traces (in the pre-built database) as correct.
Then, this variant uses a well-known fault localization technique,
Tarantula~\cite{Jones:2002}, to rank the likelihood of each
statement being buggy, and query the result of thin slice
to identify its affecting configuration options as the root causes. 


\vspace{1mm}
\noindent \textbf{Variant 3. \ourtool with Invariant Analysis.}
This variant uses method-level invariant as the abstraction level
to diagnose configuration errors. It stores invariants detected
by Daikon~\cite{Ernst:1999} from correct traces in the database. When a configuration
error occurs, this variant detects invariants from the erroneous trace;
and compares the invariants detected from
the erroneous trace with those stored in the database.
It treats a method to have suspicious behaviors if the observed invariants
from the erroneous trace are different than those stored in the database. Then, it ranks
a method's suspiciousness by the number of different variants, and
queries the result of thin slice
to identify its affecting configuration options as the root causes. 

\vspace{1mm}
\noindent \textbf{ConfAnalyzer: A Dynamic Information Flow-based Approach~\cite{Rabkin:2011:PPC}.}
Rabkin and Katz proposed a family of techniques to precompute possible
configuration diagnosis for Java software~\cite{Rabkin:2011:PPC}. In their work,
the most accurate technique (also the most precise technique in the literature)
is based on dynamic information flow analysis.
This technique works remarkably well for crashing errors, but can
not diagnose non-crashing errors.

\vspace{1mm}

The experimental results of comparing \ourtool with the above
four alternative approaches are shown in Table~\ref{tab:results}.
$\blacksquare$ discuss the results here

\vspace{1mm}
\noindent \textbf{\textit{Summary.}} Using thin slicing to identify
the affected predicates produce more accurate results than using
full slicing. Monitoring dynamic behaviors of the affected
predicates are more effective in diagnosing a configuration error
than statement-level profiling and method-level invariant detection.
Dynamic information flow-based approach is slightly precise in diagnosing
a crashing error, but fails to diagnose non-crashing errors.

\subsubsection{Effects of Varying Comparison Traces}
\label{sec:ranking}


\begin{table}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|l|c|c||c|}
\hline
 Error ID. & \multicolumn{3}{|c|}{Rank of the Actual Responsible Option } \\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{2-4}
 Program & All Traces & Random Selection&  Similarity-Based\\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop & 1 & 2 & 1\\
 2. Weka & 7 & 6 & 1\\
 3. JChord & 16 & 19 & 2\\
 4. Synoptic & 1 & 1 & 1\\
 5. Soot & 13 & 13 & 3\\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6. JChord & 1 & 1 &1\\
 7. JChord & 1 & 1 &1\\
 8. JChord & 17 & 17 &17\\
 9. JChord & 1 &  1&1\\
 10. JChord & 15 & 15 &15\\
 11. JChord & 16 & 16 &16\\
 12. JChord & 1 & 1 &1\\
 13. JChord & 25 & 25 &1\\
 14. JChord & 8 & 8 &8\\
\hline
\end{tabular}

\Caption{{\label{tab:selection} Comparison with different trace selection
strategies (Section~\ref{sec:ranking}).
The last column ``Similarity-based'' is the selection strategy
used in \ourtool, and the data in that column is taken from Table~\ref{tab:results}.}}
\end{table}


\ourtool compares the predicate behaviors in the erroneous trace against
correct traces in the pre-built database.
We next investigate the effect of using different comparison
traces. We compare the similarity-based strategy used in \ourtool
 (Section~\ref{sec:similar}) with two alternatives: using all
available traces in the database, and randomly selecting
traces from the database. Table~\ref{tab:selection} shows the experimental results.

We can see that varying the comparison strategy has result in
substantially different effects on the diagnosis results,
depending on the application being analyzed. Diagnosing
a non-crashing error is more sensitive in selecting similar
comparison traces $\blacksquare$ than diagnosing
a crashing error.
For the \crash crashing errors, the only difference yielded
from using different trace selection strategies is on
the error 13. $\blacksquare$


\vspace{1mm}
\noindent \textbf{\textit{Summary.}} Diagnosing a non-crashing
error is more sensitive to the comparison traces than a crashing one.

\subsubsection{Performance of \ourtool}

We measure \ourtool's performance in two ways: the time cost
in diagnosing an error; and the overhead introduced
in reproducing an error during diagnosis.  Table~\ref{tab:performance}
shows the results.

\begin{table}[t]
\setlength{\tabcolsep}{.44\tabcolsep}
\begin{tabular}{|l|c|c|c|}
\hline
 Error ID. & \multicolumn{2}{|c|}{Time Cost (minutes)} & Slowdown ($\times$)\\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{2-3}
 Program & Thin Slicing & Error Diagnosis &  \\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop &  &  & \\
 2. Weka &  &  & \\
 3. JChord &  &  & \\
 4. Synoptic &  &  & \\
 5. Soot &  &  & \\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6. JChord &  &  &\\
 7. JChord &  &  &\\
 8. JChord &  &  &\\
 9. JChord &  &  &\\
 10. JChord &  &  &\\
 11. JChord &  &  &\\
 12. JChord &  &  &\\
 13. JChord &  &  &\\
 14. JChord &  &  &\\
\hline
\hline
Average & & &\\
\hline
\end{tabular}

\Caption{{\label{tab:performance} \ourtool's
performance in diagnosing configuration
errors. The time cost has been divided into
two parts: computing thin slices and diagnosing
an error.}}
\end{table}

Overall, \ourtool performs very well on these errors.
On average, it uses $\blacksquare$ minutes to
diagnose one configuration error. The time cost for
computing a thin slice from each configuration option
is high. However, this step is a one-time effort
for each program and computed results can be further cached.

This performance overhead in error reproduction,
admittedly high, has nonetheless proved acceptable
for offline error diagnosis.


%The performance of \ourtool is reasonable. The time to diagnose
%an error varies among applications.  XXX app takes less than xxx,
%while xxx takes xxx to complete.


\vspace{1mm}
\noindent \textbf{\textit{Summary.}} say about \ourtool's performance

\vspace{1mm}

\subsection{Experimental Discussions}


\noindent \textbf{\textit{Limitations.}} 
Our technique is limited in three aspects.
First, we only focus on named configuration options
with a common key-value semantic, and our tool implementation
and experiments are
restricted to Java. 
Second,  our tool implementation currently does not
support debugging non-deterministic errors. 
For non-deterministic errors, \ourtool could potentially leverage one of
several deterministic replay systems~\cite{Huang:2010:LLD}
that can capture a buggy non-deterministic
execution and faithfully reproduce it for late analysis.
Third, the effectiveness of \ourtool largely
depends on the availability of a similar but correct trace.
Using an abitrary trace (as we demonstrated in Section~\ref{sec:ranking}
by random selection) may significantly affect the results.

%similar inputs, if no input is available, test adequacy.



%Our current \ourtool prototype assists in solving configuration errors that are confined
%to a single computer system, such as a home computer, personal workstation, or stand-alone server. (no process communication...)

%similar inputs, if no inputs is available.

%how easy to construct such database in practice.

%User study of usefulness of the results
\vspace{1mm}

\noindent \textbf{\textit{Threats to Validity.}} 
There are two major threats to validity in our evaluation. 
First, the \subjectnum programs and the configuration errors may not be
representative. Thus, we can not claim the results can be
extended to an arbitrary program.
Another threat is that we only employed two dependence
analyses (thin slicing and full slicing) and three
abstraction granularities (at the predicate level,
statement level, and method level) in our evaluation.
 Using other dependence analyses or abstraction levels
might achieve different results.



%\vspace{1mm}

%\noindent \textbf{\textit{Experimental Conclusions.}} 
%\ourtool makes configuration error diagnosis easier by suggesting
%the specific options that may lead to an unexpected behavior. Compared to
%alternative approaches, \ourtool distinguishes itself by being able to
%diagnose both crashing and non-crashing errors without requiring
%a user-provided testing oracle. $\blacksquare$
