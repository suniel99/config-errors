We measured \ourtool's performance in two ways: the time cost
in diagnosing an error and the overhead introduced
in reproducing an error in a \ourtool-instrumented program. 
%Figure~\ref{tab:performance} shows the results.

\begin{figure}[t]
\setlength{\tabcolsep}{.94\tabcolsep}
\small{
\begin{tabular}{|l|c|c|c|}
\hline
 Error ID.  & Run-time & \multicolumn{2}{|c|}{\ourtool time (seconds)}\\
  %& \multicolumn{3}{|c|}{Different Comparison Profile Selection Strategy} \\
\cline{3-4}
 Program & Slowdown ($\times$) & Thin Slicing & Error Diagnosis  \\
 \hline
\hline
\multicolumn{4}{|l|}{Non-crashing errors}   \\
 \hline
 1. Randoop  & 1.1 & 50  & $<$ 1 \\
 2. Weka     & 1.2 & 43  & $<$ 1 \\
 3. JChord   & 13.2& 147 & 82    \\
 4. Synoptic & 3.6 & 24  & $<$ 1 \\
 5. Soot     & 3.1 & 95  & 21    \\
\hline
% With arithmetic mean:  Average & 4.4 & 72 & 21 \\
% (geometric-mean 1.1 1.2 13.2 3.6 3.1)
Mean & 2.9 & 72 & 21 \\
\hline
\hline
\multicolumn{4}{|l|}{Crashing errors}   \\
\hline
 6. JChord   & 2.4  & 147 & 79 \\
 7. JChord   & 1.4  & 147 & 75 \\
 8. JChord   & 1.5  & 147 & 17 \\
 9. JChord   & 28.5 & 147 & 30 \\
 10. JChord  & 13.7 & 147 & 13 \\
 11. JChord  & 65.1 & 147 & 10 \\
 12. JChord  & 1.6  & 147 & 83 \\
 13. JChord  & 1.9  & 147 & 8  \\
 14. JChord  & 1.4  & 147 & 80 \\
\hline
% With arithmetic mean:  Average & 13 & 147 & 44 \\
% (geometric-mean 2.4 1.4 1.5 28.5 13.7 65.1 1.6 1.9 1.4)
Mean & 4.3 & 147 & 44 \\
\hline
\end{tabular}
}
\Caption{{\label{tab:performance} \ourtool's
performance.  The run-time slowdown column shows the cost of reproducing
the error in an instrumented version of the subject program, and the
mean is the geometric mean.
The \ourtool time has been divided into
two parts --- computing thin slices and diagnosing
an error --- and the mean is the arithmetic mean.}}
\end{figure}

Figure~\ref{tab:performance} shows the results.
The performance of \ourtool is reasonable.
On average, it uses less than \avgtime minutes to
diagnose one configuration error (including
the time to compute thin slices and the time
to recommend suspicious configuration options). Computing
thin slices for all configuration options
is expensive. However, this step is one-time effort
per program and the computed slices can be cached
to share across diagnoses. %for future use.

The performance overhead to reproduce the buggy behavior varies
among applications. The current tool implementation
imposes a substantial slowdown when reproducing
errors 3, 9, 10, and 11 in a \ourtool-instrumented version.
%This performance overhead, admittedly high, is still acceptable
%for offline error diagnosis.
This is due to \ourtool's naive,
inefficient instrumentation code, which we have made no effort to optimize.
Even so, an error can be reproduced in less than \avgtime 
minutes on average, with a worst case of 13 minutes.
\todo{Add run times to table??}
% For the other 10 errors, the average slowdown is only 1.9.

%\todo{Maybe add one more sentence here: for error 9, 10, 11, the slowdown for
%reproducing a crashing error is very large. However, the absolute
%time cost is very low, on the original JChord version, all these three errors exhibit in less than 5 seconds. Thus, even with a 65X slowdown, the absolute time cost
%is still acceptable (around 5 mins).}

%The size of profile files in the database $\blacksquare$


%The performance of \ourtool is reasonable. The time to diagnose
%an error varies among applications.  XXX app takes less than xxx,
%while xxx takes xxx to complete.


%%  LocalWords:  Weka JChord
